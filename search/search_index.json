{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Danube Pub/Sub messaging docs","text":"<p>Danube is an open-source, distributed publish-subscribe (Pub/Sub) message broker system developed in Rust. Inspired by the Apache Pulsar messaging and streaming platform, Danube incorporates some similar concepts but is designed to carve its own path within the distributed messaging ecosystem.</p> <p>Currently, the Danube platform exclusively supports Non-persistent messages. Meaning that  the messages reside solely in memory and are promptly distributed to consumers if they are available, utilizing a dispatch mechanism based on subscription types.</p> <p>I'm continuously working on enhancing and adding new features. Contributions are welcome, and you can also report any issues you encounter.</p> <p>The following crates are part of the Danube workspace:</p> <ul> <li>danube-broker - The main crate, danube pubsub platform</li> <li>danube-admin - Admin CLI designed for interacting with and managing the Danube cluster</li> <li>danube-client - An async Rust client library for interacting with Danube Pub/Sub messaging platform</li> <li>danube-pubsub - CLI to handle message publishing and consumption</li> </ul> <p>The available client libraries:</p> <ul> <li>danube-client - Danube Pub/Sub async Rust client library</li> <li>danube-go - Danube Pub/Sub Go client library</li> </ul> <p>Contributions in other languages, such as Python, Java, etc., are also greatly appreciated.</p> <p>\u26a0\ufe0f The messsaging platform is currently under active development and may have missing or incomplete functionalities. Use with caution.</p>"},{"location":"architecture/PubSub_messaging_vs_Streaming/","title":"Danube Platform","text":"<p>Danube Platform service is designed for high-performance &amp; low-latency messaging. As for now it supports only Pub/Sub Messaging.</p>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#danube-pubsub-messaging","title":"Danube Pub/Sub messaging","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#purpose-and-use-cases","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system.</li> <li>Use Cases: Event-driven architectures, real-time notifications, decoupled microservices, and distributed systems. Suitable for scenarios where low latency is critical and some message loss is acceptable, such as real-time monitoring, telemetry data, and ephemeral chat messages.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#architecture-and-design","title":"Architecture and Design","text":"<ul> <li>Components: Consists of Producers, Consumers (subscriptions),  and the message broker.</li> <li>Message Flow: Producers send messages to a broker, which then distributes them to subscribers based on subscription criteria.</li> <li>Scaling: Scales by adding more brokers or distributing load (topics / partitions) across multiple brokers.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#data-handling-and-processing-models","title":"Data Handling and Processing Models","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#pubsub-messaging-producers","title":"Pub/Sub messaging Producers","text":"<ul> <li>Low Latency: Messages sent to topics are not stored on disk, which reduces the latency associated with producing messages.</li> <li>Order Guarantees: Provide ordering within the topic or partition.</li> <li>Message Delivery: There are no guarantees that messages will be delivered. If the broker crashes or if there are network issues, messages might be lost.</li> <li>Transient Acknowledgements: Acknowledgements to the producer are quicker since they are based on in-memory operations rather than disk writes.</li> <li>Publishing Without Consumers: Producers are allowed to publish messages to topics even if there are no active consumers. However, if no consumers are connected, these messages will effectively be dropped because the topics do not store messages.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#pubsub-messaging-consumers","title":"Pub/Sub messaging Consumers","text":"<ul> <li>Real-time Consumption: Consumers of the topics typically process messages in real-time. If a consumer is not available, the message might be lost.</li> <li>No Replay: Since messages are not stored, consumers cannot replay messages. They must process them as they arrive.</li> <li>Reduced Overhead: The topics can handle higher throughput with lower overhead, suitable for use cases where occasional message loss is acceptable.</li> <li>Message Delivery: Messages are delivered to consumers only if they are currently connected and subscribed to the topic. If there are no consumers, the messages are not retained and are discarded by the broker.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#order-of-operations-of-pubsub-messaging","title":"Order of Operations of Pub/Sub messaging","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Broker Receives Message: The broker processes the message.</li> <li>Consumer Availability Check: If consumers are available, the message is delivered to them in real-time.</li> <li>No Consumers: If no consumers are connected, the message is discarded.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#danube-streaming-design-considerations","title":"Danube Streaming (design considerations)","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#purpose-and-use-cases_1","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for processing and analyzing large volumes of data in real-time as it is generated.</li> <li>Use Cases: Real-time analytics, data pipelines, event sourcing, continuous data processing, and stream processing applications. Ideal for use cases requiring high reliability and message durability, such as financial transactions, order processing, and logging critical events.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#architecture-and-design_1","title":"Architecture and Design","text":"<ul> <li>Components: Consists of producers, consumers, stream processors, and a distributed log.</li> <li>Data Flow: Producers write data to a distributed log, which consumers and stream processors read from in a continuous fashion.</li> <li>Scaling: Designed to handle high throughput and scale horizontally by partitioning data across multiple nodes in a cluster.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#data-handling-and-processing-models_1","title":"Data Handling and Processing Models","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#streaming-producers","title":"Streaming Producers","text":"<ul> <li>Durability: Messages sent to topics are stored to persistent storage and replicated according to the topic's configuration. This ensures that messages are not lost even if brokers crash. This allows playback of streams for for historical data analysis and reprocessing.</li> <li>Acknowledgements: Producers receive acknowledgments once the message is safely stored and replicated. This adds a small amount of latency compared to pub/sub messaging.</li> <li>Order Guarantees: Provide ordering within the topic or partition.</li> <li>Delivery Guarantees: Producers can rely on stronger delivery guarantees (e.g., at least once or effectively once).</li> <li>Publishing Without Consumers: Producers can publish messages to a topic even if there are no active consumers. These messages will be stored by the broker.</li> <li>Processing: Supports complex processing such as windowed operations, aggregations, joins, and stateful transformations.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#streaming-consumers","title":"Streaming Consumers","text":"<ul> <li>Message Retention: Consumers can consume messages at any time as long as the retention policy allows. This is useful for replaying messages, ensuring that no messages are missed.</li> <li>Consumption Acknowledgements: Consumers acknowledge each message, allowing the broker to track which messages have been consumed and manage retention accordingly.</li> <li>Fault Tolerance: If a consumer crashes, it can resume consumption from where it left off, as the messages are stored persistently on the broker.</li> <li>Message Retention: Messages are stored according to the configured retention policies. This ensures that even if no consumers are currently connected, the messages will be available for consumption later.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#order-of-operations-of-streaming","title":"Order of Operations of Streaming","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Broker Receives and Stores Message: The broker stores the message on the persistent storage and replicates it according to the configuration.</li> <li>Message Acknowledgment: The broker acknowledges the producer that the message is safely stored.</li> <li>Consumer Availability Check: If consumers are available, the message is delivered to them.</li> <li>No Consumers: If no consumers are connected, the message remains stored and is available for future consumption.</li> </ul>"},{"location":"architecture/Queuing_PubSub_messaging/","title":"Pub-Sub messaging","text":"<p>Danube is built on the publish-subscribe pattern. In this pattern, producers publish messages to topics; consumers subscribe to those topics, process incoming messages, and send acknowledgments to the broker when processing is finished.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#queuing-vs-pubsub-fan-out","title":"Queuing vs Pub/Sub (fan-out)","text":"<p>Below is some general documentation about Queuing and Pub-Sub, not related to Danube implementation, but just to ensure we are aware of the standards.</p> <p>Messaging queuing and pub-sub are both messaging patterns used for asynchronous communication between applications, but they differ in their approach:</p>"},{"location":"architecture/Queuing_PubSub_messaging/#queuing-messaging","title":"Queuing Messaging","text":"<ul> <li>Model: Point-to-Point (One-to-One). A message producer sends a message to a specific queue, and only one consumer can receive and process that message.</li> <li>Order: Messages are typically processed in the order they are received (FIFO - First-In-First-Out). This ensures tasks are completed sequentially.</li> <li>Delivery: Messages are guaranteed to be delivered at least once. This reliability is crucial for critical tasks.</li> </ul> <p>Examples: Use cases include processing orders, sending emails, or handling failed transactions.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#pubsub-messaging-fan-out","title":"Pub/Sub Messaging (fan-out)","text":"<ul> <li>Model: Publish-Subscribe (One-to-Many). A producer publishes messages to a topic, and any interested subscribers can receive the message. Multiple subscribers can receive the same message.</li> <li>Order: Message order is not guaranteed. Subscribers receive messages as they are published. This is suitable for real-time updates or notifications.</li> <li>Delivery: Delivery is often \"fire-and-forget,\" meaning there's no guarantee a subscriber receives the message. This is acceptable for non-critical data.</li> </ul> <p>Examples: Use cases include broadcasting stock price updates, sending chat messages, or triggering real-time analytics.</p> <p>In Summary:</p> <ul> <li>Messaging queues are for reliable, ordered delivery to a single consumer, ideal for task processing.</li> <li>Pub-sub is for broadcasting messages to many interested parties, good for real-time updates.</li> </ul>"},{"location":"architecture/Queuing_PubSub_messaging/#danube-subscriptions","title":"Danube Subscriptions","text":"<p>A Danube subscription is a named configuration rule that determines how messages are delivered to consumers. It is a lease on a topic established by a group of consumers:</p> <ul> <li>Exclusive (can be used for pub-sub) - The exclusive type is a subscription type that only allows a single consumer to attach to the subscription. If multiple consumers subscribe to a topic using the same subscription, an error occurs.</li> <li>Shared (for queuing) - The shared subscription type Danube allows multiple consumers to attach to the same subscription. Messages are delivered in a round-robin distribution across consumers, and any given message is delivered to only one consumer.</li> </ul>"},{"location":"architecture/Queuing_PubSub_messaging/#queuing-or-pubsub-fan-out-using-danube","title":"Queuing or Pub/Sub (fan-out) using Danube","text":"<ul> <li>If you want to achieve message queuing among consumers, share the same subscription name among multiple consumers</li> <li>If you want to achieve traditional fan-out pub-sub messaging among consumers, specify a unique subscription name for each consumer with an exclusive subscription type.</li> </ul>"},{"location":"architecture/architecture/","title":"Danube Architecture","text":""},{"location":"architecture/architecture/#danube-pubsub-messaging","title":"Danube Pub/Sub Messaging","text":"<p>Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system. Suitable for scenarios where low latency is critical and some message loss is acceptable, such as real-time monitoring / notifications, telemetry data, event-driven architectures.</p> <p>Currently, the Danube platform supports only Non-persistent messaging.</p> <p>The messages reside only in memory, providing low latency but not guaranteed to survive broker crashes or consumer disconnections.  The producers are allowed to send messages to Topics even if there are no active consumers. If no consumers are found the messages are droped. The messages are promptly distributed to consumers if they are available, utilizing a dispatch mechanism based on subscription types.</p> <p>Read Here for more detailed design considerations.</p> <p></p>"},{"location":"architecture/architecture/#brokers","title":"Brokers","text":"<p>A cluster consist of one or more Danube Brokers.</p> <p>The producers connect to the brokers to publish messages and the consumers connect to the brokers to consume the messages.</p> <p>Messages are dispatched immediatelly to available consumers, for increased performance.</p>"},{"location":"architecture/architecture/#metadatastore","title":"MetadataStore","text":"<p>Used for cluster-level metadata storage, configuration and coordination. Maintain the metadata store of the Danube Cluster, such as namespace / topic metadata, broker load data and others.</p>"},{"location":"architecture/architecture/#danube-stream-not-yet-supported","title":"Danube Stream (not yet supported)","text":"<p>Designed for processing and analyzing large volumes of data in real-time as it is generated.</p> <p>In Danube Stream the messages are stored durably on disk (across multiple disks for reliability). This ensures message survival even during broker restarts or consumer failures. Ideal for use cases requiring high reliability and message durability, such as financial transactions, order processing, logging critical events, etc.</p> <p></p>"},{"location":"architecture/architecture/#_brokers","title":"_Brokers","text":"<p>A cluster consist of one or more Danube Brokers.</p> <p>The producers connect to the brokers to publish messages and the consumers connect to the brokers to consume the messages.</p> <p>Messages are typically dispatched out of a in memory cache for the sake of performance. If the backlog grows too large for the cache, the broker will start reading entries from the distributed storage.</p>"},{"location":"architecture/architecture/#_metadatastore","title":"_MetadataStore","text":"<p>Used for cluster-level metadata storage, configuration and coordination. Maintain the metadata store of the Danube Cluster, such as namespace / topic metadata, broker load data and others.</p>"},{"location":"architecture/architecture/#message-store","title":"Message Store","text":"<p>It provides message delivery guarantee for applications. If a message successfully reaches the Danube broker, it will be delivered to its intended target.</p> <p>This guarantee requires that non-acknowledged messages are stored durably until they can be delivered to and acknowledged by consumers. This mode of messaging is commonly called persistent messaging.</p>"},{"location":"architecture/internal_danube_services/","title":"Danube Cluster Services Role","text":"<p>This document enumerates the principal components of the Danube Cluster and their responsibilities.</p>"},{"location":"architecture/internal_danube_services/#danube-service-components","title":"Danube Service Components","text":""},{"location":"architecture/internal_danube_services/#leader-election-service","title":"Leader Election Service","text":"<p>The Leader Election Service selects one broker from the cluster to act as the Leader. The Broker Leader is responsible for making decisions. This service is used by the Load Manager, ensuring only one broker in the cluster posts the cluster aggregated Load Report.</p> <p>Leader Election Flow:</p> <ul> <li>The first broker registered in the cluster becomes the Leader by registering itself on \"/cluster/leader\".</li> <li>The field is registered with a lease, so the leader broker must periodically renew its lease to maintain leadership.</li> <li>Subsequent brokers attempt to become leaders but become Followers if the path is already in use.</li> <li>All brokers periodically check the leader path. If there is no change, the state is maintained; otherwise, brokers attempt to become the leader.</li> </ul>"},{"location":"architecture/internal_danube_services/#load-manager-service","title":"Load Manager Service","text":"<p>The Load Manager monitors and distributes load across brokers by managing topic and partition assignments. It implements rebalancing logic to redistribute topics/partitions when brokers join or leave the cluster and is responsible for failover mechanisms to handle broker failures.</p> <p>Load Manager Flow:</p> <ul> <li>All brokers periodically post their Load Reports on the path \"/cluster/brokers/load/{broker-id}\".</li> <li>The leader broker watches for load reports from all brokers in the cluster.</li> <li>It calculates rankings using the selected Load Balance algorithm.</li> <li>It posts its calculations for the cluster on the \"/cluster/load_balance\" path.</li> </ul> <p>Creation of a New Topic:</p> <ul> <li>A broker registers the Topic on the \"/cluster/unassigned\" path.</li> <li>The Load Manager of the leader Broker watches this path and assigns the broker with the least load to host the new topic by posting the topic to the \"/cluster/brokers/{broker-id}/{topic_name}\" path.</li> <li>Each broker watches its own path: \"/cluster/brokers/{broker-id}\". For any event on that path, such as the addition or deletion of topics, it acts accordingly by creating a new topic locally or deleting the topic it owned and all related resources.</li> <li>On topic creation, the broker checks if the topic already exists locally. If not, it retrieves all data about the topic, including subscriptions and producers, from the Local Metadata Cache.</li> <li>On topic removal, the broker handles the disconnections of producers and consumers and removes the locally allocated resources.</li> </ul> <p>For further consideration: We may want the broker to ask the Load Manager to get the next broker and initiate topic creation. Either it just posts the topic on the \"/cluster/unassigned\" path, or if it is the selected broker, it also creates the topic locally.</p>"},{"location":"architecture/internal_danube_services/#local-metadata-cache","title":"Local Metadata Cache","text":"<p>This cache stores various types of metadata required by Danube brokers, such as topic and namespace data, which are frequently accessed during message production and consumption. This reduces the need for frequent queries to the central metadata store, ETCD.</p> <p>The docs/internal_resources.md document describes how the resources are organized in the Metadata Store.</p> <p>Updates/events are received via ETCD Watch events and/or the metadata event synchronizer.</p>"},{"location":"architecture/internal_danube_services/#syncronizer","title":"Syncronizer","text":"<p>The synchronizer ensures that metadata and configuration settings across different brokers remain consistent. It propagates changes to metadata and configuration settings using client Producers and Consumers.</p> <p>This is in addition to Metadata Storage watch events, allowing brokers to process metadata updates even if there was a communication glitch or the broker was unavailable for a short period, potentially missing the Store Watch events. The synchronizer allows for dynamic updates to configuration settings without requiring a broker service restart.</p>"},{"location":"architecture/internal_danube_services/#danube-broker","title":"Danube Broker","text":"<p>The Broker owns the topics and manages their lifecycle. It also facilitates the creation of producers, subscriptions, and consumers, ensuring that producers can publish messages to topics and consumers can consume messages from topics.</p>"},{"location":"architecture/internal_danube_services/#external-metadata-storage-etcd","title":"External Metadata Storage (ETCD)","text":"<p>This is the Metadata Storage responsible for the persistent storage of metadata and cluster synchronization.</p>"},{"location":"architecture/messages/","title":"Message","text":"<p>It is the basic unit, it is what producers publish to topics and what consumers then consume from topics. It includes metadata about the message as well as the actual payload.</p>"},{"location":"architecture/messages/#sent-message-by-producer","title":"Sent message - by Producer","text":"<p>The user is able to send through producer the following data:</p>"},{"location":"architecture/messages/#payload-bytes","title":"payload (bytes)","text":"<ul> <li><code>Description</code>: The actual payload of the message. This field contains the data that the consumer will process.</li> <li><code>Usage</code>: This is the core content of the message that is intended to be consumed by the receiving party.</li> </ul>"},{"location":"architecture/messages/#attributes-mapstring-string","title":"attributes (map(string, string) )","text":"<ul> <li><code>Description</code>: A map of user-defined properties or attributes associated with the message. This can include custom metadata or tags.</li> <li><code>Usage</code>: This field provides additional flexibility for including extra context or metadata that might be required for specific processing or filtering needs.</li> </ul>"},{"location":"architecture/messages/#received-message-by-consumer","title":"Received message - by Consumer","text":""},{"location":"architecture/messages/#payload-bytes_1","title":"payload ( bytes)","text":"<ul> <li><code>Description</code>: The actual payload of the message. This field contains the data that the consumer will process.</li> <li><code>Usage</code>: This is the core content of the message that is intended to be consumed by the receiving party.</li> </ul>"},{"location":"architecture/messages/#metadata-messagemetadata","title":"metadata (MessageMetadata)","text":"<ul> <li><code>Description</code>: Contains additional metadata related to the message. This can include information about the producer and the message itself.</li> <li><code>Usage</code>: This field provides context and additional information that can be used for processing, tracking, and analyzing messages.</li> </ul>"},{"location":"architecture/messages/#messagemetadata","title":"MessageMetadata","text":"<p>The <code>MessageMetadata</code> message includes supplementary information about the message itself, which is useful for managing message processing and analytics. It contain the following fields:</p> <p>producer_name (string):</p> <ul> <li><code>Description</code>: The name of the producer that sent the message. This can be used for identifying the source of the message.</li> <li><code>Usage</code>: This field is optional and can be used for logging, tracing, or any other purposes where the producer's identity is needed.</li> </ul> <p>sequence_id (uint64):</p> <ul> <li><code>Description</code>: Represents the sequence ID of the message within the topic. It helps in maintaining the order of messages.</li> <li><code>Usage</code>: This field is crucial for ordering messages correctly when received by consumers. It helps in ensuring that messages are processed in the correct sequence.</li> </ul> <p>publish_time (uint64):</p> <ul> <li><code>Description</code>: Indicates the time when the message was published. It is typically a timestamp value.</li> <li><code>Usage</code>: This field can be used for time-based processing, such as determining the age of a message or for scheduling purposes.</li> </ul> <p>attributes (map(string, string)):</p> <ul> <li><code>Description</code>: A map of user-defined properties or attributes associated with the message. This can include custom metadata or tags.</li> <li><code>Usage</code>: This field provides additional flexibility for including extra context or metadata that might be required for specific processing or filtering needs.</li> </ul>"},{"location":"architecture/subscriptions/","title":"Subscription","text":"<p>A Danube subscription is a named configuration rule that determines how messages are delivered to consumers. It is a lease on a topic established by a group of consumers.</p> <p>The Danube permits multiple producers and subcribers to the same topic. The Subscription Types can be combined to obtain message queueing or fan-out pub-sub messaging patterns.</p> <p></p>"},{"location":"architecture/subscriptions/#exclusive","title":"Exclusive","text":"<p>The Exclusive type is a subscription type that only allows a single consumer to attach to the subscription. If multiple consumers subscribe to a topic using the same subscription, an error occurs. This consumer has exclusive access to all messages published to the topic or partition.</p>"},{"location":"architecture/subscriptions/#exclusive-subscription-on-non-partitioned-topic","title":"Exclusive subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumer</code>: Only one consumer can be attached to the topic with an Exclusive subscription.</li> <li><code>Message Handling</code>: The single consumer handles all messages from the topic, receiving every message published to that topic.</li> </ul>"},{"location":"architecture/subscriptions/#exclusive-subscription-on-partitioned-topic-multiple-partitions","title":"Exclusive subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumer</code>: One consumer is allowed to connect to the subscription across all partitions of the partitioned topic.</li> <li><code>Message Handling</code> : This single consumer processes messages from all partitions of the partitioned topic. If a topic is partitioned into multiple partitions, the exclusive consumer handles messages from every partition.</li> </ul>"},{"location":"architecture/subscriptions/#shared","title":"Shared","text":"<p>The Shared subscription type Danube allows multiple consumers to attach to the same subscription. Messages are delivered in a round-robin distribution across consumers, and any given message is delivered to only one consumer.</p>"},{"location":"architecture/subscriptions/#shared-subscription-on-non-partitioned-topic","title":"Shared subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumers</code>: Multiple consumers can subscribe to the same topic.</li> <li><code>Message Handling</code>: Messages are distributed among all consumers in a round-robin fashion.</li> </ul>"},{"location":"architecture/subscriptions/#shared-subscription-on-partitioned-topic-multiple-partitions","title":"Shared subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumers</code>: Multiple consumers can subscribe to the partitioned topic.</li> <li><code>Message Handling</code>: Messages are distributed across all partitions, and then among consumers in a round-robin fashion. Each message from any partition is delivered to only one consumer.</li> </ul>"},{"location":"architecture/subscriptions/#failover","title":"FailOver","text":"<p>The FailOver subscription type allows only one consumer (the active consumer) to receive messages at any given time. An active consumer is picked for a non-partitioned topic or for each partition of a partitioned topic and receives messages.</p>"},{"location":"architecture/subscriptions/#failover-subscription-on-non-partitioned-topic","title":"FailOver subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumers</code>: Only one active consumer processes messages at a time.</li> <li><code>Message Handling</code>: If the active consumer fails, a standby consumer will take over.</li> </ul>"},{"location":"architecture/subscriptions/#failover-subscription-on-partitioned-topic-multiple-partitions","title":"FailOver subscription on  Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumers</code>: One active consumer processes messages for each partition.</li> <li><code>Message Handling</code>: If the active consumer for a partition fails, a standby consumer takes over.</li> </ul>"},{"location":"architecture/topics/","title":"Topic","text":"<p>A topic is a unit of storage that organizes messages into a stream. As in other pub-sub systems, topics are named channels for transmitting messages from producers to consumers. Topic names are URLs that have a well-defined structure:</p>"},{"location":"architecture/topics/#namespacetopic_name","title":"/{namespace}/{topic_name}","text":"<p>Example: /default/markets (where default is the namespace and markets the topic)</p>"},{"location":"architecture/topics/#partitioned-topics","title":"Partitioned Topics","text":"<p>Danube support both partitioned and non-partitioned topics. The non-partitioned topics are served by a single broker, while the partitioned topic has partitiones that are served by multiple brokers within the cluster, thus allowing for higher throughput.</p> <p>A partitioned topic is implemented as N internal topics, where N is the number of partitions. When publishing messages to a partitioned topic, each message is routed to one of several brokers. The distribution of partitions across brokers is handled automatically.</p> <p></p> <p>Messages for the topic are broadcast to two consumers. The routing mode determines each message should be published to which partition, while the subscription type determines which messages go to which consumers.</p>"},{"location":"architecture/topics/#benefits-of-the-partitioned-topics","title":"Benefits of the Partitioned topics","text":"<ul> <li><code>Scalability</code>: Partitioned topics enable horizontal scaling by distributing the load across multiple partitions. This is essential for high-throughput systems that need to handle large volumes of data efficiently.</li> <li><code>Parallel Processing</code>: It allows multiple consumers to process different partitions of the same topic concurrently, improving throughput and processing efficiency.</li> <li><code>Data Locality</code>: Partitioning can help in maintaining data locality and reducing processing latency, as consumers handle a specific subset of the data (key-shared distribution).</li> </ul>"},{"location":"architecture/topics/#creation-of-partitioned-topics","title":"Creation of Partitioned Topics","text":"<ul> <li><code>Static Creation</code>: Partitioned topics are created with a predefined number of partitions. When you create a partitioned topic, you specify the number of partitions it should have. This number remains fixed for the lifetime of the topic, although you can configure this number at topic creation time.</li> <li><code>Dynamic Scaling</code>: Not supported Yet</li> </ul>"},{"location":"architecture/topics/#producers","title":"Producers","text":"<p>The producers routing mechanism determine which messages go to which partition.</p>"},{"location":"architecture/topics/#routing-modes","title":"Routing modes","text":"<p>When publishing to partitioned topics, you must specify a routing mode. The routing mode determines each message should be published to which partition or which internal topic.</p> <ul> <li>RoundRobinPartition - The producer will publish messages across all partitions in round-robin fashion to achieve maximum throughput. If a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> <li>SinglePartition - If no key is provided, the producer will randomly pick one single partition and publish all the messages into that partition. While if a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> </ul>"},{"location":"architecture/topics/#consumers-subscriptions","title":"Consumers (subscriptions)","text":"<p>The subscription type determines which messages go to which consumers.</p> <p>Check the Subscription documentation for details on how messages are distributed to consumers based on the subscription type.</p>"},{"location":"client_libraries/clients/","title":"Danube client library","text":"<p>Currently, the supported clients are the Rust Client and Go Client clients. However, the community is encouraged to contribute by developing clients in other programming languages.</p>"},{"location":"client_libraries/clients/#rust-client","title":"Rust client","text":"<p>The Rust danube-client is an asynchronous Rust client library. To start using the <code>danube-client</code> library in your Rust project, you need to add it as a dependency. You can do this by running the following command:</p> <pre><code>cargo add danube-client\n</code></pre> <p>This command will add danube-client to your <code>Cargo.toml</code> file. Once added, you can import and use the library in your Rust code to interact with the Danube Pub/Sub messaging platform.</p>"},{"location":"client_libraries/clients/#go-client","title":"Go client","text":"<p>To start using the danube-go library in your Go project, you need to add it as a dependency. You can do this by running the following command:</p> <pre><code>go get github.com/danrusei/danube-go\n</code></pre> <p>This command will fetch the <code>danube-go</code> library and add it to your <code>go.mod</code> file. Once added, you can import and use the library in your Go code to interact with the Danube Pub/Sub messaging platform.</p>"},{"location":"client_libraries/clients/#community-danube-clients","title":"Community Danube clients","text":"<p>TBD</p>"},{"location":"client_libraries/consumer/","title":"Consumer","text":"<p>A consumer is a process that attaches to a topic via a subscription and then receives messages.</p> <p>Subscription Types - describe the way the consumers receive the messages from topics</p> <ul> <li>Exclusive -  Only one consumer can subscribe, guaranteeing message order.</li> <li>Shared -  Multiple consumers can subscribe, messages are delivered round-robin, offering good scalability but no order guarantee.</li> <li>Failover - Similar to shared subscriptions, but multiple consumers can subscribe, and one actively receives messages.</li> </ul>"},{"location":"client_libraries/consumer/#example","title":"Example","text":"RustGo <pre><code>let topic = \"/default/test_topic\".to_string();\n\nlet mut consumer = client\n    .new_consumer()\n    .with_topic(topic.clone())\n    .with_consumer_name(\"test_consumer\")\n    .with_subscription(\"test_subscription\")\n    .with_subscription_type(SubType::Exclusive)\n    .build();\n\n// Subscribe to the topic\nlet consumer_id = consumer.subscribe().await?;\nprintln!(\"The Consumer with ID: {:?} was created\", consumer_id);\n\nlet _schema = client.get_schema(topic).await.unwrap();\n\n// Start receiving messages\nlet mut message_stream = consumer.receive().await?;\n\n while let Some(message) = message_stream.next().await {\n\n    //process the message and ack for receive\n\n }\n</code></pre> <pre><code>ctx := context.Background()\ntopic := \"/default/test_topic\"\nsubType := danube.Exclusive\n\nconsumer, err := client.NewConsumer(ctx).\n    WithConsumerName(\"test_consumer\").\n    WithTopic(topic).\n    WithSubscription(\"test_subscription\").\n    WithSubscriptionType(subType).\n    Build()\nif err != nil {\n    log.Fatalf(\"Failed to initialize the consumer: %v\", err)\n}\n\nconsumerID, err := consumer.Subscribe(ctx)\nif err != nil {\n    log.Fatalf(\"Failed to subscribe: %v\", err)\n}\nlog.Printf(\"The Consumer with ID: %v was created\", consumerID)\n\n// Receiving messages\nstreamClient, err := consumer.Receive(ctx)\nif err != nil {\n    log.Fatalf(\"Failed to receive messages: %v\", err)\n}\n\nfor {\n    msg, err := streamClient.Recv()\n\n   //process the message and ack for receive\n\n}\n</code></pre>"},{"location":"client_libraries/consumer/#complete-example","title":"Complete example","text":"<p>For a complete example implementation of the above code using producers and consumers, check the examples:</p> <ul> <li>Rust Examples</li> <li>Go Examples</li> </ul>"},{"location":"client_libraries/producer/","title":"Producer","text":"<p>A producer is a process that attaches to a topic and publishes messages to a Danube broker. The Danube broker processes the messages.</p> <p>Access Mode is a mechanism to determin the permissions of producers on topics.</p> <ul> <li>Shared - Multiple producers can publish on a topic.</li> <li>Exclusive - If there is already a producer connected, other producers trying to publish on this topic get errors immediately.</li> </ul>"},{"location":"client_libraries/producer/#example","title":"Example","text":"RustGo <pre><code>let topic = \"/default/test_topic\".to_string();\n\nlet json_schema = r#\"{\"type\": \"object\", \"properties\": {\"field1\": {\"type\": \"string\"}, \"field2\": {\"type\": \"integer\"}}}\"#.to_string();\n\nlet mut producer = client\n    .new_producer()\n    .with_topic(topic)\n    .with_name(\"test_producer1\")\n    .with_schema(\"my_app\".into(), SchemaType::Json(json_schema))\n    .build();\n\nlet prod_id = producer.create().await?;\ninfo!(\"The Producer was created with ID: {:?}\", prod_id);\n\nlet data = json!({\n        \"field1\": format!{\"value{}\", i},\n        \"field2\": 2020+i,\n    });\n\n// Convert to string and encode to bytes\nlet json_string = serde_json::to_string(&amp;data).unwrap();\nlet encoded_data = json_string.as_bytes().to_vec();\n\n// let json_message = r#\"{\"field1\": \"value\", \"field2\": 123}\"#.as_bytes().to_vec();\nlet message_id = producer.send(encoded_data, None).await?;\nprintln!(\"The Message with id {} was sent\", message_id);\n</code></pre> <pre><code>ctx := context.Background()\ntopic := \"/default/test_topic\"\njsonSchema := `{\"type\": \"object\", \"properties\": {\"field1\": {\"type\": \"string\"}, \"field2\": {\"type\": \"integer\"}}}`\n\nproducer, err := client.NewProducer(ctx).\n    WithName(\"test_producer\").\n    WithTopic(topic).\n    WithSchema(\"test_schema\", danube.SchemaType_JSON, jsonSchema).\n    Build()\nif err != nil {\n    log.Fatalf(\"unable to initialize the producer: %v\", err)\n}\n\nproducerID, err := producer.Create(ctx)\nif err != nil {\n    log.Fatalf(\"Failed to create producer: %v\", err)\n}\nlog.Printf(\"The Producer was created with ID: %v\", producerID)\n\ndata := map[string]interface{}{\n    \"field1\": fmt.Sprintf(\"value%d\", 24),\n    \"field2\": 2024,\n }\n\njsonData, err := json.Marshal(data)\nif err != nil {\n    log.Fatalf(\"Failed to marshal data: %v\", err)\n}\n\nmessageID, err := producer.Send(ctx, jsonData)\nif err != nil {\n    log.Fatalf(\"Failed to send message: %v\", err)\n}\n    log.Printf(\"The Message with id %v was sent\", messageID)\n</code></pre>"},{"location":"client_libraries/producer/#complete-example","title":"Complete example","text":"<p>For a complete example implementation of the above code using producers and consumers, check the examples:</p> <ul> <li>Rust Examples</li> <li>Go Examples</li> </ul>"},{"location":"client_libraries/setup/","title":"Client Setup","text":"<p>Before an application creates a producer/consumer, the  client library needs to initiate a setup phase including two steps:</p> <ul> <li>The client attempts to determine the owner of the topic by sending a Lookup request to Broker.  </li> <li>Once the client library has the broker address, it creates a RPC connection (or reuses an existing connection from the pool) and (in later stage authenticates it ).</li> <li>Within this connection, the clients (producer, consumer) and brokers exchange RPC commands. At this point, the client sends a command to create producer/consumer to the broker, which will comply after doing some validation checks.</li> </ul> RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    // Setup tracing\n    tracing_subscriber::fmt::init();\n\n    let client = DanubeClient::builder()\n        .service_url(\"http://[::1]:6650\")\n        .build()\n        .unwrap();\n}\n</code></pre> <pre><code>import \"github.com/danrusei/danube-go\"\n\nfunc main() {\n\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n}\n</code></pre>"},{"location":"client_libraries/setup/#refer-to-the-documentation","title":"Refer to the Documentation","text":"<p>For more details on how to use the library, including available methods and configuration options, refer to the docs.rs documentation.</p>"},{"location":"danube_clis/danube_admin/brokers/","title":"danube-admin: Brokers Commands","text":"<p>The <code>danube-admin</code> tool provides commands to manage and view information about brokers in your Danube cluster. Below is the documentation for the commands related to brokers.</p>"},{"location":"danube_clis/danube_admin/brokers/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/brokers/#danube-admin-brokers-list","title":"<code>danube-admin brokers list</code>","text":"<p>List all active brokers in the cluster.</p> <p>Usage:</p> <pre><code>danube-admin brokers list\n</code></pre> <p>Description:</p> <p>This command retrieves and displays a list of all active brokers in the cluster. The output is formatted into a table with the following columns:</p> <ul> <li>BROKER ID: The unique identifier for the broker.</li> <li>BROKER ADDRESS: The network address of the broker.</li> <li>BROKER ROLE: The role assigned to the broker (e.g., \"leader\", \"follower\").</li> </ul> <p>Example Output:</p> <pre><code>+------------+---------------------+-------------+\n| BROKER ID  | BROKER ADDRESS      | BROKER ROLE |\n+------------+---------------------+-------------+\n| 1          | 192.168.1.1:6650    | leader      |\n| 2          | 192.168.1.2:6650    | follower    |\n+------------+---------------------+-------------+\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#danube-admin-brokers-leader-broker","title":"<code>danube-admin brokers leader-broker</code>","text":"<p>Get information about the leader broker in the cluster.</p> <p>Usage:</p> <pre><code>danube-admin brokers leader-broker\n</code></pre> <p>Description:</p> <p>This command fetches and displays the details of the current leader broker in the cluster. The information includes the broker ID, address, and role of the leader.</p> <p>Example Output:</p> <pre><code>Leader Broker: BrokerId: 1, Address: 192.168.1.1:6650, Role: leader\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#danube-admin-brokers-namespaces","title":"<code>danube-admin brokers namespaces</code>","text":"<p>List all namespaces in the cluster.</p> <p>Usage:</p> <pre><code>danube-admin brokers namespaces\n</code></pre> <p>Description:</p> <p>This command retrieves and lists all namespaces associated with the cluster. Each namespace is printed on a new line.</p> <p>Example Output:</p> <pre><code>Namespace: default\nNamespace: public\nNamespace: my-namespace\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Make sure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"danube_clis/danube_admin/brokers/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List all brokers:</li> </ul> <pre><code>danube-admin brokers list\n</code></pre> <ul> <li>Get the leader broker:</li> </ul> <pre><code>danube-admin brokers leader-broker\n</code></pre> <ul> <li>List all namespaces:</li> </ul> <pre><code>danube-admin brokers namespaces\n</code></pre> <p>For more detailed information or help with the <code>danube-admin</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-admin brokers --help\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/","title":"danube-admin: Namespaces Commands","text":"<p>The <code>danube-admin</code> tool provides commands to manage and view information about namespaces in your Danube cluster. Below is the documentation for the commands related to namespaces.</p>"},{"location":"danube_clis/danube_admin/namespaces/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-namespaces-topics-namespace","title":"<code>danube-admin namespaces topics NAMESPACE</code>","text":"<p>Get the list of topics for a specified namespace.</p> <p>Usage:</p> <pre><code>danube-admin namespaces topics NAMESPACE\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all topics associated with a specific namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Example Output:</p> <pre><code>Topic: topic1\nTopic: topic2\nTopic: topic3\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-namespaces-policies-namespace","title":"<code>danube-admin namespaces policies NAMESPACE</code>","text":"<p>Get the configuration policies for a specified namespace.</p> <p>Usage:</p> <pre><code>danube-admin namespaces policies NAMESPACE\n</code></pre> <p>Description:</p> <p>This command fetches and displays the configuration policies for a specific namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Example Output:</p> <pre><code>Policy Name: policy1\nPolicy Description: Description of policy1\nPolicy Name: policy2\nPolicy Description: Description of policy2\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-namespaces-create-namespace","title":"<code>danube-admin namespaces create NAMESPACE</code>","text":"<p>Create a new namespace.</p> <p>Usage:</p> <pre><code>danube-admin namespaces create NAMESPACE\n</code></pre> <p>Description:</p> <p>This command creates a new namespace with the specified name. Replace <code>NAMESPACE</code> with the desired name for the new namespace.</p> <p>Example Output:</p> <pre><code>Namespace Created: true\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-namespaces-delete-namespace","title":"<code>danube-admin namespaces delete NAMESPACE</code>","text":"<p>Delete a specified namespace. The namespace must be empty.</p> <p>Usage:</p> <pre><code>danube-admin namespaces delete NAMESPACE\n</code></pre> <p>Description:</p> <p>This command deletes a namespace. The specified namespace must be empty before it can be deleted. Replace <code>NAMESPACE</code> with the name of the namespace you wish to delete.</p> <p>Example Output:</p> <pre><code>Namespace Deleted: true\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Make sure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"danube_clis/danube_admin/namespaces/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List all topics in a namespace:</li> </ul> <pre><code>danube-admin namespaces topics my-namespace\n</code></pre> <ul> <li>Get the policies for a namespace:</li> </ul> <pre><code>danube-admin namespaces policies my-namespace\n</code></pre> <ul> <li>Create a new namespace:</li> </ul> <pre><code>danube-admin namespaces create my-new-namespace\n</code></pre> <ul> <li>Delete a namespace:</li> </ul> <pre><code>danube-admin namespaces delete my-old-namespace\n</code></pre> <p>For more detailed information or help with the <code>danube-admin</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-admin namespaces --help\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/","title":"danube-admin: Topics Commands","text":"<p>The <code>danube-admin</code> tool provides commands to manage and view information about topics in your Danube cluster. Below is the documentation for the commands related to topics.</p>"},{"location":"danube_clis/danube_admin/topics/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/topics/#danube-admin-topics-list-namespace","title":"<code>danube-admin topics list NAMESPACE</code>","text":"<p>Get the list of topics in a specified namespace.</p> <p>Usage:</p> <pre><code>danube-admin topics list NAMESPACE\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all topics within a specified namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Example Output:</p> <pre><code>Topic: topic1\nTopic: topic2\nTopic: topic3\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-topics-create-topic","title":"<code>danube-admin topics create TOPIC</code>","text":"<p>Create a non-partitioned topic.</p> <p>Usage:</p> <pre><code>danube-admin topics create TOPIC\n</code></pre> <p>Description:</p> <p>This command creates a new non-partitioned topic with the specified name. Replace <code>TOPIC</code> with the desired name for the new topic.</p> <p>Example Output:</p> <pre><code>Topic Created: true\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-topics-delete-topic","title":"<code>danube-admin topics delete TOPIC</code>","text":"<p>Delete a specified topic.</p> <p>Usage:</p> <pre><code>danube-admin topics delete TOPIC\n</code></pre> <p>Description:</p> <p>This command deletes the specified topic. Replace <code>TOPIC</code> with the name of the topic you want to delete.</p> <p>Example Output:</p> <pre><code>Topic Deleted: true\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-topics-subscriptions-topic","title":"<code>danube-admin topics subscriptions TOPIC</code>","text":"<p>Get the list of subscriptions on a specified topic.</p> <p>Usage:</p> <pre><code>danube-admin topics subscriptions TOPIC\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all subscriptions associated with a specified topic. Replace <code>TOPIC</code> with the name of the topic you want to query.</p> <p>Example Output:</p> <pre><code>Subscriptions: [subscription1, subscription2]\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-topics-unsubscribe-subscription-subscription-topic","title":"<code>danube-admin topics unsubscribe --subscription SUBSCRIPTION TOPIC</code>","text":"<p>Delete a subscription from a topic.</p> <p>Usage:</p> <pre><code>danube-admin topics unsubscribe --subscription SUBSCRIPTION TOPIC\n</code></pre> <p>Description:</p> <p>This command deletes a subscription from a specified topic. Replace <code>SUBSCRIPTION</code> with the name of the subscription and <code>TOPIC</code> with the name of the topic.</p> <p>Example Output:</p> <pre><code>Unsubscribed: true\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Ensure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"danube_clis/danube_admin/topics/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List topics in a namespace:</li> </ul> <pre><code>danube-admin topics list my-namespace\n</code></pre> <ul> <li>Create a topic:</li> </ul> <pre><code>danube-admin topics create my-topic\n</code></pre> <ul> <li>Delete a topic:</li> </ul> <pre><code>danube-admin topics delete my-topic\n</code></pre> <ul> <li>Unsubscribe from a topic:</li> </ul> <pre><code>danube-admin topics unsubscribe --subscription my-subscription my-topic\n</code></pre> <ul> <li>List subscriptions for a topic:</li> </ul> <pre><code>danube-admin topics subscriptions my-topic\n</code></pre> <ul> <li>Create a new subscription for a topic:</li> </ul> <pre><code>danube-admin topics create-subscription --subscription my-subscription my-topic\n</code></pre> <p>For more detailed information or help with the <code>danube-admin</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-admin topics --help\n</code></pre>"},{"location":"danube_clis/danube_pubsub/consumer/","title":"Danube-Pubsub CLI - Consume messages","text":"<p>The <code>consume</code> command retrieves messages from a specified topic.</p>"},{"location":"danube_clis/danube_pubsub/consumer/#usage","title":"Usage","text":"<pre><code>danube-pubsub consume [OPTIONS] --service-addr &lt;SERVICE_ADDR&gt; --subscription &lt;SUBSCRIPTION&gt;\n</code></pre>"},{"location":"danube_clis/danube_pubsub/consumer/#options","title":"Options","text":"<ul> <li> <p><code>-s, --service-addr &lt;SERVICE_ADDR&gt;</code>   Description: The service URL for the Danube broker.   Example: <code>http://127.0.0.1:6650</code></p> </li> <li> <p><code>-t, --topic &lt;TOPIC&gt;</code>   Description: The topic to consume messages from.   If not specified: <code>/default/test_topic</code></p> </li> <li> <p><code>-c, --consumer &lt;CONSUMER&gt;</code>   Description: The consumer name.   If not specified: <code>consumer_pubsub</code></p> </li> <li> <p><code>-m, --subscription &lt;SUBSCRIPTION&gt;</code>   Description: The subscription name.   Required: Yes</p> </li> <li> <p><code>--sub-type &lt;SUB_TYPE&gt;</code>   Description: The subscription type.   Default: <code>shared</code>   Possible values: <code>exclusive</code>, <code>shared</code>, <code>fail-over</code></p> </li> <li> <p><code>-h, --help</code> Description: Print help information.</p> </li> </ul>"},{"location":"danube_clis/danube_pubsub/consumer/#example-shared-subscription","title":"Example: Shared Subscription","text":"<p>To receive messages from a shared subscription:</p> <pre><code>danube-pubsub consume -s http://127.0.0.1:6650 -m my_shared_subscription\n</code></pre>"},{"location":"danube_clis/danube_pubsub/consumer/#example-exclusive-subscription","title":"Example: Exclusive Subscription","text":"<p>To receive messages from an exclusive subscription:</p> <pre><code>danube-pubsub consume -s http://127.0.0.1:6650 -m my_exclusive --sub-type exclusive\n</code></pre> <p>To create a new exclusive subscription:</p> <pre><code>danube-pubsub consume -s http://127.0.0.1:6650 -m my_exclusive2 --sub-type exclusive\n</code></pre>"},{"location":"danube_clis/danube_pubsub/consumer/#resources","title":"Resources","text":"<p>Check this article that covers how to combine the subscription types in order to obtain message queueing or fan-out pub-sub messaging patterns.</p>"},{"location":"danube_clis/danube_pubsub/producer/","title":"Danube-Pubsub CLI - Produce messages","text":"<p>The <code>produce</code> command sends messages to a specified topic.</p>"},{"location":"danube_clis/danube_pubsub/producer/#usage","title":"Usage","text":"<pre><code>danube-pubsub produce [OPTIONS] --service-addr &lt;SERVICE_ADDR&gt; --message &lt;MESSAGE&gt;\n</code></pre>"},{"location":"danube_clis/danube_pubsub/producer/#options","title":"Options","text":"<ul> <li> <p><code>-s, --service-addr &lt;SERVICE_ADDR&gt;</code>   Description: The service URL for the Danube broker.   Example: <code>http://127.0.0.1:6650</code></p> </li> <li> <p><code>-n, --producer-name &lt;PRODUCER_NAME&gt;</code>   Description: The producer name .   If not specified: <code>test_producer</code></p> </li> <li> <p><code>-t, --topic &lt;TOPIC&gt;</code>   Description: The topic to produce messages to.   If not specified: <code>/default/test_topic</code></p> </li> <li> <p><code>-p, --partitions &lt;number&gt;</code>   Description: The number of topic partitions.   Default: <code>None</code></p> </li> <li> <p><code>-y, --schema &lt;SCHEMA&gt;</code>   Description: The schema type of the message.   Possible values: <code>bytes</code>, <code>string</code>, <code>int64</code>, <code>json</code></p> </li> <li> <p><code>-m, --message &lt;MESSAGE&gt;</code>   Description: The message to send.   Required: Yes</p> </li> <li> <p><code>--json-schema &lt;JSON_SCHEMA&gt;</code>   Description: The JSON schema, required if schema type is <code>json</code>.</p> </li> <li> <p><code>-c, --count &lt;COUNT&gt;</code>   Description: Number of times to send the message.   Default: <code>1</code></p> </li> <li> <p><code>-i, --interval &lt;INTERVAL&gt;</code>   Description: Interval between messages in milliseconds.   Default: <code>500</code>   Minimum: <code>100</code></p> </li> <li> <p><code>-a, --attributes &lt;ATTRIBUTES&gt;</code>   Description: Attributes in the form <code>'parameter:value'</code>.   Example: <code>'key1:value1,key2:value2'</code></p> </li> <li> <p><code>-h, --help</code>   Description: Print help information</p> </li> </ul>"},{"location":"danube_clis/danube_pubsub/producer/#example","title":"Example","text":"<p>To send 1000 messages with the content \"Hello, Danube!\" to the default topic:</p> <pre><code>danube-pubsub produce -s http://127.0.0.1:6650 -c 1000 -m \"Hello, Danube!\"\n</code></pre>"},{"location":"development/dev_environment/","title":"Development Environment Setup for Danube Broker","text":"<p>This document guides you through setting up the development environment, running danube broker instances, and be able to effectively contribute to the code.</p>"},{"location":"development/dev_environment/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following installed:</p> <ul> <li> <p>Rust: Ensure you have Rust installed. You can download and install it from the Rust website.</p> </li> <li> <p>Docker: Install Docker if you haven\u2019t already. Follow the installation instructions on the Docker website.</p> </li> </ul>"},{"location":"development/dev_environment/#contributing-to-the-repository","title":"Contributing to the Repository","text":"<ol> <li> <p>Fork the Repository:</p> </li> <li> <p>Go to the Danube Broker GitHub repository.</p> </li> <li> <p>Click the \"Fork\" button on the top right corner of the page to create your own copy of the repository.</p> </li> <li> <p>Clone Your Fork:</p> </li> </ol> <p>Once you have forked the repository, clone your forked repository:</p> <pre><code>git clone https://github.com/&lt;your-username&gt;/danube.git\ncd danube\n</code></pre> <ol> <li>Add the Original Repository as a Remote (optional but recommended for keeping up-to-date):</li> </ol> <pre><code>git remote add upstream https://github.com/danrusei/danube.git\n</code></pre>"},{"location":"development/dev_environment/#building-the-project","title":"Building the Project","text":"<ol> <li>Build the Project:</li> </ol> <p>To build the Danube Broker:</p> <pre><code>cargo build \nor  \ncargo build --release\n</code></pre>"},{"location":"development/dev_environment/#running-etcd","title":"Running ETCD","text":"<ol> <li>Start ETCD:</li> </ol> <p>Use the Makefile to start an ETCD instance. This will run ETCD in a Docker container.</p> <pre><code>make etcd\n</code></pre> <ol> <li>Clean Up ETCD:</li> </ol> <p>To stop and remove the ETCD instance and its data:</p> <pre><code>make etcd-clean\n</code></pre>"},{"location":"development/dev_environment/#running-a-single-broker-instance","title":"Running a Single Broker Instance","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. If not, use the <code>make etcd</code> command to start it.</p> <ol> <li>Run the Broker:</li> </ol> <p>Use the following command to start a single broker instance:</p> <pre><code>RUST_LOG=danube_broker=trace target/debug/danube-broker --cluster-name MY_cluster --meta-store-addr 127.0.0.1:2379\n</code></pre>"},{"location":"development/dev_environment/#running-multiple-broker-instances","title":"Running Multiple Broker Instances","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. Use:</p> <pre><code>make etcd\n</code></pre> <ol> <li>Run Multiple Brokers:</li> </ol> <p>Use the following Makefile command to start multiple broker instances:</p> <pre><code>make brokers\n</code></pre> <p>This will start brokers on ports 6650, 6651, and 6652. Logs for each broker will be saved in <code>temp/</code> directory.</p> <ol> <li>Clean Up Broker Instances:</li> </ol> <p>To stop all running broker instances:</p> <pre><code>make brokers-clean\n</code></pre>"},{"location":"development/dev_environment/#reading-logs","title":"Reading Logs","text":"<p>Logs for each broker instance are stored in the <code>temp/</code> directory. You can view them using:</p> <pre><code>cat temp/broker_&lt;port&gt;.log\n</code></pre> <p>Replace <code>&lt;port&gt;</code> with the actual port number (6650, 6651, or 6652).</p>"},{"location":"development/dev_environment/#inspecting-etcd-metadata","title":"Inspecting ETCD Metadata","text":"<ol> <li>Set Up <code>etcdctl</code>:</li> </ol> <p>Export the following environment variables:</p> <pre><code>export ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=http://localhost:2379\n</code></pre> <ol> <li>Inspect Metadata:</li> </ol> <p>Use <code>etcdctl</code> commands to inspect metadata. For example, to list all keys:</p> <pre><code>etcdctl get \"\" --prefix\n</code></pre> <p>To get a specific key:</p> <pre><code>etcdctl get &lt;key&gt;\n</code></pre>"},{"location":"development/dev_environment/#makefile-targets-summary","title":"Makefile Targets Summary","text":"<ul> <li><code>make etcd</code>: Starts an ETCD instance in Docker.</li> <li><code>make etcd-clean</code>: Stops and removes the ETCD instance and its data.</li> <li><code>make brokers</code>: Builds and starts broker instances on predefined ports.</li> <li><code>make brokers-clean</code>: Stops and removes all running broker instances.</li> </ul>"},{"location":"development/dev_environment/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>ETCD Not Starting: Check Docker logs and ensure no other service is using port 2379.</li> <li>Broker Not Starting: Ensure ETCD is running and accessible at the specified address and port.</li> </ul>"},{"location":"development/internal_resources/","title":"Resources mapping","text":"<p>This document describes how the resources are organized in the Metadata store</p>"},{"location":"development/internal_resources/#metadatastore-and-localcache","title":"MetadataStore and LocalCache","text":"<p>Basically the entire configuration and the metadata for all the cluster's objects (topics, namespaces, etc) are stored in MetadataStorage (ETCD) and in the LocalCache to ensure fast retrieval for the local broker and to reduce the number of request to the metadata database.</p> <p>The pattern:</p> <ul> <li>Put / Delete requests should use MetadataStore (ETCD), to ensure consistency across cluster</li> <li>Get requests should be served from the Local Cache</li> </ul> <p>The LocalCache continuously update from 2 sources for increase consistency:</p> <ul> <li>the Watch operation on ETCD</li> <li>the Syncronizer topic, where all Put / Delete requests are published and read by the brokers.</li> </ul>"},{"location":"development/internal_resources/#resources-types","title":"Resources Types","text":""},{"location":"development/internal_resources/#cluster-resources","title":"Cluster Resources","text":"<p>Holds information about the cluster and the cluster's brokers. Mainly read and write by Danube Service.</p> <ul> <li>/cluster/cluster-name</li> <li>holds a String with the name of the cluster</li> <li>/cluster/register/{broker-id}</li> <li>the broker register once it join the cluster, contain the broker metadata (broker id &amp; socket addr)  </li> <li>/cluster/brokers/{broker-id}/{namespace}/{topic}</li> <li>topics served by the broker, with value ()</li> <li>Load Manager updates the path, with topic assignments to brokers</li> <li>Brokers should watch it's own path like (/cluster/brokers/1122334455) - and perform the neccesary actions on adding or removing a topic</li> <li>/cluster/unassigned/{namespace}/{topic}</li> <li>New unassigned topics created by Broker</li> <li>Load Manager should watch this path, add assign the topic to a broker</li> <li>/cluster/load/{broker-id}</li> <li>broker periodically reports its load metrics on this path</li> <li>Load Manager watch this path to calculate broker load rankings for the cluster</li> <li>/cluster/load_balance</li> <li>the load_balance updated decision, posted by the Load Manager, contain a HashMap with keys the broker_id and value the list of topic_name</li> <li>/cluster/leader</li> <li>the value posted by Leader Election service, it holds broker_id of the current Leader of the CLuster</li> </ul> <p>Example:</p> <ul> <li>/cluster/brokers/1122334455/markets/trade-events - value is ()</li> <li>/cluster/brokers/1122334455/markets/trade-events - value is ()</li> </ul>"},{"location":"development/internal_resources/#namespace-resources","title":"Namespace Resources","text":"<p>Holds information about the namespace policy and the namespace's topics</p> <ul> <li>/namespaces/{namespace}/policy</li> <li>/namespaces/{namespace}/topics/{namespace}/{topic}</li> </ul> <p>Example:</p> <ul> <li>/namespaces/markets/policy - the value stores a Json like { \"retentionTimeInMinutes\": 1440 }</li> <li>/namespaces/markets/topics/markets/trade-events - topics part of the namespace, value is ()</li> </ul>"},{"location":"development/internal_resources/#topic-resources","title":"Topic Resources","text":"<p>Holds information about the topic policy and the associated producers / subscriptions, including partitioned topic.</p> <ul> <li>/topics/{namespace}/{topic}/policy</li> <li>holds the topic policy, the value stores a Json</li> <li>/topics/{namespace}/{topic}/schema</li> <li>holds the topic schema, the value stores the schema</li> <li>/topics/{namespace}/{topic}/producers/{producer_id}</li> <li>holds the producer config</li> <li>/topics/{namespace}/{topic}/subscriptions/{subscription_name}</li> <li>holds the subscription config</li> </ul> <p>Example:</p> <ul> <li>/topics/markets/trade-events/producers/1122334455 - with value Producer Metadata</li> <li>/topics/markets/trade-events/subscriptions/my_subscription - with value Subscription Metadata</li> <li>/topics/markets/trade-events-part-1/policy - where /markets/trade-events-part-1 is the partitioned topic that stores partition policy</li> </ul>"},{"location":"development/internal_resources/#subscriptions-resources","title":"Subscriptions Resources","text":"<p>Holds information about the topic subscriptions, including associated consumers</p> <ul> <li>/subscriptions/{subscription_name}/{consumer_id}</li> <li>holds the consumer metadata</li> </ul> <p>Example:</p> <ul> <li>/subscriptions/my_subscription/23232323</li> </ul>"},{"location":"getting_started/Danube_on_k8s_helm/","title":"Danube Cluster Instalation Guide on Kubernetes with Helm Chart","text":"<p>The Helm chart deploys the Danube Cluster with ETCD as metadata storage in the same namespace.</p>"},{"location":"getting_started/Danube_on_k8s_helm/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+</li> <li>Helm 3.2.0+</li> </ul>"},{"location":"getting_started/Danube_on_k8s_helm/#installation","title":"Installation","text":""},{"location":"getting_started/Danube_on_k8s_helm/#add-helm-repository","title":"Add Helm Repository","text":"<p>First, add the repository to your Helm client:</p> <pre><code>helm repo add danube https://danrusei.github.io/danube_helm\nhelm repo update\n</code></pre>"},{"location":"getting_started/Danube_on_k8s_helm/#install-the-helm-chart","title":"Install the Helm Chart","text":"<p>You can install the chart with the release name <code>my-danube-cluster</code> using the following command:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart\n</code></pre> <p>This will deploy the Danube Broker and an ETCD instance with the default configuration.</p>"},{"location":"getting_started/Danube_on_k8s_helm/#configuration","title":"Configuration","text":""},{"location":"getting_started/Danube_on_k8s_helm/#etcd-configuration","title":"ETCD Configuration","text":"<p>The following table lists the configurable parameters of the ETCD chart and their default values.</p> Parameter Description Default <code>etcd.enabled</code> Enable or disable ETCD deployment <code>true</code> <code>etcd.replicaCount</code> Number of ETCD instances <code>1</code> <code>etcd.image.repository</code> ETCD image repository <code>bitnami/etcd</code> <code>etcd.image.tag</code> ETCD image tag <code>latest</code> <code>etcd.image.pullPolicy</code> ETCD image pull policy <code>IfNotPresent</code> <code>etcd.service.type</code> ETCD service type <code>ClusterIP</code> <code>etcd.service.port</code> ETCD service port <code>2379</code>"},{"location":"getting_started/Danube_on_k8s_helm/#broker-configuration","title":"Broker Configuration","text":"<p>The following table lists the configurable parameters of the Danube Broker chart and their default values.</p> Parameter Description Default <code>broker.replicaCount</code> Number of broker instances <code>1</code> <code>broker.image.repository</code> Broker image repository <code>ghcr.io/your-username/danube-broker</code> <code>broker.image.tag</code> Broker image tag <code>latest</code> <code>broker.image.pullPolicy</code> Broker image pull policy <code>IfNotPresent</code> <code>broker.service.type</code> Broker service type <code>ClusterIP</code> <code>broker.service.port</code> Broker service port <code>6650</code> <code>broker.resources.limits.cpu</code> CPU limit for broker container <code>500m</code> <code>broker.resources.limits.memory</code> Memory limit for broker container <code>512Mi</code> <code>broker.resources.requests.cpu</code> CPU request for broker container <code>200m</code> <code>broker.resources.requests.memory</code> Memory request for broker container <code>256Mi</code> <code>broker.env.RUST_LOG</code> Rust log level for broker <code>danube_broker=trace</code> <code>broker.brokerAddr</code> Broker address <code>0.0.0.0:6650</code> <code>broker.clusterName</code> Cluster name <code>MY_CLUSTER</code> <code>broker.metaStoreAddr</code> Metadata store address <code>etcd:2379</code> <p>You can override the default values by providing a custom <code>values.yaml</code> file:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart -f custom-values.yaml\n</code></pre> <p>Alternatively, you can specify individual values using the <code>--set</code> flag:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart --set broker.replicaCount=2 --set broker.brokerAddr=\"0.0.0.0:6651\"\n</code></pre>"},{"location":"getting_started/Danube_on_k8s_helm/#accessing-the-brokers","title":"Accessing the Brokers","text":"<p>To access the broker service, you can use port forwarding:</p> <pre><code>kubectl port-forward svc/my-danube-cluster-broker 6650:6650\n</code></pre> <p>Then you can connect to the broker at <code>localhost:6650</code>.</p>"},{"location":"getting_started/Danube_on_k8s_helm/#uninstallation","title":"Uninstallation","text":"<p>To uninstall the <code>my-danube-cluster</code> release:</p> <pre><code>helm uninstall my-danube-cluster\n</code></pre> <p>This command removes all the Kubernetes components associated with the chart and deletes the release.</p>"},{"location":"getting_started/Danube_on_k8s_helm/#troubleshooting","title":"Troubleshooting","text":"<p>To get the status of the ETCD and Broker pods:</p> <pre><code>kubectl get pods -l app=etcd\nkubectl get pods -l app=broker\n</code></pre> <p>To view the logs of a specific broker pod:</p> <pre><code>kubectl logs &lt;broker-pod-name&gt;\n</code></pre>"},{"location":"getting_started/Danube_on_vms/","title":"Danube Installation Guide","text":"<p>Danube is an open-source distributed Pub/Sub messaging platform written in Rust. It relies on ETCD for persistent metadata storage. This guide will walk you through installing Danube brokers and ETCD.</p>"},{"location":"getting_started/Danube_on_vms/#prerequisites","title":"Prerequisites","text":"<p>ETCD: Follow the ETCD installation instructions to set up ETCD on a VM or bare metal server.</p> <p>Number of VMs: Recommended 4 Linux machines or VMs:</p> <ul> <li>One or more VMs for running ETCD</li> <li>3 VMs for running Danube brokers</li> </ul>"},{"location":"getting_started/Danube_on_vms/#installation-steps","title":"Installation Steps","text":""},{"location":"getting_started/Danube_on_vms/#1-install-etcd","title":"1. Install ETCD","text":"<ol> <li>Follow the ETCD installation guide to set up ETCD on a separate VM or server.</li> <li>Configuration: Ensure ETCD listens on port <code>2379</code> for client requests.</li> </ol>"},{"location":"getting_started/Danube_on_vms/#2-download-danube-broker","title":"2. Download Danube Broker","text":"<ol> <li>Visit the Danube releases page.</li> <li>Download the latest binary for Linux named <code>danube-broker</code>.</li> </ol>"},{"location":"getting_started/Danube_on_vms/#3-install-danube-brokers","title":"3. Install Danube Brokers","text":"<ol> <li> <p>Upload the <code>danube-broker</code> binary to each of the 3 VMs designated for brokers.</p> </li> <li> <p>Run the Broker: Start each broker with the appropriate configuration.    Example command to start a broker:</p> </li> </ol> <pre><code>RUST_LOG=danube_broker=info ./danube-broker --cluster-name \"MY_CLUSTER\" --meta-store-addr \"ETCD_SERVER_IP:2379\"\n</code></pre> <p>Replace <code>ETCD_SERVER_IP</code> with the IP address of your ETCD server.</p> <p>Repeat for each additional broker instance.</p>"},{"location":"getting_started/Danube_on_vms/#4-verify-the-setup","title":"4. Verify the Setup","text":"<ol> <li>ETCD: Ensure ETCD is running and accessible. You can check its status by accessing <code>http://&lt;ETCD_SERVER_IP&gt;:2379</code> from a browser or using <code>curl</code>:</li> </ol> <pre><code>curl http://&lt;ETCD_SERVER_IP&gt;:2379/v3/version\n</code></pre> <ol> <li>Danube Brokers: Ensure each broker instance is running and listening on the specified port. You can check this with <code>netstat</code> or <code>ss</code>:</li> </ol> <pre><code>netstat -tuln | grep 6650\n</code></pre> <p>Log Files: For debugging, check the logs of each Danube broker instance.</p>"},{"location":"getting_started/Danube_on_vms/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>ETCD Connection Issues: Ensure that the <code>--meta-store-addr</code> is correctly set and that ETCD is accessible from each broker VM.</li> <li>Broker Not Starting: Verify that the correct ports are available and that the Danube binary has execute permissions.</li> </ul>"},{"location":"getting_started/Danube_on_vms/#additional-information","title":"Additional Information","text":"<p>For more details, visit the Danube GitHub repository or contact the project maintainers for support.</p>"}]}