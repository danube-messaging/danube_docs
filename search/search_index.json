{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Danube Messaging","text":"<p>Danube is a lightweight, cloud\u2011native messaging platform built in Rust. It delivers sub\u2011second dispatch with cloud economics by combining a Write\u2011Ahead Log (WAL) with object storage, so you get low\u2011latency pub/sub and durable streaming\u2014on one broker.</p> <p>Danube enables one or many producers publish to topics, and multiple consumers receive messages via named subscriptions. Choose Non\u2011Reliable (best\u2011effort pub/sub) or Reliable (at\u2011least\u2011once streaming) per topic to match your workload.</p> <p>For design details, see the Architecture.</p>"},{"location":"#try-danube-in-minutes","title":"Try Danube in minutes","text":"<p>Docker Compose Quickstart: Use the provided Docker Compose setup with MinIO and ETCD.</p>"},{"location":"#danube-capabilities","title":"Danube capabilities","text":""},{"location":"#cluster-broker-characteristics","title":"\ud83c\udfd7\ufe0f Cluster &amp; Broker Characteristics","text":"<ul> <li>Stateless brokers: Metadata in ETCD and data in WAL/Object Storage</li> <li>Horizontal scaling: Add brokers in seconds; partitions rebalance automatically</li> <li>Leader election &amp; HA: Automatic failover and coordination via ETCD</li> <li>Rolling upgrades: Restart or replace brokers with minimal disruption</li> <li>Multi-tenancy: Isolated namespaces with policy controls</li> <li>Security-ready: TLS/mTLS support in Admin and data paths</li> </ul>"},{"location":"#schema-registry","title":"\ud83d\udccb Schema Registry","text":"<ul> <li>Centralized schema management: Single source of truth for message schemas across all topics</li> <li>Schema versioning: Automatic version tracking with compatibility enforcement</li> <li>Multiple formats: Bytes, String, Number, JSON Schema, Avro, Protobuf</li> <li>Validation &amp; governance: Prevent invalid messages and ensure data quality</li> </ul>"},{"location":"#cloud-native-by-design","title":"Cloud-Native by Design","text":"<p>Danube's architecture separates compute from storage, enabling:</p>"},{"location":"#write-ahead-log-cloud-persistence","title":"\ud83c\udf29\ufe0f Write-Ahead Log + Cloud Persistence","text":"<ul> <li>Sub-millisecond producer acknowledgments via local WAL</li> <li>Asynchronous background uploads to S3/GCS/Azure object storage</li> <li>Automatic failover with shared cloud state</li> <li>Infinite retention without local disk constraints</li> </ul>"},{"location":"#performance-scalability","title":"\u26a1 Performance &amp; Scalability","text":"<ul> <li>Hot path optimization: Messages served from in-memory WAL cache</li> <li>Stream per subscription: WAL + cloud storage from selected offset </li> <li>Multi-cloud support: AWS S3, Google Cloud Storage, Azure Blob, MinIO</li> </ul>"},{"location":"#core-concepts","title":"Core Concepts","text":"<p>Learn the fundamental concepts that power Danube messaging:</p> <p>Topics - Named channels for message streams</p> <ul> <li>Non\u2011partitioned: served by a single broker</li> <li>Partitioned: split across brokers for scale and HA</li> </ul> <p>Subscriptions - Named configurations for message delivery</p> <ul> <li><code>Exclusive</code>, <code>Shared</code>, <code>Failover</code> patterns for queueing and fan\u2011out</li> </ul> <p>Dispatch Strategies - Message delivery guarantees</p> <ul> <li><code>Non\u2011Reliable</code>: in\u2011memory, best\u2011effort delivery, lowest latency</li> <li><code>Reliable</code>: WAL + Cloud persistence with acknowledgments and replay</li> </ul> <p>Danube Stream Messages - Message structure </p> <p>Messaging Patterns</p> <ul> <li>Pub/Sub vs Streaming - Compare messaging modes</li> <li>Queuing vs Pub/Sub - Understand delivery patterns</li> </ul>"},{"location":"#architecture-deep-dives","title":"Architecture Deep Dives","text":"<p>Explore how Danube works under the hood:</p> <p>System Overview - Complete architecture diagram and component interaction</p> <p>Persistence (WAL + Cloud) - Two-tier storage architecture   - Write\u2011Ahead Log on local disk for fast durable writes   - Background uploads to object storage for durability and replay at cloud cost   - Seamless handoff from historical replay to live tail</p> <p>Schema Registry - Centralized schema management   - Schema versioning and compatibility checking   - Support for JSON Schema, Avro, and Protobuf   - Data validation and governance</p> <p>Internal Services - Service discovery and coordination</p>"},{"location":"#crates-in-the-workspace","title":"Crates in the workspace","text":"<p>Repository: https://github.com/danube-messaging/danube</p> <ul> <li>danube-broker \u2013 The broker service (topics, producers, consumers, subscriptions).</li> <li>danube-core \u2013 Core types, protocol, and shared logic.</li> <li>danube-metadata-store \u2013 Metadata storage and cluster coordination.</li> <li>danube-persistent-storage \u2013 WAL and cloud persistence backends.</li> </ul> <p>CLIs and client libraries:</p> <ul> <li>danube-client \u2013 Async Rust client library.</li> <li>danube-cli \u2013 Publish/consume client CLI.</li> <li>danube-admin-cli \u2013 Admin CLI for cluster management.</li> </ul>"},{"location":"#client-libraries","title":"Client libraries","text":"<ul> <li>danube-client (Rust)</li> <li>danube-go (Go)</li> </ul> <p>Contributions for other languages (Python, Java, etc.) are welcome.</p>"},{"location":"architecture/architecture/","title":"Danube Messaging Architecture","text":"<p>The Danube messaging system is a distributed messaging system, based on a publish-subscribe model, aiming to provide high throughput and low latency.</p> <p>The Danube Messaging system's architecture is designed for flexibility and scalability, making it suitable for event-driven and cloud-native applications. Its decoupled and pluggable architecture allows for independent scaling and easy integration of various storage backends. Using the dispatch strategies and the subscription models the system can accommodate different messaging patterns.</p> <p></p>"},{"location":"architecture/architecture/#brokers","title":"Brokers","text":"<p>Brokers are the core of the Danube Messaging system, responsible for routing and distributing messages, managing client connections and subscriptions, and implementing both reliable and non-reliable dispatch strategies. They act as the main entry point for publishers and subscribers, ensuring efficient and effective message flow.</p> <p>The Producers and Consumers connect to the Brokers to publish and consume messages, and use the subscription and dispatch mechanisms to accommodate various messaging patterns and reliability requirements.</p>"},{"location":"architecture/architecture/#metadata-storage","title":"Metadata Storage","text":"<p>The ETCD cluster serves as the metadata storage for the system by maintaining configuration data, topic information, and broker coordination and load-balancing, ensuring the entire system operates with high availability and consistent state management across all nodes.</p>"},{"location":"architecture/architecture/#storage-layer","title":"Storage Layer","text":"<p>The Storage Layer is responsible for message durability and replay. Danube now uses a cloud-native model based on a local Write-Ahead Log (WAL) for the hot path and background persistence to cloud object storage via OpenDAL. This keeps publish/dispatch latency low while enabling durable, elastic storage across providers (S3, GCS, Azure Blob, local FS, memory).</p> <p>Readers use tiered access: if data is within local WAL retention it is served from WAL/cache; otherwise historical data is streamed from cloud objects (using ETCD metadata) and seamlessly handed off to the WAL tail.</p>"},{"location":"architecture/architecture/#non-reliable-dispatch","title":"Non-Reliable Dispatch","text":"<p>Non-Reliable Dispatch operates with zero storage overhead, as messages flow directly from publishers to subscribers without intermediate persistence. This mode delivers maximum performance and lowest latency, making it ideal for scenarios where occasional message loss is acceptable, such as real-time metrics or live streaming data.</p>"},{"location":"architecture/architecture/#reliable-dispatch","title":"Reliable Dispatch","text":"<p>Reliable Dispatch offers guaranteed message delivery backed by the WAL + cloud persistence:</p> <ul> <li>WAL on local disk for low-latency appends and fast replay from in-memory cache and WAL files.</li> <li>Cloud object storage via OpenDAL (S3/GCS/Azure/FS/Memory) for durable, scalable historical data, uploaded asynchronously by a background uploader with resumable checkpoints.</li> <li>ETCD metadata tracks cloud objects and sparse indexes for efficient historical reads.</li> </ul> <p>The ability to choose between these dispatch modes gives users the flexibility to optimize their messaging infrastructure based on their specific requirements for performance, reliability, and resource utilization.</p> <p>For details and provider-specific configuration, see Persistence (WAL + Cloud).</p>"},{"location":"architecture/architecture/#design-considerations","title":"Design Considerations","text":""},{"location":"architecture/architecture/#decoupled-architecture","title":"Decoupled Architecture","text":"<p>The Danube Messaging system features a decoupled architecture where components are loosely coupled, allowing for independent scaling, easy maintenance and upgrades, and failure isolation.</p>"},{"location":"architecture/architecture/#plugin-architecture","title":"Plugin Architecture","text":"<p>With a plugin architecture, the system supports flexible storage backend options, making it easy to extend and customize according to different use cases. This adaptability ensures that the system can meet diverse application requirements and is cloud-native ready.</p>"},{"location":"architecture/architecture/#event-driven-focus","title":"Event-Driven Focus","text":"<p>Optimized for event-driven systems, the Danube Messaging system supports various message delivery patterns and scalable message processing. Its design is well-suited for microservices, providing efficient and scalable handling of event-driven workloads.</p>"},{"location":"architecture/internal_danube_services/","title":"Danube Cluster Services Role","text":"<p>This document enumerates the principal internal components of the Danube Broker.</p>"},{"location":"architecture/internal_danube_services/#danube-service-components","title":"Danube Service Components","text":"<p>The Broker owns the topics and manages their lifecycle. It also facilitates the creation of producers, subscriptions, and consumers, ensuring that producers can publish messages to topics and consumers can consume messages from topics.</p>"},{"location":"architecture/internal_danube_services/#leader-election-service","title":"Leader Election Service","text":"<p>The Leader Election Service selects one broker from the cluster to act as the Leader. The Broker Leader is responsible for making decisions. This service is used by the Load Manager, ensuring only one broker in the cluster posts the cluster aggregated Load Report.</p> <p>Leader Election Flow:</p> <ul> <li>The first broker registered in the cluster becomes the Leader by registering itself on \"/cluster/leader\".</li> <li>The field is registered with a lease, so the leader broker must periodically renew its lease to maintain leadership.</li> <li>Subsequent brokers attempt to become leaders but become Followers if the path is already in use.</li> <li>All brokers periodically check the leader path. If there is no change, the state is maintained; otherwise, brokers attempt to become the leader.</li> </ul>"},{"location":"architecture/internal_danube_services/#load-manager-service","title":"Load Manager Service","text":"<p>The Load Manager monitors and distributes load across brokers by managing topic and partition assignments. It implements rebalancing logic to redistribute topics/partitions when brokers join or leave the cluster and is responsible for failover mechanisms to handle broker failures.</p> <p>Load Manager Flow:</p> <ul> <li>All brokers periodically post their Load Reports on the path \"/cluster/brokers/load/{broker-id}\".</li> <li>The leader broker watches for load reports from all brokers in the cluster.</li> <li>It calculates rankings using the selected Load Balance algorithm.</li> <li>It posts its calculations for the cluster on the \"/cluster/load_balance\" path.</li> </ul> <p>Creation of a New Topic:</p> <ul> <li>A broker registers the Topic on the \"/cluster/unassigned\" path.</li> <li>The Load Manager of the leader Broker watches this path and assigns the broker with the least load to host the new topic by posting the topic to the \"/cluster/brokers/{broker-id}/{topic_name}\" path.</li> <li>Each broker watches its own path: \"/cluster/brokers/{broker-id}\". For any event on that path, such as the addition or deletion of topics, it acts accordingly by creating a new topic locally or deleting the topic it owned and all related resources.</li> <li>On topic creation, the broker checks if the topic already exists locally. If not, it retrieves all data about the topic, including subscriptions and producers, from the Local Metadata Cache.</li> <li>On topic removal, the broker handles the disconnections of producers and consumers and removes the locally allocated resources.</li> </ul> <p>For further consideration: We may want the broker to ask the Load Manager to get the next broker and initiate topic creation. Either it just posts the topic on the \"/cluster/unassigned\" path, or if it is the selected broker, it also creates the topic locally.</p>"},{"location":"architecture/internal_danube_services/#local-metadata-cache","title":"Local Metadata Cache","text":"<p>This cache stores various types of metadata required by Danube brokers, such as topic and namespace data, which are frequently accessed during message production and consumption. This reduces the need for frequent queries to the central metadata store, ETCD.</p> <p>The docs/internal_resources.md document describes how the resources are organized in the Metadata Store.</p> <p>Updates/events are received via ETCD Watch events and/or the metadata event synchronizer.</p>"},{"location":"architecture/internal_danube_services/#syncronizer","title":"Syncronizer","text":"<p>Not yet implemented.</p> <p>The synchronizer ensures that metadata and configuration settings across different brokers remain consistent. It propagates changes to metadata and configuration settings using client Producers and Consumers.</p> <p>This is in addition to Metadata Storage watch events, allowing brokers to process metadata updates even if there was a communication glitch or the broker was unavailable for a short period, potentially missing the Store Watch events. The synchronizer allows for dynamic updates to configuration settings without requiring a broker service restart.</p>"},{"location":"architecture/persistence/","title":"Danube Persistence Architecture (WAL + Cloud)","text":"<p>Danube's persistence layer has been recently revamped to make the platform cloud\u2011native, aiming for sub\u2011second dispatch with cloud economics. </p> <p>Danube uses a cloud\u2011native persistence architecture that combines a local Write\u2011Ahead Log (WAL) for the hot path with background uploads to cloud object storage. This keeps publish/dispatch latency low while providing durable, elastic, and cost\u2011effective storage in the cloud.</p> <p>This page explains the main concepts, how the system works, why it scales in cloud environments, and how to configure it for major providers (S3, GCS, Azure). Danube relies on OpenDAL as a storage abstraction, so it can be plugged into any storage backend.</p>"},{"location":"architecture/persistence/#key-benefits","title":"Key Benefits","text":"<ul> <li>Fast hot path: Producers append to a local WAL and consumers read from memory/WAL; no remote writes on the critical path.</li> <li>Cloud-native durability: A background uploader persists WAL frames to cloud object storage.</li> <li>Seamless reader experience: Readers transparently stream historical data from cloud, then live data from WAL.</li> <li>Cost-effective and elastic: Object storage provides massive scale, lifecycle policies, and low TCO.</li> <li>Portable by design: Built on OpenDAL, enabling S3, GCS, Azure Blob, local FS, memory, and more.</li> </ul>"},{"location":"architecture/persistence/#high-level-architecture","title":"High-level Architecture","text":"<ul> <li>Local WAL: Append-only log with an in-memory cache for ultra-fast reads. Files periodically fsync and rotate.</li> <li>Cloud Uploader: Periodically streams complete WAL frames to cloud objects. Writes object descriptors and sparse indexes to ETCD.</li> <li>Cloud Reader: Reads historical messages from cloud using ETCD metadata, then hands off to local WAL for live data.</li> <li>ETCD: Tracks object descriptors, offsets, and indexes for efficient range reads.</li> </ul>"},{"location":"architecture/persistence/#core-concepts-and-components","title":"Core Concepts and Components","text":"<p>WalStorageFactory creates per-topic <code>WalStorage</code>, starts one uploader and one deleter (retention) task per topic, and normalizes each topic to its own WAL directory under the configured root.</p> <p>Wal is an append-only log with an in-memory ordered cache; records are framed as <code>[u64 offset][u32 len][u32 crc][bytes]</code> with CRC32 validation, and a background writer batches and fsyncs data, supporting rotation by size and/or time.</p> <p>WalStorage implements the <code>PersistentStorage</code> trait; when creating readers it applies tiered logic to serve from WAL if possible, otherwise transparently chains a Cloud\u2192WAL stream to cover historical then live data.</p> <p>Uploader streams safe frame prefixes from WAL files to cloud, finalizes objects atomically, builds sparse offset indexes for efficient seeks, and operates one object per cycle with resumable checkpoints.</p> <p>CloudReader reads objects referenced in ETCD metadata, uses sparse indexes to seek efficiently to the requested range, and validates each frame by CRC during decoding.</p> <p>Retention/Deleter enforces time/size retention on WAL files after they are safely uploaded to cloud, advancing the WAL <code>start_offset</code> accordingly.</p> <p>For persistent storage implementation details, check the source code.</p>"},{"location":"architecture/persistence/#how-reads-work-cloudwal-handoff","title":"How Reads Work (Cloud\u2192WAL Handoff)","text":"<ol> <li>A subscription requests a reader at a <code>StartPosition</code> (latest or from a specific offset).</li> <li><code>WalStorage</code> checks the WAL\u2019s local <code>start_offset</code>.</li> <li>If <code>start_offset</code> is older than requested, it:    Streams historical range from cloud objects via <code>CloudReader</code>.    Seamlessly chains into WAL tail for fresh/live data.</li> <li>Consumers see a single ordered stream with no gaps or duplicates.</li> </ol>"},{"location":"architecture/persistence/#configuration-overview","title":"Configuration Overview","text":"<p>Broker configuration is under the <code>wal_cloud</code> section of <code>config/danube_broker.yml</code>:</p> <pre><code>wal_cloud:\n  wal:\n    dir: \"./danube-data/wal\"\n    file_name: \"wal.log\"\n    cache_capacity: 1024\n    file_sync:\n      interval_ms: 5000         # fsync cadence (checkpoint frequency)\n      max_batch_bytes: 10485760 # 10 MiB write batch\n    rotation:\n      max_bytes: 536870912      # 512 MiB WAL file size\n      # max_hours: 24           # optional time-based rotation\n    retention:\n      time_minutes: 2880        # prune locally after 48h\n      size_mb: 20480            # or when size exceeds 20 GiB\n      check_interval_minutes: 5 # deleter tick interval\n\n  uploader:\n    enabled: true\n    interval_seconds: 300       # one upload cycle every 5 minutes\n    root_prefix: \"/danube-data\"\n    max_object_mb: 1024         # optional cap per object\n\n  # Cloud storage backend - MinIO S3 configuration\n  cloud:\n    backend: \"s3\"\n    root: \"s3://danube-messages/cluster-data\"  # S3 bucket and prefix\n    region: \"us-east-1\"\n    endpoint: \"http://minio:9000\"     # MinIO endpoint\n    access_key: \"minioadmin\"         # From environment variable\n    secret_key: \"minioadmin123\"     # From environment variable\n    anonymous: false\n    virtual_host_style: false\n\n  metadata:\n    etcd_endpoint: \"127.0.0.1:2379\"\n    in_memory: false\n</code></pre> <ul> <li>WAL: local durability + replay cache for hot reads.</li> <li>Uploader: cadence and object sizing knobs.</li> <li>Cloud: provider/backend specific settings (see below).</li> <li>Metadata: ETCD endpoint used to store object descriptors and indexes.</li> </ul>"},{"location":"architecture/persistence/#provider-examples-opendal-powered","title":"Provider Examples (OpenDAL-powered)","text":"<p>Danube uses OpenDAL under the hood. Switch providers by changing <code>wal_cloud.cloud</code>.</p> <ul> <li>Amazon S3</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"s3\"\n    root: \"s3://my-bucket/prefix\"\n    region: \"us-east-1\"\n    endpoint: \"https://s3.us-east-1.amazonaws.com\"   # optional\n    access_key: \"${AWS_ACCESS_KEY_ID}\"               # or depend on env/IMDS\n    secret_key: \"${AWS_SECRET_ACCESS_KEY}\"\n    profile: null\n    role_arn: null\n    session_token: null\n    anonymous: false\n</code></pre> <ul> <li>Google Cloud Storage (GCS)</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"gcs\"\n    root: \"gcs://my-bucket/prefix\"\n    project: \"my-gcp-project\"\n    credentials_json: null                 # inline JSON string\n    credentials_path: \"/path/to/creds.json\" # or path to file\n</code></pre> <ul> <li>Azure Blob Storage</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"azblob\"\n    root: \"my-container/prefix\"                       # container[/prefix]\n    endpoint: \"https://&lt;account&gt;.blob.core.windows.net\" # or Azurite endpoint\n    account_name: \"&lt;account_name&gt;\"\n    account_key: \"&lt;account_key&gt;\"\n</code></pre> <ul> <li>In-memory (development)</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"memory\"\n    root: \"mem-prefix\" # namespace-only; persisted in memory\n</code></pre> <ul> <li>Local filesystem</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"fs\"\n    root: \"./object_root\"   # local directory for persisted objects\n</code></pre>"},{"location":"architecture/persistence/#tuning-and-best-practices","title":"Tuning and Best Practices","text":"<ul> <li>WAL cache size (<code>wal.cache_capacity</code>): increase for higher consumer hit rates; memory-bound.</li> <li>Flush cadence (<code>wal.file_sync.interval_ms</code>): smaller values improve checkpoint freshness but increase fsync pressure.</li> <li>Rotation thresholds (<code>wal.rotation.*</code>): tune file sizes for smoother uploads and operational hygiene.</li> <li>Uploader interval (<code>uploader.interval_seconds</code>): shorter intervals reduce RPO and speed up historical availability in cloud.</li> <li>Object size (<code>uploader.max_object_mb</code>): bigger objects reduce listing overhead; </li> <li>Retention: ensure local retention is safely larger than upload interval so deleter never prunes data not yet uploaded.</li> <li>Credentials: prefer environment-based credentials for cloud providers</li> </ul>"},{"location":"architecture/persistence/#operational-notes","title":"Operational Notes","text":"<ul> <li>At-least-once delivery: With reliable topics, dispatch uses the WAL to guarantee at-least-once delivery; cloud persistence is asynchronous and does not block producers/consumers.</li> <li>Resilience: Uploader uses precise checkpoints <code>(file_seq, byte_pos)</code> and never re-uploads confirmed bytes; CloudReader validates CRCs.</li> <li>Observability: Checkpoints and rotation metadata are stored under the per-topic WAL directory; ETCD keeps object descriptors.</li> <li>Extensibility: Because Danube uses OpenDAL, adding a new backend typically means adding backend-specific options in config; no broker code changes needed.</li> </ul>"},{"location":"architecture/schema_registry_architecture/","title":"Danube Schema Registry Architecture","text":"<p>Danube's Schema Registry is a centralized service that manages message schemas with versioning, compatibility checking, and validation capabilities. It ensures data quality, enables safe schema evolution, and provides a governance layer for all messages flowing through the Danube messaging system.</p> <p>This page explains what the Schema Registry is, why it's essential for Danube messaging systems, how it works at a high level, and how to interact with it through APIs and CLI tools.</p>"},{"location":"architecture/schema_registry_architecture/#what-is-a-schema-registry","title":"What is a Schema Registry?","text":"<p>A Schema Registry is a standalone service that stores, versions, and validates schemas, the contracts that define the structure of the messages. Instead of each topic managing its own schema independently, the Schema Registry provides:</p> <ul> <li>Centralized schema management - Single source of truth for all schemas across the messaging infrastructure</li> <li>Schema versioning - Track changes over time with automatic version management</li> <li>Compatibility enforcement - Prevent breaking changes that could crash consumers</li> <li>Schema reuse - Share schemas across multiple topics to reduce duplication</li> <li>Data governance - Audit who created schemas, when they changed, and track dependencies</li> </ul> <p>Think of it as a \"contract repository\" where producers and consumers agree on message structure, ensuring everyone speaks the same language.</p>"},{"location":"architecture/schema_registry_architecture/#key-benefits","title":"Key Benefits","text":"<p>\u2705 Data Quality Guarantee Every message is validated against its schema before being sent. Invalid messages are rejected, preventing data corruption from reaching consumers.</p> <p>\u2705 Safe Schema Evolution Compatibility checking ensures new schema versions don't break existing consumers. Add fields safely, knowing old clients will continue working.</p> <p>\u2705 Reduced Bandwidth Instead of sending the full schema with every message (potentially kilobytes), only a small schema ID (8 bytes) is transmitted.</p> <p>\u2705 Development Velocity Developers can evolve schemas confidently, knowing the system prevents breaking changes from reaching production.</p> <p>\u2705 Operational Insight Track all schema changes with full metadata: who created it, when, what changed, and which topics use it.</p> <p>\u2705 Industry Standard Follows the proven model from Confluent Schema Registry and Apache Pulsar, ensuring familiar patterns and easy migration.</p>"},{"location":"architecture/schema_registry_architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Producer   \u2502         \u2502  Schema Registry \u2502         \u2502   Consumer   \u2502\n\u2502             \u2502         \u2502                  \u2502         \u2502              \u2502\n\u2502 1. Register \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502  \u2022 Store schemas \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 4. Fetch     \u2502\n\u2502    Schema   \u2502         \u2502  \u2022 Version ctrl  \u2502         \u2502    Schema    \u2502\n\u2502             \u2502         \u2502  \u2022 Validate      \u2502         \u2502              \u2502\n\u2502 2. Get ID   \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2022 Check compat  \u2502         \u2502 5. Deserialize\u2502\n\u2502             \u2502         \u2502                  \u2502         \u2502    Messages   \u2502\n\u2502 3. Send msg \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502              \u2502\n\u2502  (with ID)  \u2502                  \u2502                   \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                         \u2502                          \u2502\n       \u2502                    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510                     \u2502\n       \u2502                    \u2502  ETCD   \u2502                     \u2502\n       \u2502                    \u2502Metadata \u2502                     \u2502\n       \u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n       \u2502                                                    \u2502\n       \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502  Danube Broker   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502                  \u2502\n                 \u2502 \u2022 Route messages \u2502\n                 \u2502 \u2022 Validate IDs   \u2502\n                 \u2502 \u2022 Enforce policy \u2502\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/schema_registry_architecture/#component-roles","title":"Component Roles","text":"<p>Schema Registry Service Standalone gRPC service managing all schema operations. Handles registration, retrieval, versioning, and compatibility checking independently from message routing.</p> <p>ETCD Metadata Store Persistent storage for all schema metadata, versions, and compatibility settings. Provides distributed consistency across broker cluster.</p> <p>Danube Broker Enforces schema validation on messages. Checks that message schema IDs match topic requirements before accepting or dispatching messages.</p> <p>Producers Register schemas, serialize messages according to schema, include schema ID in message metadata.</p> <p>Consumers Fetch schemas from registry, deserialize messages using correct schema version, optionally validate data structures.</p>"},{"location":"architecture/schema_registry_architecture/#core-concepts","title":"Core Concepts","text":""},{"location":"architecture/schema_registry_architecture/#subjects","title":"Subjects","text":"<p>A subject is a named container for schema versions. Typically, one subject corresponds to one message type (e.g., <code>user-events</code>, <code>payment-transactions</code>).</p> <ul> <li>Subjects enable multiple topics to share the same schema</li> <li>Subject names follow your naming conventions (commonly matches topic name)</li> <li>Each subject tracks its own version history and compatibility settings</li> </ul>"},{"location":"architecture/schema_registry_architecture/#versions","title":"Versions","text":"<p>Every time you register a schema, it creates a new version if the content differs from existing versions.</p> <ul> <li>Versions start at 1 and increment automatically</li> <li>Versions are immutable once created</li> <li>Full version history is preserved indefinitely</li> <li>Duplicate schemas are detected via fingerprinting (no duplicate versions created)</li> </ul>"},{"location":"architecture/schema_registry_architecture/#compatibility-modes","title":"Compatibility Modes","text":"<p>Compatibility modes control what schema changes are allowed when registering new versions:</p> Mode Description When to Use Example Change Backward New schema can read old data Consumers upgrade before producers Add optional field Forward Old schema can read new data Producers upgrade before consumers Remove optional field Full Both backward and forward Critical schemas needing both directions Only add optional fields None No validation Development/testing only Any change allowed <p>Default: Backward (industry standard, covers 90% of use cases)</p> <p>Each subject has its own compatibility mode, configurable independently.</p>"},{"location":"architecture/schema_registry_architecture/#schema-types","title":"Schema Types","text":"<p>Danube supports multiple schema formats:</p> <ul> <li>JSON Schema - Fully validated and production-ready</li> <li>Avro - Apache Avro format (registration and storage ready)</li> <li>Protobuf - Protocol Buffers (registration and storage ready)</li> <li>String - UTF-8 text validation</li> <li>Number - Numeric types</li> <li>Bytes - Raw binary (no validation)</li> </ul>"},{"location":"architecture/schema_registry_architecture/#how-it-works","title":"How It Works","text":""},{"location":"architecture/schema_registry_architecture/#1-schema-registration","title":"1. Schema Registration","text":"<p>When you register a schema:</p> <pre><code># Using CLI\ndanube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file user-events.json \\\n  --description \"User activity events\"\n</code></pre> <p>The Schema Registry:</p> <ol> <li>Validates the schema definition (syntax, structure)</li> <li>Checks compatibility with existing versions (if mode != None)</li> <li>Computes fingerprint to detect duplicates</li> <li>Assigns a unique schema ID (global across all subjects)</li> <li>Creates new version number (auto-increment)</li> <li>Stores in ETCD with full metadata (creator, timestamp, description, tags)</li> <li>Returns schema ID and version to client</li> </ol>"},{"location":"architecture/schema_registry_architecture/#2-schema-evolution","title":"2. Schema Evolution","text":"<p>When you update a schema:</p> <pre><code># Test compatibility first\ndanube-admin-cli schemas check user-events \\\n  --file user-events-v2.json \\\n  --schema-type json_schema\n\n# If compatible, register new version\ndanube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file user-events-v2.json\n</code></pre> <p>The compatibility checker:</p> <ol> <li>Retrieves latest version for the subject</li> <li>Compares old vs. new schema based on compatibility mode</li> <li>Validates the change is safe (e.g., adding optional field in Backward mode)</li> <li>Rejects incompatible changes with detailed error message</li> <li>If compatible, allows registration as new version</li> </ol>"},{"location":"architecture/schema_registry_architecture/#3-message-production","title":"3. Message Production","text":"<p>Producers reference schemas when sending messages:</p> <pre><code># Using CLI\ndanube-cli produce \\\n  -t /default/user-events \\\n  --schema-subject user-events \\\n  -m '{\"user_id\": \"123\", \"action\": \"login\"}'\n</code></pre> <p>The flow:</p> <ol> <li>Producer retrieves schema ID from registry (cached locally)</li> <li>Serializes message according to schema (validation happens client-side)</li> <li>Includes schema ID in message metadata (8 bytes overhead)</li> <li>Sends message to broker</li> <li>Broker validates schema ID matches topic requirements</li> <li>Message is accepted and routed to consumers</li> </ol>"},{"location":"architecture/schema_registry_architecture/#4-message-consumption","title":"4. Message Consumption","text":"<p>Consumers fetch schemas to deserialize messages:</p> <pre><code># Using CLI\ndanube-cli consume \\\n  -t /default/user-events \\\n  -m my-subscription\n</code></pre> <p>The flow:</p> <ol> <li>Consumer receives message with schema ID</li> <li>Fetches schema definition from registry (cached locally)</li> <li>Deserializes message using schema</li> <li>Optional: Validates deserialized data against struct definition</li> <li>Processes validated message</li> </ol>"},{"location":"architecture/schema_registry_architecture/#validation-layers","title":"Validation Layers","text":"<p>Danube provides three validation layers for maximum flexibility:</p>"},{"location":"architecture/schema_registry_architecture/#producer-side-validation","title":"Producer-Side Validation","text":"<ul> <li>Applications serialize data according to schema before sending</li> <li>Schema validation happens during serialization</li> <li>Catches errors at source before data enters system</li> <li>Recommended: Always validate at producer</li> </ul>"},{"location":"architecture/schema_registry_architecture/#broker-side-validation","title":"Broker-Side Validation","text":"<ul> <li>Broker checks message schema ID matches topic requirements</li> <li>Three policy levels: None, Warn, Enforce</li> <li>Enforce mode: Rejects invalid messages before routing</li> <li>Warn mode: Logs warnings but allows message (for monitoring)</li> <li>None mode: No validation (development only)</li> </ul>"},{"location":"architecture/schema_registry_architecture/#consumer-side-validation","title":"Consumer-Side Validation","text":"<ul> <li>Consumers deserialize messages using schema</li> <li>Optional struct validation at startup to ensure compatibility</li> <li>Prevents runtime deserialization errors</li> <li>Recommended: Validate structs at consumer startup</li> </ul> <p>This multi-layer approach ensures data quality at every stage while maintaining flexibility.</p>"},{"location":"architecture/schema_registry_architecture/#integration-points","title":"Integration Points","text":""},{"location":"architecture/schema_registry_architecture/#cli-tools","title":"CLI Tools","text":"<p>Admin CLI - Schema management operations:</p> <pre><code># Register schema\ndanube-admin-cli schemas register &lt;subject&gt; --file schema.json\n\n# Get schema\ndanube-admin-cli schemas get --subject &lt;subject&gt;\n\n# List versions\ndanube-admin-cli schemas versions &lt;subject&gt;\n\n# Check compatibility\ndanube-admin-cli schemas check &lt;subject&gt; --file new-schema.json\n\n# Set compatibility mode\ndanube-admin-cli schemas set-compatibility &lt;subject&gt; --mode backward\n\n# Delete version\ndanube-admin-cli schemas delete &lt;subject&gt; --version N --confirm\n</code></pre> <p>Danube CLI - Producer/Consumer with schemas:</p> <pre><code># Produce with schema\ndanube-cli produce -t /topic --schema-subject events -m '{\"data\": \"value\"}'\n\n# Consume with schema validation\ndanube-cli consume -t /topic -m subscription\n</code></pre>"},{"location":"architecture/schema_registry_architecture/#grpc-api","title":"gRPC API","text":"<p>The Schema Registry exposes 7 core operations via gRPC:</p> <ul> <li>RegisterSchema - Register new schema or version</li> <li>GetSchema - Retrieve schema by ID</li> <li>GetLatestSchema - Get newest version by subject</li> <li>ListVersions - Get all versions for subject</li> <li>CheckCompatibility - Validate schema changes</li> <li>DeleteSchemaVersion - Remove specific version</li> <li>SetCompatibilityMode - Configure compatibility rules</li> </ul> <p>All operations use standard gRPC with Protobuf definitions available in <code>danube-core/proto/SchemaRegistry.proto</code>.</p>"},{"location":"architecture/schema_registry_architecture/#client-sdk","title":"Client SDK","text":"<p>Type-safe Rust client for schema operations:</p> <pre><code>// Registration\nschema_client.register_schema(\"events\")\n    .with_type(SchemaType::JsonSchema)\n    .with_description(\"Event schema v1\")\n    .execute().await?;\n\n// Retrieval\nlet schema = schema_client.get_latest(\"events\").await?;\n\n// Compatibility check\nschema_client.check_compatibility(\"events\", new_schema).await?;\n</code></pre>"},{"location":"architecture/schema_registry_architecture/#storage-model","title":"Storage Model","text":""},{"location":"architecture/schema_registry_architecture/#etcd-organization","title":"ETCD Organization","text":"<p>Schemas are stored hierarchically in ETCD:</p> <pre><code>/schemas/\n  \u251c\u2500\u2500 {subject}/\n  \u2502   \u251c\u2500\u2500 metadata              # Subject-level metadata\n  \u2502   \u2502   \u251c\u2500\u2500 compatibility_mode\n  \u2502   \u2502   \u251c\u2500\u2500 created_at\n  \u2502   \u2502   \u2514\u2500\u2500 created_by\n  \u2502   \u2514\u2500\u2500 versions/\n  \u2502       \u251c\u2500\u2500 1                 # Version 1 data\n  \u2502       \u251c\u2500\u2500 2                 # Version 2 data\n  \u2502       \u2514\u2500\u2500 3                 # Version 3 data\n</code></pre>"},{"location":"architecture/schema_registry_architecture/#caching-strategy","title":"Caching Strategy","text":"<p>To optimize performance, Danube uses distributed caching:</p> <ul> <li>Writes go directly to ETCD (source of truth)</li> <li>Reads served from local cache (eventually consistent)</li> <li>Updates propagated via ETCD watch mechanism (automatic invalidation)</li> <li>Schema IDs cached in topics for fast validation (no registry lookup per message)</li> </ul> <p>This pattern ensures cluster-wide consistency while maintaining low-latency reads.</p>"},{"location":"architecture/schema_registry_architecture/#use-cases","title":"Use Cases","text":""},{"location":"architecture/schema_registry_architecture/#microservices-event-bus","title":"Microservices Event Bus","text":"<p>Share schemas across microservices to ensure contract compliance:</p> <ul> <li>User Service publishes <code>user-registered</code> events</li> <li>Email Service subscribes and validates against schema</li> <li>Analytics Service subscribes and validates against same schema</li> <li>CRM Service subscribes and validates against same schema</li> </ul> <p>All services share one schema subject, ensuring consistent message structure.</p>"},{"location":"architecture/schema_registry_architecture/#schema-evolution-during-upgrades","title":"Schema Evolution During Upgrades","text":"<p>Safely evolve schemas during rolling deployments:</p> <ol> <li>v1 Schema: <code>{user_id, action}</code></li> <li>Add optional field: <code>{user_id, action, email?}</code> \u2192 Backward compatible</li> <li>Deploy consumers first (can read old + new messages)</li> <li>Deploy producers second (send new format)</li> <li>All consumers upgraded \u2192 Make field required in v3 if needed</li> </ol>"},{"location":"architecture/schema_registry_architecture/#multi-environment-management","title":"Multi-Environment Management","text":"<p>Use different compatibility modes per environment:</p> <ul> <li>Development: <code>mode: none</code> - Fast iteration, no restrictions</li> <li>Staging: <code>mode: backward</code> - Test compatibility checks</li> <li>Production: <code>mode: full</code> - Strictest safety</li> </ul> <p>Same schemas, different governance levels based on environment needs.</p>"},{"location":"architecture/schema_registry_architecture/#data-governance-and-compliance","title":"Data Governance and Compliance","text":"<p>Track all schema changes with audit trail:</p> <ul> <li>Who created each schema version (created_by)</li> <li>When it was created (timestamp)</li> <li>What changed (description, tags)</li> <li>Which topics use which schemas</li> <li>Full version history for compliance audits</li> </ul>"},{"location":"architecture/schema_registry_architecture/#metrics-and-monitoring","title":"Metrics and Monitoring","text":"<p>The Schema Registry exposes Prometheus metrics for observability:</p> <p>Schema Validation Metrics:</p> <ul> <li><code>schema_validation_total</code> - Total validation attempts</li> <li><code>schema_validation_failures_total</code> - Failed validations</li> </ul> <p>Labels:</p> <ul> <li><code>topic</code> - Which topic validation occurred on</li> <li><code>policy</code> - Validation policy (Warn/Enforce)</li> <li><code>reason</code> - Failure reason (missing_schema_id, schema_mismatch)</li> </ul> <p>Use these metrics to:</p> <ul> <li>Monitor schema adoption across topics</li> <li>Track validation failure rates</li> <li>Alert on breaking changes or misconfigurations</li> <li>Measure impact of schema updates</li> </ul>"},{"location":"architecture/schema_registry_architecture/#summary","title":"Summary","text":"<p>The Danube Schema Registry transforms schema management from an ad-hoc, error-prone process into a robust, centralized governance layer. It enables:</p> <ul> <li>Safe evolution of message contracts without breaking consumers</li> <li>Data quality guarantees through multi-layer validation</li> <li>Operational visibility with full audit trails and versioning</li> <li>Developer confidence with compatibility checking and type safety</li> <li>Production readiness following industry-standard patterns</li> </ul> <p>By centralizing schema management, Danube ensures that all participants in the messaging infrastructure speak the same language, evolving together safely over time.</p>"},{"location":"client_libraries/clients/","title":"Danube Client Libraries","text":"<p>Danube provides official client libraries for multiple programming languages, allowing you to integrate messaging capabilities into your applications. All clients follow consistent patterns and support core Danube features including topics, subscriptions, partitions, and schema registry.</p>"},{"location":"client_libraries/clients/#supported-languages","title":"Supported Languages","text":""},{"location":"client_libraries/clients/#rust-client","title":"Rust Client","text":"<p>The official danube-client is an asynchronous Rust client library built on Tokio.</p> <p>Installation:</p> <pre><code>cargo add danube-client\n</code></pre> <p>Features:</p> <ul> <li>\u2705 Full async/await support with Tokio</li> <li>\u2705 Type-safe schema registry integration</li> <li>\u2705 Partitioned topics</li> <li>\u2705 Reliable dispatch</li> <li>\u2705 TLS and JWT authentication</li> <li>\u2705 All subscription types (Exclusive, Shared, Failover)</li> <li>\u2705 Schema validation (JSON Schema, Avro, Protobuf)</li> </ul> <p>Learn more: Rust Examples</p>"},{"location":"client_libraries/clients/#go-client","title":"Go Client","text":"<p>The official danube-go library provides Go language bindings.</p> <p>Installation:</p> <pre><code>go get github.com/danube-messaging/danube-go\n</code></pre> <p>Features:</p> <ul> <li>\u2705 Context-based operations</li> <li>\u2705 Partitioned topics</li> <li>\u2705 Reliable dispatch</li> <li>\u2705 All subscription types (Exclusive, Shared, Failover)</li> <li>\u23f3 TLS support (coming soon)</li> <li>\u23f3 Schema registry (coming soon)</li> </ul> <p>Learn more: Go Examples</p>"},{"location":"client_libraries/clients/#feature-comparison-matrix","title":"Feature Comparison Matrix","text":"Feature Rust Go Python* Java* Core Messaging Producers \u2705 \u2705 \u23f3 \u23f3 Consumers \u2705 \u2705 \u23f3 \u23f3 Partitioned Topics \u2705 \u2705 \u23f3 \u23f3 Reliable Dispatch \u2705 \u2705 \u23f3 \u23f3 Subscriptions Exclusive \u2705 \u2705 \u23f3 \u23f3 Shared \u2705 \u2705 \u23f3 \u23f3 Failover \u2705 \u2705 \u23f3 \u23f3 Schema Registry JSON Schema \u2705 \u23f3 \u23f3 \u23f3 Avro \u2705 \u23f3 \u23f3 \u23f3 Protobuf \u2705 \u23f3 \u23f3 \u23f3 Compatibility Checking \u2705 \u23f3 \u23f3 \u23f3 Security TLS \u2705 \u23f3 \u23f3 \u23f3 JWT Authentication \u2705 \u23f3 \u23f3 \u23f3 <p>* Coming soon - community contributions welcome</p>"},{"location":"client_libraries/clients/#community-clients","title":"Community Clients","text":"<p>We encourage the community to develop and maintain clients for additional languages. If you're building a Danube client:</p> <ul> <li>Follow the protocol specification</li> <li>Reference existing clients for patterns</li> <li>Submit a PR to add your client to this list</li> </ul>"},{"location":"client_libraries/clients/#guidelines-for-client-development","title":"Guidelines for Client Development","text":"<p>Core Requirements:</p> <ul> <li>Support for producer/consumer operations</li> <li>Schema registry integration</li> <li>Topic lookup and partitioning</li> <li>Subscription management (Exclusive, Shared, Failover)</li> <li>Message acknowledgment</li> <li>Error handling and retries</li> </ul> <p>Recommended Features:</p> <ul> <li>TLS support</li> <li>JWT authentication</li> <li>Connection pooling</li> <li>Graceful shutdown</li> </ul>"},{"location":"client_libraries/clients/#next-steps","title":"Next Steps","text":"<ul> <li>Client Setup - Configure and connect your client</li> <li>Producer Guide - Send messages to topics</li> <li>Consumer Guide - Receive and process messages</li> <li>Schema Registry - Work with typed messages</li> </ul>"},{"location":"client_libraries/clients/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Danube Docs</li> <li>Examples: Rust | Go</li> <li>Issues: GitHub Issues</li> </ul>"},{"location":"client_libraries/consumer-advanced/","title":"Advanced Consumer Features","text":"<p>This guide covers advanced consumer capabilities including partitioned topics, multiple consumers, and integration with schemas.</p>"},{"location":"client_libraries/consumer-advanced/#consuming-from-partitioned-topics","title":"Consuming from Partitioned Topics","text":"<p>When a topic has partitions, consumers automatically receive from all partitions.</p>"},{"location":"client_libraries/consumer-advanced/#automatic-partition-handling","title":"Automatic Partition Handling","text":"RustGo <pre><code>use danube_client::{DanubeClient, SubType};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    // Topic has 3 partitions: my-topic-part-0, my-topic-part-1, my-topic-part-2\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/my-topic\")  // Parent topic name\n        .with_consumer_name(\"partition-consumer\")\n        .with_subscription(\"partition-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n    println!(\"\u2705 Subscribed to all partitions\");\n\n    // Automatically receives from all 3 partitions\n    let mut message_stream = consumer.receive().await?;\n\n    while let Some(message) = message_stream.recv().await {\n        println!(\"\ud83d\udce5 Received from partition: {}\", message.payload);\n        consumer.ack(&amp;message).await?;\n    }\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    consumer, err := client.NewConsumer(ctx).\n        WithConsumerName(\"partition-consumer\").\n        WithTopic(\"/default/my-topic\").  // Parent topic\n        WithSubscription(\"partition-sub\").\n        WithSubscriptionType(danube.Exclusive).\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create consumer: %v\", err)\n    }\n\n    if err := consumer.Subscribe(ctx); err != nil {\n        log.Fatalf(\"Failed to subscribe: %v\", err)\n    }\n    fmt.Println(\"\u2705 Subscribed to all partitions\")\n\n    stream, err := consumer.Receive(ctx)\n    if err != nil {\n        log.Fatalf(\"Failed to receive: %v\", err)\n    }\n\n    for msg := range stream {\n        fmt.Printf(\"\ud83d\udce5 Received from partition: %s\\n\", string(msg.GetPayload()))\n        consumer.Ack(ctx, msg)\n    }\n}\n</code></pre> <p>What happens:</p> <ul> <li>Client discovers all partitions automatically</li> <li>Creates one consumer per partition internally</li> <li>Messages from all partitions merged into single stream</li> <li>Ordering preserved per-partition, not cross-partition</li> </ul>"},{"location":"client_libraries/consumer-advanced/#shared-subscription-patterns","title":"Shared Subscription Patterns","text":""},{"location":"client_libraries/consumer-advanced/#load-balancing-across-consumers","title":"Load Balancing Across Consumers","text":"<p>With Shared subscription type, multiple consumers can subscribe to the same topic with the same subscription name. Messages are distributed round-robin across all active consumers.</p> <p>How it works:</p> <ol> <li>Deploy multiple consumer instances with:</li> <li>Same topic: <code>/default/work-queue</code></li> <li>Same subscription: <code>\"work-sub\"</code></li> <li>Subscription type: <code>SubType::Shared</code></li> <li> <p>Different consumer names: <code>\"worker-1\"</code>, <code>\"worker-2\"</code>, etc.</p> </li> <li> <p>Broker distributes messages:</p> </li> <li>Message 1 \u2192 Worker 1</li> <li>Message 2 \u2192 Worker 2  </li> <li>Message 3 \u2192 Worker 3</li> <li>Message 4 \u2192 Worker 1 (round-robin continues)</li> </ol> <p>Benefits:</p> <ul> <li>\u2705 Horizontal scaling - add more consumers to increase throughput</li> <li>\u2705 Load distribution - work shared across all consumers</li> <li>\u2705 Dynamic scaling - add/remove workers without coordination</li> <li>\u2705 High throughput - parallel processing</li> </ul> <p>Trade-offs:</p> <ul> <li>\u274c No ordering guarantee - messages may be processed out of order</li> <li>\u274c No affinity - same entity may go to different consumers</li> </ul> <p>Use cases:</p> <ul> <li>Log processing pipelines</li> <li>Analytics workloads</li> <li>Image processing queues</li> <li>Any workload where order doesn't matter</li> </ul>"},{"location":"client_libraries/consumer-advanced/#failover-pattern","title":"Failover Pattern","text":""},{"location":"client_libraries/consumer-advanced/#high-availability-setup","title":"High Availability Setup","text":"<p>With Failover subscription type, multiple consumers can subscribe to the same topic, but only one is active at a time. The others remain in standby. If the active consumer fails, the broker automatically promotes a standby consumer.</p> <p>How it works:</p> <ol> <li>Deploy multiple consumer instances with:</li> <li>Same topic: <code>/default/critical-orders</code></li> <li>Same subscription: <code>\"order-processor\"</code></li> <li>Subscription type: <code>SubType::FailOver</code></li> <li> <p>Different consumer names: <code>\"processor-1\"</code>, <code>\"processor-2\"</code>, etc.</p> </li> <li> <p>Broker manages active consumer:</p> </li> <li>First connected consumer becomes active (receives messages)</li> <li>Other consumers remain in standby (no messages received)</li> <li>If active disconnects/fails, broker promotes next standby instantly</li> <li>New active continues from last acknowledged message</li> </ol> <p>Behavior:</p> <ul> <li>\u2705 Only one consumer active at a time</li> <li>\u2705 Automatic failover - no manual intervention</li> <li>\u2705 Message ordering preserved (single active consumer)</li> <li>\u2705 High availability - standbys ready to take over</li> <li>\u2705 Zero message loss - standby resumes from last ack</li> </ul> <p>Use cases:</p> <ul> <li>Critical order processing (needs ordering + HA)</li> <li>Financial transactions</li> <li>State machine workflows</li> <li>Any workload requiring both ordering and high availability</li> </ul>"},{"location":"client_libraries/consumer-advanced/#schema-integration","title":"Schema Integration","text":"<p>Consume typed messages validated against schemas (see Schema Registry for details).</p> <p>Note: Schema Registry integration is not yet available in the Go client.</p>"},{"location":"client_libraries/consumer-advanced/#basic-schema-consumption","title":"Basic Schema Consumption","text":"Rust <pre><code>use danube_client::{DanubeClient, SubType};\nuse serde::Deserialize;\n\n#[derive(Deserialize, Debug)]\nstruct Event {\n    event_id: String,\n    timestamp: i64,\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/events\")\n        .with_consumer_name(\"event-consumer\")\n        .with_subscription(\"event-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n\n    let mut stream = consumer.receive().await?;\n\n    while let Some(message) = stream.recv().await {\n        // Deserialize JSON message\n        match serde_json::from_slice::&lt;Event&gt;(&amp;message.payload) {\n            Ok(event) =&gt; {\n                println!(\"\ud83d\udce5 Event: {:?}\", event);\n                consumer.ack(&amp;message).await?;\n            }\n            Err(e) =&gt; {\n                eprintln!(\"\u274c Deserialization failed: {}\", e);\n                // Don't ack invalid messages\n            }\n        }\n    }\n\n    Ok(())\n}\n</code></pre>"},{"location":"client_libraries/consumer-advanced/#validated-schema-consumption","title":"Validated Schema Consumption","text":"<p>Validate your Rust struct against the registered schema at startup to catch incompatibilities before processing messages (Rust only):</p> Rust <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SubType};\nuse serde::{Deserialize, Serialize};\nuse jsonschema::JSONSchema;\n\n#[derive(Deserialize, Serialize, Debug)]\nstruct MyMessage {\n    field1: String,\n    field2: i32,\n}\n\n/// Validates that consumer struct matches the schema in the registry\nasync fn validate_struct_against_registry&lt;T: Serialize&gt;(\n    schema_client: &amp;mut SchemaRegistryClient,\n    subject: &amp;str,\n    sample: &amp;T,\n) -&gt; Result&lt;u32, Box&lt;dyn std::error::Error&gt;&gt; {\n    println!(\"\ud83d\udd0d Fetching schema from registry: {}\", subject);\n\n    let schema_response = schema_client.get_latest_schema(subject).await?;\n    println!(\"\ud83d\udccb Schema version: {}\", schema_response.version);\n\n    // Parse schema definition\n    let schema_def: serde_json::Value = \n        serde_json::from_slice(&amp;schema_response.schema_definition)?;\n\n    // Compile JSON Schema validator\n    let validator = JSONSchema::compile(&amp;schema_def)\n        .map_err(|e| format!(\"Invalid schema: {}\", e))?;\n\n    // Validate sample struct\n    let sample_json = serde_json::to_value(sample)?;\n\n    if let Err(errors) = validator.validate(&amp;sample_json) {\n        eprintln!(\"\u274c VALIDATION FAILED: Struct incompatible with schema v{}\", \n            schema_response.version);\n        for error in errors {\n            eprintln!(\"   - {}\", error);\n        }\n        return Err(\"Struct validation failed\".into());\n    }\n\n    println!(\"\u2705 Struct validated against schema v{}\", schema_response.version);\n    Ok(schema_response.version)\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\n    // VALIDATE BEFORE CONSUMING - fails fast if struct is wrong!\n    let schema_version = validate_struct_against_registry(\n        &amp;mut schema_client,\n        \"my-app-events\",\n        &amp;MyMessage {\n            field1: \"test\".to_string(),\n            field2: 0,\n        },\n    ).await?;\n\n    println!(\"\u2705 Consumer validated - safe to deserialize\\n\");\n\n    // Now create consumer\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/test_topic\")\n        .with_consumer_name(\"validated-consumer\")\n        .with_subscription(\"validated-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n    let mut stream = consumer.receive().await?;\n\n    while let Some(message) = stream.recv().await {\n        match serde_json::from_slice::&lt;MyMessage&gt;(&amp;message.payload) {\n            Ok(msg) =&gt; {\n                println!(\"\ud83d\udce5 Message: {:?}\", msg);\n                consumer.ack(&amp;message).await?;\n            }\n            Err(e) =&gt; {\n                eprintln!(\"\u274c Deserialization failed: {}\", e);\n                eprintln!(\"   Schema drift detected - check version {}\", schema_version);\n                // Don't ack - message will retry or go to DLQ\n            }\n        }\n    }\n\n    Ok(())\n}\n</code></pre> <p>Why validate at startup?</p> <ul> <li>\u2705 Fail fast - catch schema mismatches before processing messages</li> <li>\u2705 Clear errors - know exactly which fields don't match</li> <li>\u2705 Prevent runtime failures - no surprises during message processing</li> <li>\u2705 Safe deployments - validates before going live</li> </ul> <p>Note: Requires <code>jsonschema</code> crate dependency.</p>"},{"location":"client_libraries/consumer-advanced/#performance-tuning","title":"Performance Tuning","text":""},{"location":"client_libraries/consumer-advanced/#batch-processing","title":"Batch Processing","text":"<p>Process messages in batches for efficiency:</p> Rust <pre><code>use tokio::time::{timeout, Duration};\n\nlet mut stream = consumer.receive().await?;\nlet mut batch = Vec::new();\nlet batch_size = 100;\nlet batch_timeout = Duration::from_millis(500);\n\nloop {\n    match timeout(batch_timeout, stream.recv()).await {\n        Ok(Some(message)) =&gt; {\n            batch.push(message);\n\n            if batch.len() &gt;= batch_size {\n                // Process batch\n                process_batch(&amp;batch).await?;\n\n                // Ack all\n                for msg in &amp;batch {\n                    consumer.ack(msg).await?;\n                }\n\n                batch.clear();\n            }\n        }\n        Ok(None) =&gt; break,  // Stream closed\n        Err(_) =&gt; {\n            // Timeout - process partial batch\n            if !batch.is_empty() {\n                process_batch(&amp;batch).await?;\n                for msg in &amp;batch {\n                    consumer.ack(msg).await?;\n                }\n                batch.clear();\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"client_libraries/consumer-basics/","title":"Consumer Basics","text":"<p>Consumers receive messages from Danube topics via subscriptions. This guide covers fundamental consumer operations.</p>"},{"location":"client_libraries/consumer-basics/#creating-a-consumer","title":"Creating a Consumer","text":""},{"location":"client_libraries/consumer-basics/#simple-consumer","title":"Simple Consumer","text":"<p>The minimal setup to receive messages:</p> RustGo <pre><code>use danube_client::{DanubeClient, SubType};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/my-topic\")\n        .with_consumer_name(\"my-consumer\")\n        .with_subscription(\"my-subscription\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n    println!(\"\u2705 Consumer subscribed\");\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    consumer, err := client.NewConsumer(ctx).\n        WithConsumerName(\"my-consumer\").\n        WithTopic(\"/default/my-topic\").\n        WithSubscription(\"my-subscription\").\n        WithSubscriptionType(danube.Exclusive).\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create consumer: %v\", err)\n    }\n\n    if err := consumer.Subscribe(ctx); err != nil {\n        log.Fatalf(\"Failed to subscribe: %v\", err)\n    }\n\n    fmt.Println(\"\u2705 Consumer subscribed\")\n}\n</code></pre> <p>Key concepts:</p> <ul> <li>Topic: Source of messages</li> <li>Consumer Name: Unique identifier for this consumer instance</li> <li>Subscription: Logical grouping of consumers (multiple consumers can share)</li> <li>Subscription Type: Controls how messages are distributed (Exclusive, Shared, Failover)</li> </ul>"},{"location":"client_libraries/consumer-basics/#subscription-types","title":"Subscription Types","text":""},{"location":"client_libraries/consumer-basics/#exclusive","title":"Exclusive","text":"<p>Only one consumer can be active. Guarantees message order.</p> RustGo <pre><code>use danube_client::SubType;\n\nlet mut consumer = client\n    .new_consumer()\n    .with_topic(\"/default/orders\")\n    .with_consumer_name(\"order-processor\")\n    .with_subscription(\"order-sub\")\n    .with_subscription_type(SubType::Exclusive)  // Only this consumer\n    .build();\n</code></pre> <pre><code>consumer, err := client.NewConsumer(ctx).\n    WithConsumerName(\"order-processor\").\n    WithTopic(\"/default/orders\").\n    WithSubscription(\"order-sub\").\n    WithSubscriptionType(danube.Exclusive).\n    Build()\nif err != nil {\n    log.Fatalf(\"Failed to create consumer: %v\", err)\n}\n</code></pre> <p>Characteristics:</p> <ul> <li>\u2705 Message ordering guaranteed</li> <li>\u2705 Simple failure handling</li> <li>\u274c No horizontal scaling</li> <li>Use case: Order processing, sequential workflows</li> </ul>"},{"location":"client_libraries/consumer-basics/#shared","title":"Shared","text":"<p>Multiple consumers receive messages in round-robin. Scales horizontally.</p> RustGo <pre><code>use danube_client::SubType;\n\nlet mut consumer = client\n    .new_consumer()\n    .with_topic(\"/default/logs\")\n    .with_consumer_name(\"log-processor-1\")\n    .with_subscription(\"log-sub\")\n    .with_subscription_type(SubType::Shared)  // Multiple consumers allowed\n    .build();\n</code></pre> <pre><code>consumer, err := client.NewConsumer(ctx).\n    WithConsumerName(\"log-processor-1\").\n    WithTopic(\"/default/logs\").\n    WithSubscription(\"log-sub\").\n    WithSubscriptionType(danube.Shared).\n    Build()\nif err != nil {\n    log.Fatalf(\"Failed to create consumer: %v\", err)\n}\n</code></pre> <p>Characteristics:</p> <ul> <li>\u2705 Horizontal scaling</li> <li>\u2705 Load distribution</li> <li>\u274c No ordering guarantee</li> <li>Use case: Log processing, analytics, parallel processing</li> </ul>"},{"location":"client_libraries/consumer-basics/#failover","title":"Failover","text":"<p>Like Exclusive, but allows standby consumers. One active, others wait.</p> RustGo <pre><code>use danube_client::SubType;\n\nlet mut consumer = client\n    .new_consumer()\n    .with_topic(\"/default/critical\")\n    .with_consumer_name(\"processor-1\")\n    .with_subscription(\"critical-sub\")\n    .with_subscription_type(SubType::FailOver)  // Hot standby\n    .build();\n</code></pre> <pre><code>consumer, err := client.NewConsumer(ctx).\n    WithConsumerName(\"processor-1\").\n    WithTopic(\"/default/critical\").\n    WithSubscription(\"critical-sub\").\n    WithSubscriptionType(danube.FailOver).\n    Build()\nif err != nil {\n    log.Fatalf(\"Failed to create consumer: %v\", err)\n}\n</code></pre> <p>Characteristics:</p> <ul> <li>\u2705 Message ordering guaranteed</li> <li>\u2705 Automatic failover to standby</li> <li>\u2705 High availability</li> <li>Use case: Critical services needing HA with ordering</li> </ul>"},{"location":"client_libraries/consumer-basics/#receiving-messages","title":"Receiving Messages","text":""},{"location":"client_libraries/consumer-basics/#basic-message-loop","title":"Basic Message Loop","text":"RustGo <pre><code>use danube_client::{DanubeClient, SubType};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/events\")\n        .with_consumer_name(\"event-processor\")\n        .with_subscription(\"event-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n\n    // Start receiving\n    let mut message_stream = consumer.receive().await?;\n\n    while let Some(message) = message_stream.recv().await {\n        // Access message data\n        let payload = String::from_utf8_lossy(&amp;message.payload);\n        println!(\"\ud83d\udce5 Received: {}\", payload);\n\n        // Acknowledge the message\n        consumer.ack(&amp;message).await?;\n    }\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    consumer, err := client.NewConsumer(ctx).\n        WithConsumerName(\"event-processor\").\n        WithTopic(\"/default/events\").\n        WithSubscription(\"event-sub\").\n        WithSubscriptionType(danube.Exclusive).\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create consumer: %v\", err)\n    }\n\n    if err := consumer.Subscribe(ctx); err != nil {\n        log.Fatalf(\"Failed to subscribe: %v\", err)\n    }\n\n    // Start receiving\n    stream, err := consumer.Receive(ctx)\n    if err != nil {\n        log.Fatalf(\"Failed to receive: %v\", err)\n    }\n\n    for msg := range stream {\n        payload := string(msg.GetPayload())\n        fmt.Printf(\"\ud83d\udce5 Received: %s\\n\", payload)\n\n        // Acknowledge the message\n        if _, err := consumer.Ack(ctx, msg); err != nil {\n            log.Printf(\"Failed to ack: %v\\n\", err)\n        }\n    }\n}\n</code></pre>"},{"location":"client_libraries/consumer-basics/#message-acknowledgment","title":"Message Acknowledgment","text":"<p>Acknowledgment tells the broker the message was processed successfully.</p>"},{"location":"client_libraries/consumer-basics/#ack-pattern","title":"Ack Pattern","text":"RustGo <pre><code>while let Some(message) = message_stream.recv().await {\n    // Process message\n    match process_message(&amp;message.payload).await {\n        Ok(_) =&gt; {\n            // Acknowledge success\n            consumer.ack(&amp;message).await?;\n            println!(\"\u2705 Processed and acked\");\n        }\n        Err(e) =&gt; {\n            // Don't ack on failure - message will be redelivered\n            eprintln!(\"\u274c Processing failed: {}\", e);\n            // Message will be redelivered to this or another consumer\n        }\n    }\n}\n</code></pre> <pre><code>for msg := range stream {\n    if err := processMessage(msg.GetPayload()); err != nil {\n        // Don't ack on failure - message will be redelivered\n        log.Printf(\"\u274c Processing failed: %v\", err)\n        continue\n    }\n\n    // Acknowledge success\n    if _, err := consumer.Ack(ctx, msg); err != nil {\n        log.Printf(\"Failed to ack: %v\", err)\n    }\n\n    fmt.Println(\"\u2705 Processed and acked\")\n}\n</code></pre> <p>Important:</p> <ul> <li>\u26a0\ufe0f Only ack after successful processing</li> <li>\u26a0\ufe0f Unacked messages are redelivered</li> <li>\u26a0\ufe0f Messages persist until acked or subscription expires</li> </ul>"},{"location":"client_libraries/consumer-basics/#reading-message-attributes","title":"Reading Message Attributes","text":"<p>Access metadata sent with messages:</p> RustGo <pre><code>while let Some(message) = message_stream.recv().await {\n    // Read payload\n    let payload = String::from_utf8_lossy(&amp;message.payload);\n\n    // Read attributes (if any)\n    if let Some(attributes) = &amp;message.attributes {\n        for (key, value) in attributes {\n            println!(\"  {}: {}\", key, value);\n        }\n    }\n\n    // Read other metadata\n    println!(\"Message ID: {:?}\", message.msg_id);\n    println!(\"Publish time: {}\", message.publish_time);\n\n    consumer.ack(&amp;message).await?;\n}\n</code></pre> <pre><code>for msg := range stream {\n    payload := string(msg.GetPayload())\n\n    // Read attributes\n    for key, value := range msg.GetAttributes() {\n        fmt.Printf(\"  %s: %s\\n\", key, value)\n    }\n\n    // Read metadata\n    fmt.Printf(\"Message ID: %v\\n\", msg.GetMessageId())\n    fmt.Printf(\"Publish time: %d\\n\", msg.GetPublishTime())\n\n    consumer.Ack(ctx, msg)\n}\n</code></pre>"},{"location":"client_libraries/consumer-basics/#complete-example","title":"Complete Example","text":"RustGo <pre><code>use danube_client::{DanubeClient, SubType};\nuse tokio::time::{sleep, Duration};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // 1. Setup client\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    // 2. Create consumer\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/events\")\n        .with_consumer_name(\"event-processor\")\n        .with_subscription(\"event-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n    println!(\"\u2705 Consumer subscribed and ready\");\n\n    // 3. Receive messages\n    let mut message_stream = consumer.receive().await?;\n    let mut count = 0;\n\n    while let Some(message) = message_stream.recv().await {\n        // Extract payload\n        let payload = String::from_utf8_lossy(&amp;message.payload);\n\n        // Log receipt\n        count += 1;\n        println!(\"\ud83d\udce5 Message #{}: {}\", count, payload);\n\n        // Check attributes\n        if let Some(attrs) = &amp;message.attributes {\n            if let Some(priority) = attrs.get(\"priority\") {\n                if priority == \"high\" {\n                    println!(\"  \u26a1 High priority message!\");\n                }\n            }\n        }\n\n        // Simulate processing\n        sleep(Duration::from_millis(100)).await;\n\n        // Acknowledge\n        match consumer.ack(&amp;message).await {\n            Ok(_) =&gt; println!(\"  \u2705 Acknowledged\"),\n            Err(e) =&gt; eprintln!(\"  \u274c Ack failed: {}\", e),\n        }\n    }\n\n    Ok(())\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"time\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    // 1. Setup client\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    // 2. Create consumer\n    consumer, err := client.NewConsumer(ctx).\n        WithConsumerName(\"event-processor\").\n        WithTopic(\"/default/events\").\n        WithSubscription(\"event-sub\").\n        WithSubscriptionType(danube.Exclusive).\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create consumer: %v\", err)\n    }\n\n    if err := consumer.Subscribe(ctx); err != nil {\n        log.Fatalf(\"Failed to subscribe: %v\", err)\n    }\n\n    fmt.Println(\"\u2705 Consumer subscribed and ready\")\n\n    // 3. Receive messages\n    stream, err := consumer.Receive(ctx)\n    if err != nil {\n        log.Fatalf(\"Failed to receive: %v\", err)\n    }\n\n    count := 0\n    for msg := range stream {\n        payload := string(msg.GetPayload())\n\n        count++\n        fmt.Printf(\"\ud83d\udce5 Message #%d: %s\\n\", count, payload)\n\n        // Check attributes\n        if priority, ok := msg.GetAttributes()[\"priority\"]; ok {\n            if priority == \"high\" {\n                fmt.Println(\"  \u26a1 High priority message!\")\n            }\n        }\n\n        // Simulate processing\n        time.Sleep(100 * time.Millisecond)\n\n        // Acknowledge\n        if _, err := consumer.Ack(ctx, msg); err != nil {\n            fmt.Printf(\"  \u274c Ack failed: %v\\n\", err)\n        } else {\n            fmt.Println(\"  \u2705 Acknowledged\")\n        }\n    }\n}\n</code></pre>"},{"location":"client_libraries/producer-advanced/","title":"Advanced Producer Features","text":"<p>This guide covers advanced producer capabilities including partitions, reliable dispatch, and schema integration.</p>"},{"location":"client_libraries/producer-advanced/#partitioned-topics","title":"Partitioned Topics","text":"<p>Partitions enable horizontal scaling by distributing messages across multiple brokers.</p>"},{"location":"client_libraries/producer-advanced/#creating-partitioned-producers","title":"Creating Partitioned Producers","text":"RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/high-throughput\")\n        .with_name(\"partitioned-producer\")\n        .with_partitions(3)  // Create 3 partitions\n        .build();\n\n    producer.create().await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    producer, err := client.NewProducer(ctx).\n        WithName(\"partitioned-producer\").\n        WithTopic(\"/default/high-throughput\").\n        WithPartitions(3).  // Create 3 partitions\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create producer: %v\", err)\n    }\n\n    if err := producer.Create(ctx); err != nil {\n        log.Fatalf(\"Failed to initialize producer: %v\", err)\n    }\n}\n</code></pre> <p>What happens:</p> <ul> <li>Topic <code>/default/high-throughput</code> becomes:</li> <li><code>/default/high-throughput-part-0</code></li> <li><code>/default/high-throughput-part-1</code></li> <li><code>/default/high-throughput-part-2</code></li> <li>Messages distributed using round-robin routing</li> <li>Each partition can be on different brokers</li> </ul>"},{"location":"client_libraries/producer-advanced/#sending-to-partitions","title":"Sending to Partitions","text":"<p>Messages are automatically routed:</p> RustGo <pre><code>// Automatic round-robin distribution\nfor i in 0..9 {\n    let message = format!(\"Message {}\", i);\n    producer.send(message.as_bytes().to_vec(), None).await?;\n}\n\n// Result:\n// Message 0 -&gt; partition 0\n// Message 1 -&gt; partition 1\n// Message 2 -&gt; partition 2\n// Message 3 -&gt; partition 0 (round-robin)\n// ...\n</code></pre> <pre><code>import \"fmt\"\n\n// Automatic round-robin distribution\nfor i := 0; i &lt; 9; i++ {\n    message := fmt.Sprintf(\"Message %d\", i)\n    payload := []byte(message)\n    producer.Send(ctx, payload, nil)\n}\n</code></pre>"},{"location":"client_libraries/producer-advanced/#when-to-use-partitions","title":"When to Use Partitions","text":"<p>\u2705 Use partitions when:</p> <ul> <li>High message throughput (&gt;1K msgs/sec)</li> <li>Messages can be processed in any order</li> <li>Horizontal scaling needed</li> <li>Multiple brokers available</li> </ul> <p>\u274c Avoid partitions when:</p> <ul> <li>Strict message ordering required</li> <li>Low message volume</li> <li>Single broker deployment</li> </ul>"},{"location":"client_libraries/producer-advanced/#reliable-dispatch","title":"Reliable Dispatch","text":"<p>Reliable dispatch guarantees message delivery by persisting messages before acknowledging sends.</p>"},{"location":"client_libraries/producer-advanced/#enabling-reliable-dispatch","title":"Enabling Reliable Dispatch","text":"RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/critical-events\")\n        .with_name(\"reliable-producer\")\n        .with_reliable_dispatch()  // Enable persistence\n        .build();\n\n    producer.create().await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    // Configure reliable dispatch strategy\n    reliableStrategy := danube.NewReliableDispatchStrategy()\n\n    producer, err := client.NewProducer(ctx).\n        WithName(\"reliable-producer\").\n        WithTopic(\"/default/critical-events\").\n        WithDispatchStrategy(reliableStrategy).\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create producer: %v\", err)\n    }\n\n    if err := producer.Create(ctx); err != nil {\n        log.Fatalf(\"Failed to initialize producer: %v\", err)\n    }\n}\n</code></pre>"},{"location":"client_libraries/producer-advanced/#how-reliable-dispatch-works","title":"How Reliable Dispatch Works","text":"<pre><code>1. Producer sends message\n2. Broker persists to WAL (Write-Ahead Log)\n3. Broker uploads to cloud storage (background)\n4. Broker acknowledges send\n5. Message delivered to consumers from WAL/cloud\n</code></pre> <p>Guarantees:</p> <ul> <li>\u2705 Message never lost (persisted to disk + cloud)</li> <li>\u2705 Survives broker restarts</li> <li>\u2705 Replay from historical offset</li> <li>\u2705 Consumer can restart and resume</li> </ul> <p>Trade-offs:</p> <ul> <li>Slightly higher latency (~5-10ms added)</li> <li>Storage costs for persistence</li> <li>Good for: Critical business events, audit logs, transactions</li> </ul>"},{"location":"client_libraries/producer-advanced/#when-to-use-reliable-dispatch","title":"When to Use Reliable Dispatch","text":"<p>\u2705 Use for:</p> <ul> <li>Financial transactions</li> <li>Order processing</li> <li>Audit logs</li> <li>Critical notifications</li> </ul> <p>\u274c Avoid for:</p> <ul> <li>Real-time metrics (can tolerate loss)</li> <li>High-frequency sensor data</li> <li>Live streaming (freshness &gt; durability)</li> </ul>"},{"location":"client_libraries/producer-advanced/#combining-features","title":"Combining Features","text":""},{"location":"client_libraries/producer-advanced/#partitions-reliable-dispatch","title":"Partitions + Reliable Dispatch","text":"<p>Scale and durability together:</p> Rust <pre><code>let mut producer = client\n    .new_producer()\n    .with_topic(\"/default/orders\")\n    .with_name(\"order-producer\")\n    .with_partitions(5)           // Scale across 5 partitions\n    .with_reliable_dispatch()     // Persist all messages\n    .build();\n\nproducer.create().await?;\n\n// High throughput + guaranteed delivery\nfor order_id in 1..=10000 {\n    let order = format!(\"{{\\\"order_id\\\": {}}}\", order_id);\n    producer.send(order.as_bytes().to_vec(), None).await?;\n}\n</code></pre>"},{"location":"client_libraries/producer-advanced/#schema-integration","title":"Schema Integration","text":"<p>Link producers to schemas for type safety (see Schema Registry for details).</p> <p>Note: Schema Registry integration is not yet available in the Go client.</p>"},{"location":"client_libraries/producer-advanced/#basic-schema-usage","title":"Basic Schema Usage","text":"Rust <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SchemaType};\nuse serde::Serialize;\n\n#[derive(Serialize)]\nstruct Event {\n    event_id: String,\n    timestamp: i64,\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    // 1. Register schema\n    let json_schema = r#\"{\n        \"type\": \"object\",\n        \"properties\": {\n            \"event_id\": {\"type\": \"string\"},\n            \"timestamp\": {\"type\": \"integer\"}\n        },\n        \"required\": [\"event_id\", \"timestamp\"]\n    }\"#;\n\n    let mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n    let schema_id = schema_client\n        .register_schema(\"event-schema\")\n        .with_type(SchemaType::JsonSchema)\n        .with_schema_data(json_schema.as_bytes())\n        .execute()\n        .await?;\n\n    println!(\"\ud83d\udccb Registered schema ID: {}\", schema_id);\n\n    // 2. Create producer with schema reference\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/events\")\n        .with_name(\"schema-producer\")\n        .with_schema_subject(\"event-schema\")  // Link to schema\n        .build();\n\n    producer.create().await?;\n\n    // 3. Send typed messages\n    let event = Event {\n        event_id: \"evt-123\".to_string(),\n        timestamp: 1234567890,\n    };\n\n    let json_bytes = serde_json::to_vec(&amp;event)?;\n    let msg_id = producer.send(json_bytes, None).await?;\n\n    println!(\"\ud83d\udce4 Sent validated message: {}\", msg_id);\n\n    Ok(())\n}\n</code></pre> <p>Benefits:</p> <ul> <li>Messages validated against schema before sending</li> <li>Schema ID included in message metadata (8 bytes vs KB of schema)</li> <li>Consumers know message structure</li> <li>Safe schema evolution with compatibility checking</li> </ul>"},{"location":"client_libraries/producer-basics/","title":"Producer Basics","text":"<p>Producers send messages to Danube topics. This guide covers fundamental producer operations.</p>"},{"location":"client_libraries/producer-basics/#creating-a-producer","title":"Creating a Producer","text":""},{"location":"client_libraries/producer-basics/#simple-producer","title":"Simple Producer","text":"<p>The minimal setup to send messages:</p> RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/my-topic\")\n        .with_name(\"my-producer\")\n        .build();\n\n    producer.create().await?;\n    println!(\"\u2705 Producer created\");\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    producer, err := client.NewProducer(ctx).\n        WithName(\"my-producer\").\n        WithTopic(\"/default/my-topic\").\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create producer: %v\", err)\n    }\n\n    if err := producer.Create(ctx); err != nil {\n        log.Fatalf(\"Failed to initialize producer: %v\", err)\n    }\n\n    fmt.Println(\"\u2705 Producer created\")\n}\n</code></pre> <p>Key concepts:</p> <ul> <li>Topic: Destination for messages (e.g., <code>/default/my-topic</code>)</li> <li>Name: Unique producer identifier</li> <li>Create: Registers producer with broker before sending</li> </ul>"},{"location":"client_libraries/producer-basics/#sending-messages","title":"Sending Messages","text":""},{"location":"client_libraries/producer-basics/#byte-messages","title":"Byte Messages","text":"<p>Send raw byte data:</p> RustGo <pre><code>let message = \"Hello Danube!\";\nlet message_id = producer\n    .send(message.as_bytes().to_vec(), None)\n    .await?;\n\nprintln!(\"\ud83d\udce4 Sent message ID: {}\", message_id);\n</code></pre> <pre><code>message := \"Hello Danube!\"\nmessageID, err := producer.Send(ctx, []byte(message), nil)\nif err != nil {\n    log.Fatalf(\"Failed to send: %v\", err)\n}\n\nfmt.Printf(\"\ud83d\udce4 Sent message ID: %v\\n\", messageID)\n</code></pre> <p>Returns: Unique message ID for tracking</p>"},{"location":"client_libraries/producer-basics/#messages-with-attributes","title":"Messages with Attributes","text":"<p>Add metadata to messages:</p> RustGo <pre><code>use std::collections::HashMap;\n\nlet mut attributes = HashMap::new();\nattributes.insert(\"source\".to_string(), \"app-1\".to_string());\nattributes.insert(\"priority\".to_string(), \"high\".to_string());\n\nlet message_id = producer\n    .send(b\"Important message\".to_vec(), Some(attributes))\n    .await?;\n</code></pre> <pre><code>attributes := map[string]string{\n    \"source\":   \"app-1\",\n    \"priority\": \"high\",\n}\n\nmessageID, err := producer.Send(ctx, []byte(\"Important message\"), attributes)\n</code></pre> <p>Use cases:</p> <ul> <li>Routing hints</li> <li>Message metadata</li> <li>Custom headers</li> <li>Tracing IDs</li> </ul>"},{"location":"client_libraries/producer-basics/#complete-example","title":"Complete Example","text":"RustGo <pre><code>use danube_client::DanubeClient;\nuse std::collections::HashMap;\nuse tokio::time::{sleep, Duration};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // 1. Setup client\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    // 2. Create producer\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/events\")\n        .with_name(\"event-producer\")\n        .build();\n\n    producer.create().await?;\n    println!(\"\u2705 Producer created\");\n\n    // 3. Send messages\n    for i in 1..=10 {\n        // Prepare message\n        let message = format!(\"Event #{}\", i);\n\n        // Add attributes\n        let mut attributes = HashMap::new();\n        attributes.insert(\"event_id\".to_string(), i.to_string());\n        attributes.insert(\"timestamp\".to_string(), \n            chrono::Utc::now().to_rfc3339());\n\n        // Send\n        match producer.send(message.as_bytes().to_vec(), Some(attributes)).await {\n            Ok(msg_id) =&gt; println!(\"\ud83d\udce4 Sent: {} (ID: {})\", message, msg_id),\n            Err(e) =&gt; eprintln!(\"\u274c Failed to send: {}\", e),\n        }\n\n        sleep(Duration::from_millis(500)).await;\n    }\n\n    println!(\"\u2705 Sent 10 messages\");\n    Ok(())\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"time\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    // 1. Setup client\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    // 2. Create producer\n    producer, err := client.NewProducer(ctx).\n        WithName(\"event-producer\").\n        WithTopic(\"/default/events\").\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create producer: %v\", err)\n    }\n\n    if err := producer.Create(ctx); err != nil {\n        log.Fatalf(\"Failed to initialize producer: %v\", err)\n    }\n\n    fmt.Println(\"\u2705 Producer created\")\n\n    // 3. Send messages\n    for i := 1; i &lt;= 10; i++ {\n        message := fmt.Sprintf(\"Event #%d\", i)\n\n        attributes := map[string]string{\n            \"event_id\":  fmt.Sprintf(\"%d\", i),\n            \"timestamp\": time.Now().Format(time.RFC3339),\n        }\n\n        msgID, err := producer.Send(ctx, []byte(message), attributes)\n        if err != nil {\n            fmt.Printf(\"\u274c Failed to send: %v\\n\", err)\n            continue\n        }\n\n        fmt.Printf(\"\ud83d\udce4 Sent: %s (ID: %v)\\n\", message, msgID)\n        time.Sleep(500 * time.Millisecond)\n    }\n\n    fmt.Println(\"\u2705 Sent 10 messages\")\n}\n</code></pre>"},{"location":"client_libraries/producer-basics/#topic-naming","title":"Topic Naming","text":""},{"location":"client_libraries/producer-basics/#topic-format","title":"Topic Format","text":"<p>Topics follow a namespace structure:</p> <pre><code>/namespace/topic-name\n</code></pre> <p>Examples:</p> <ul> <li><code>/default/orders</code> - Orders in default namespace</li> <li><code>/production/user-events</code> - User events in production namespace</li> <li><code>/staging/logs</code> - Logs in staging namespace</li> </ul>"},{"location":"client_libraries/producer-basics/#best-practices","title":"Best Practices","text":"<p>\u2705 Do:</p> <ul> <li>Use descriptive names: <code>/default/payment-processed</code></li> <li>Group by domain: <code>/inventory/stock-updates</code></li> <li>Include environment in namespace: <code>/prod/...</code>, <code>/dev/...</code></li> </ul> <p>\u274c Don't:</p> <ul> <li>Use special characters except <code>-</code> and <code>_</code></li> <li>Make names too long (keep under 255 chars)</li> <li>Mix environments in same namespace</li> </ul>"},{"location":"client_libraries/schema-registry/","title":"Schema Registry Integration","text":"<p>The Schema Registry provides type safety, validation, and schema evolution for your messages. This guide shows how to use it from client applications.</p> <p>Note: Schema Registry is currently only available in the Rust client. Go client support is coming soon.</p>"},{"location":"client_libraries/schema-registry/#overview","title":"Overview","text":"<p>What you get:</p> <ul> <li>\u2705 Type-safe message serialization/deserialization</li> <li>\u2705 Automatic schema validation</li> <li>\u2705 Safe schema evolution with compatibility checking</li> <li>\u2705 Reduced bandwidth (schema ID vs. full schema)</li> <li>\u2705 Schema discovery and documentation</li> </ul> <p>Workflow:</p> <ol> <li>Register schema with Schema Registry</li> <li>Link producer to schema subject</li> <li>Send validated messages</li> <li>Consumer fetches schema and deserializes</li> </ol>"},{"location":"client_libraries/schema-registry/#schema-registry-client","title":"Schema Registry Client","text":""},{"location":"client_libraries/schema-registry/#creating-the-client","title":"Creating the Client","text":"RustGo <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n// Coming soon\n</code></pre> <p>Note: Schema Registry client is separate from producer/consumer clients but shares the same connection pool.</p>"},{"location":"client_libraries/schema-registry/#registering-schemas","title":"Registering Schemas","text":""},{"location":"client_libraries/schema-registry/#json-schema","title":"JSON Schema","text":"RustGo <pre><code>use danube_client::{SchemaRegistryClient, SchemaType};\n\nlet json_schema = r#\"{\n    \"type\": \"object\",\n    \"properties\": {\n        \"user_id\": {\"type\": \"string\"},\n        \"event\": {\"type\": \"string\"},\n        \"timestamp\": {\"type\": \"integer\"}\n    },\n    \"required\": [\"user_id\", \"event\", \"timestamp\"]\n}\"#;\n\nlet schema_id = schema_client\n    .register_schema(\"user-events\")\n    .with_type(SchemaType::JsonSchema)\n    .with_schema_data(json_schema.as_bytes())\n    .with_description(\"User activity events v1\")\n    .execute()\n    .await?;\n\nprintln!(\"\u2705 Registered schema ID: {}\", schema_id);\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n</code></pre>"},{"location":"client_libraries/schema-registry/#avro-schema","title":"Avro Schema","text":"RustGo <pre><code>use danube_client::{SchemaRegistryClient, SchemaType};\n\nlet avro_schema = r#\"{\n    \"type\": \"record\",\n    \"name\": \"UserEvent\",\n    \"namespace\": \"com.example\",\n    \"fields\": [\n        {\"name\": \"user_id\", \"type\": \"string\"},\n        {\"name\": \"event\", \"type\": \"string\"},\n        {\"name\": \"timestamp\", \"type\": \"long\"},\n        {\"name\": \"metadata\", \"type\": [\"null\", \"string\"], \"default\": null}\n    ]\n}\"#;\n\nlet schema_id = schema_client\n    .register_schema(\"user-events-avro\")\n    .with_type(SchemaType::Avro)\n    .with_schema_data(avro_schema.as_bytes())\n    .execute()\n    .await?;\n\nprintln!(\"\u2705 Registered Avro schema: {}\", schema_id);\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n</code></pre>"},{"location":"client_libraries/schema-registry/#idempotent-registration","title":"Idempotent Registration","text":"<p>Registering the same schema content multiple times returns the existing schema ID:</p> Rust <pre><code>// First registration\nlet id1 = schema_client\n    .register_schema(\"events\")\n    .with_type(SchemaType::JsonSchema)\n    .with_schema_data(schema.as_bytes())\n    .execute()\n    .await?;\n\n// Subsequent registration of same content\nlet id2 = schema_client\n    .register_schema(\"events\")\n    .with_type(SchemaType::JsonSchema)\n    .with_schema_data(schema.as_bytes())\n    .execute()\n    .await?;\n\nassert_eq!(id1, id2);  // Same ID returned\n</code></pre>"},{"location":"client_libraries/schema-registry/#retrieving-schemas","title":"Retrieving Schemas","text":""},{"location":"client_libraries/schema-registry/#get-latest-version","title":"Get Latest Version","text":"RustGo <pre><code>let schema = schema_client\n    .get_latest_schema(\"user-events\")\n    .await?;\n\nprintln!(\"Schema version: {}\", schema.version);\nprintln!(\"Schema type: {:?}\", schema.schema_type);\nprintln!(\"Schema definition: {}\", String::from_utf8_lossy(&amp;schema.data));\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n</code></pre>"},{"location":"client_libraries/schema-registry/#list-all-versions","title":"List All Versions","text":"RustGo <pre><code>let versions = schema_client\n    .list_versions(\"user-events\")\n    .await?;\n\nprintln!(\"Available versions: {:?}\", versions);  // e.g., [1, 2, 3]\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n</code></pre>"},{"location":"client_libraries/schema-registry/#compatibility-checking","title":"Compatibility Checking","text":""},{"location":"client_libraries/schema-registry/#check-before-registering","title":"Check Before Registering","text":"Rust <pre><code>use danube_client::{SchemaType, CompatibilityMode};\n\nlet new_schema = r#\"{\n    \"type\": \"object\",\n    \"properties\": {\n        \"user_id\": {\"type\": \"string\"},\n        \"event\": {\"type\": \"string\"},\n        \"timestamp\": {\"type\": \"integer\"},\n        \"email\": {\"type\": \"string\"}\n    },\n    \"required\": [\"user_id\", \"event\", \"timestamp\"]\n}\"#;\n\n// Check compatibility before registering\nlet result = schema_client\n    .check_compatibility(\n        \"user-events\",\n        new_schema.as_bytes(),\n        SchemaType::JsonSchema,\n        None,  // Use subject's default mode\n    )\n    .await?;\n\nif result.is_compatible {\n    println!(\"\u2705 Safe to register!\");\n\n    // Now register\n    schema_client\n        .register_schema(\"user-events\")\n        .with_type(SchemaType::JsonSchema)\n        .with_schema_data(new_schema.as_bytes())\n        .execute()\n        .await?;\n} else {\n    eprintln!(\"\u274c Incompatible: {:?}\", result.errors);\n}\n</code></pre>"},{"location":"client_libraries/schema-registry/#set-compatibility-mode","title":"Set Compatibility Mode","text":"RustGo <pre><code>use danube_client::CompatibilityMode;\n\n// Set compatibility mode for a subject\nschema_client\n    .set_compatibility_mode(\"user-events\", CompatibilityMode::Full)\n    .await?;\n\nprintln!(\"\u2705 Compatibility mode set to Full\");\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n</code></pre> <p>Available modes:</p> <ul> <li><code>CompatibilityMode::None</code> - No checking (development only)</li> <li><code>CompatibilityMode::Backward</code> - New schema reads old data (default)</li> <li><code>CompatibilityMode::Forward</code> - Old schema reads new data</li> <li><code>CompatibilityMode::Full</code> - Both backward and forward</li> </ul>"},{"location":"client_libraries/schema-registry/#producer-with-schema","title":"Producer with Schema","text":""},{"location":"client_libraries/schema-registry/#basic-pattern","title":"Basic Pattern","text":"RustGo <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SchemaType};\nuse serde::Serialize;\n\n#[derive(Serialize)]\nstruct UserEvent {\n    user_id: String,\n    event: String,\n    timestamp: i64,\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    // 1. Register schema\n    let schema = r#\"{\n        \"type\": \"object\",\n        \"properties\": {\n            \"user_id\": {\"type\": \"string\"},\n            \"event\": {\"type\": \"string\"},\n            \"timestamp\": {\"type\": \"integer\"}\n        },\n        \"required\": [\"user_id\", \"event\", \"timestamp\"]\n    }\"#;\n\n    let mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n    schema_client\n        .register_schema(\"user-events\")\n        .with_type(SchemaType::JsonSchema)\n        .with_schema_data(schema.as_bytes())\n        .execute()\n        .await?;\n\n    // 2. Create producer with schema reference\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/user-events\")\n        .with_name(\"event-producer\")\n        .with_schema_subject(\"user-events\")  // Link to schema\n        .build();\n\n    producer.create().await?;\n\n    // 3. Send typed messages\n    let event = UserEvent {\n        user_id: \"user-123\".to_string(),\n        event: \"login\".to_string(),\n        timestamp: 1234567890,\n    };\n\n    // Serialize to JSON\n    let json_bytes = serde_json::to_vec(&amp;event)?;\n\n    // Send (schema ID automatically included)\n    let msg_id = producer.send(json_bytes, None).await?;\n    println!(\"\ud83d\udce4 Sent message: {}\", msg_id);\n\n    Ok(())\n}\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n</code></pre>"},{"location":"client_libraries/schema-registry/#consumer-with-schema","title":"Consumer with Schema","text":""},{"location":"client_libraries/schema-registry/#basic-pattern_1","title":"Basic Pattern","text":"Rust <pre><code>use danube_client::{DanubeClient, SubType};\nuse serde::Deserialize;\n\n#[derive(Deserialize, Debug)]\nstruct UserEvent {\n    user_id: String,\n    event: String,\n    timestamp: i64,\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/user-events\")\n        .with_consumer_name(\"event-consumer\")\n        .with_subscription(\"event-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n    let mut stream = consumer.receive().await?;\n\n    while let Some(message) = stream.recv().await {\n        // Deserialize JSON message\n        match serde_json::from_slice::&lt;UserEvent&gt;(&amp;message.payload) {\n            Ok(event) =&gt; {\n                println!(\"\ud83d\udce5 Event: {:?}\", event);\n                consumer.ack(&amp;message).await?;\n            }\n            Err(e) =&gt; {\n                eprintln!(\"\u274c Deserialization failed: {}\", e);\n            }\n        }\n    }\n\n    Ok(())\n}\n</code></pre>"},{"location":"client_libraries/schema-registry/#schema-evolution-example","title":"Schema Evolution Example","text":""},{"location":"client_libraries/schema-registry/#adding-optional-field-backward-compatible","title":"Adding Optional Field (Backward Compatible)","text":"Rust <pre><code>use danube_client::{SchemaRegistryClient, SchemaType};\n\n// V1 Schema\nlet schema_v1 = r#\"{\n    \"type\": \"object\",\n    \"properties\": {\n        \"user_id\": {\"type\": \"string\"},\n        \"event\": {\"type\": \"string\"}\n    },\n    \"required\": [\"user_id\", \"event\"]\n}\"#;\n\n// Register V1\nlet mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\nschema_client\n    .register_schema(\"events\")\n    .with_type(SchemaType::JsonSchema)\n    .with_schema_data(schema_v1.as_bytes())\n    .execute()\n    .await?;\n\n// V2 Schema (add optional field)\nlet schema_v2 = r#\"{\n    \"type\": \"object\",\n    \"properties\": {\n        \"user_id\": {\"type\": \"string\"},\n        \"event\": {\"type\": \"string\"},\n        \"metadata\": {\"type\": \"string\"}\n    },\n    \"required\": [\"user_id\", \"event\"]\n}\"#;\n\n// Check compatibility\nlet compat = schema_client\n    .check_compatibility(\n        \"events\",\n        schema_v2.as_bytes(),\n        SchemaType::JsonSchema,\n        None,\n    )\n    .await?;\n\nif compat.is_compatible {\n    // Register V2\n    schema_client\n        .register_schema(\"events\")\n        .with_type(SchemaType::JsonSchema)\n        .with_schema_data(schema_v2.as_bytes())\n        .execute()\n        .await?;\n\n    println!(\"\u2705 Successfully evolved schema to V2\");\n}\n</code></pre> <p>Result:</p> <ul> <li>Old consumers can still read V2 messages (ignore extra field)</li> <li>New consumers can use <code>metadata</code> field</li> <li>No breaking changes</li> </ul>"},{"location":"client_libraries/schema-registry/#schema-types","title":"Schema Types","text":""},{"location":"client_libraries/schema-registry/#supported-types","title":"Supported Types","text":"Type Description Status <code>SchemaType::JsonSchema</code> JSON Schema validation \u2705 Production <code>SchemaType::Avro</code> Apache Avro binary \u2705 Registration ready <code>SchemaType::Protobuf</code> Protocol Buffers \u2705 Registration ready <code>SchemaType::String</code> UTF-8 text \u2705 Basic validation <code>SchemaType::Number</code> Numeric types \u2705 Basic validation <code>SchemaType::Bytes</code> Raw binary \u2705 No validation"},{"location":"client_libraries/schema-registry/#troubleshooting","title":"Troubleshooting","text":""},{"location":"client_libraries/schema-registry/#schema-registration-fails","title":"Schema Registration Fails","text":"<pre><code>Error: Schema validation failed\n</code></pre> <p>Solutions:</p> <ul> <li>Validate JSON/Avro schema syntax</li> <li>Check schema is well-formed</li> <li>Ensure schema type matches content</li> </ul>"},{"location":"client_libraries/schema-registry/#compatibility-check-fails","title":"Compatibility Check Fails","text":"<pre><code>Error: Incompatible schema: removing required field\n</code></pre> <p>Solutions:</p> <ul> <li>Make field optional instead of removing</li> <li>Use <code>CompatibilityMode::None</code> for development</li> <li>Review compatibility mode requirements</li> </ul>"},{"location":"client_libraries/schema-registry/#deserialization-errors","title":"Deserialization Errors","text":"<pre><code>Error: missing field `new_field`\n</code></pre> <p>Solutions:</p> <ul> <li>Make new fields <code>Option&lt;T&gt;</code> in Rust</li> <li>Add <code>#[serde(default)]</code> attribute</li> <li>Ensure schema evolution is backward compatible</li> </ul>"},{"location":"client_libraries/setup/","title":"Client Setup and Configuration","text":"<p>This guide covers how to configure and connect Danube clients to your broker.</p>"},{"location":"client_libraries/setup/#basic-connection","title":"Basic Connection","text":"<p>Connect to Danube broker with an gRPC endpoint:</p> RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n}\n</code></pre> <p>Endpoint format: <code>http://host:port</code> or <code>https://host:port</code> for TLS</p>"},{"location":"client_libraries/setup/#tls-configuration","title":"TLS Configuration","text":"<p>For secure production environments, enable TLS encryption:</p> RustGo <pre><code>use danube_client::DanubeClient;\nuse rustls::crypto;\nuse tokio::sync::OnceCell;\n\nstatic CRYPTO_PROVIDER: OnceCell&lt;()&gt; = OnceCell::const_new();\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // Initialize crypto provider (required once)\n    CRYPTO_PROVIDER.get_or_init(|| async {\n        let crypto_provider = crypto::ring::default_provider();\n        crypto_provider\n            .install_default()\n            .expect(\"Failed to install default CryptoProvider\");\n    })\n    .await;\n\n    let client = DanubeClient::builder()\n        .service_url(\"https://127.0.0.1:6650\")\n        .with_tls(\"./certs/ca-cert.pem\")?\n        .build()\n        .await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>// TLS support coming soon\n</code></pre> <p>Requirements:</p> <ul> <li>CA certificate file (PEM format)</li> <li>HTTPS URL (<code>https://</code> instead of <code>http://</code>)</li> <li>Broker must be configured with TLS enabled</li> </ul> <p>Certificate paths:</p> <ul> <li>Relative: <code>./certs/ca-cert.pem</code></li> <li>Absolute: <code>/etc/danube/certs/ca-cert.pem</code></li> </ul>"},{"location":"client_libraries/setup/#jwt-authentication","title":"JWT Authentication","text":"<p>For authenticated environments, use API keys to obtain JWT tokens:</p> RustGo <pre><code>use danube_client::DanubeClient;\nuse rustls::crypto;\nuse tokio::sync::OnceCell;\n\nstatic CRYPTO_PROVIDER: OnceCell&lt;()&gt; = OnceCell::const_new();\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    CRYPTO_PROVIDER.get_or_init(|| async {\n        let crypto_provider = crypto::ring::default_provider();\n        crypto_provider\n            .install_default()\n            .expect(\"Failed to install default CryptoProvider\");\n    })\n    .await;\n\n    let api_key = std::env::var(\"DANUBE_API_KEY\")\n        .expect(\"DANUBE_API_KEY environment variable not set\");\n\n    let client = DanubeClient::builder()\n        .service_url(\"https://127.0.0.1:6650\")\n        .with_tls(\"./certs/ca-cert.pem\")?\n        .with_api_key(api_key)\n        .build()\n        .await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>// JWT authentication support coming soon\n</code></pre> <p>How it works:</p> <ol> <li>Client exchanges API key for JWT token on first request</li> <li>Token is cached and automatically renewed when expired</li> <li>Token included in <code>Authorization</code> header for all requests</li> <li>Default token lifetime: 1 hour</li> </ol> <p>Security best practices:</p> <ul> <li>Store API keys in environment variables</li> <li>Never hardcode API keys in source code</li> <li>Use different API keys per environment (dev/staging/prod)</li> <li>Rotate API keys regularly</li> </ul>"},{"location":"client_libraries/setup/#connection-options","title":"Connection Options","text":""},{"location":"client_libraries/setup/#connection-pooling","title":"Connection Pooling","text":"<p>Clients automatically manage connection pools. Multiple producers/consumers share underlying connections efficiently.</p> Rust <pre><code>let client = DanubeClient::builder()\n    .service_url(\"http://127.0.0.1:6650\")\n    .build()\n    .await?;\n\n// All producers/consumers share the same connection pool\nlet producer1 = client.new_producer().with_topic(\"/topic1\").build();\nlet producer2 = client.new_producer().with_topic(\"/topic2\").build();\nlet consumer = client.new_consumer().with_topic(\"/topic1\").build();\n</code></pre>"},{"location":"client_libraries/setup/#service-discovery","title":"Service Discovery","text":"<p>For clustered deployments, the client performs automatic topic lookup:</p> <pre><code>// Client connects to any broker in the cluster\nlet client = DanubeClient::builder()\n    .service_url(\"&lt;http://broker1:6650&gt;\")\n    .build()\n    .await?;\n\n// Topic lookup finds the owning broker\nlet producer = client.new_producer()\n    .with_topic(\"/default/my-topic\")\n    .build();\n\n// Producer connects to the correct broker automatically\nproducer.create().await?;\n</code></pre>"},{"location":"client_libraries/setup/#environment-based-configuration","title":"Environment-Based Configuration","text":"<pre><code># Production\nexport DANUBE_URL=https://danube.example.com:6650\nexport DANUBE_CA_CERT=/etc/danube/certs/ca.pem\nexport DANUBE_API_KEY=your-secret-api-key\n</code></pre>"},{"location":"client_libraries/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"client_libraries/setup/#connection-refused","title":"Connection Refused","text":"<pre><code>Error: Connection refused (os error 111)\n</code></pre> <p>Solutions:</p> <ul> <li>Verify broker is running: <code>curl http://localhost:6650</code></li> <li>Check firewall rules</li> <li>Confirm correct host and port</li> </ul>"},{"location":"client_libraries/setup/#tls-certificate-errors","title":"TLS Certificate Errors","text":"<pre><code>Error: InvalidCertificate\n</code></pre> <p>Solutions:</p> <ul> <li>Verify CA certificate path is correct</li> <li>Ensure certificate is PEM format</li> <li>Check certificate hasn't expired</li> <li>Confirm broker TLS configuration matches client</li> </ul>"},{"location":"client_libraries/setup/#authentication-failures","title":"Authentication Failures","text":"<pre><code>Error: Unauthenticated\n</code></pre> <p>Solutions:</p> <ul> <li>Verify API key is valid</li> <li>Check broker authentication mode (tls vs tlswithjwt)</li> <li>Ensure token hasn't expired (client auto-renews, but check logs)</li> </ul>"},{"location":"client_libraries/setup/#next-steps","title":"Next Steps","text":"<ul> <li>Producer Basics - Start sending messages</li> <li>Consumer Basics - Start receiving messages</li> <li>Schema Registry - Add type safety with schemas</li> </ul>"},{"location":"concepts/dispatch_strategy/","title":"Dispatch Strategy","text":"<p>The dispatch strategies in Danube represent two distinct approaches to message delivery, each serving different use cases:</p>"},{"location":"concepts/dispatch_strategy/#non-reliable-dispatch-strategy","title":"Non-Reliable Dispatch Strategy","text":"<p>This strategy prioritizes speed and minimal resource usage by delivering messages directly from producers to subscribers without persistence. Messages flow through the broker in a \"fire and forget\" manner, achieving the lowest possible latency. It fits real-time metrics, live telemetry, or any workload where occasional loss is acceptable.</p> <p>Writer path (producer)</p> <ul> <li>The producer sends a message to the broker specifying the topic.</li> <li>The broker validates and routes the message to the topic\u2019s dispatcher.</li> <li>Depending on subscription type (Exclusive/Shared/Failover), the dispatcher selects the target consumer(s).</li> <li>The message is immediately forwarded to consumer channels. There is no on-disk persistence and no acknowledgment gating.</li> </ul> <p>Reader path (consumer)</p> <ul> <li>A consumer subscribes to a topic under an existing subscription (Exclusive/Shared/Failover).</li> <li>The broker registers the consumer and attaches a live message stream to it.</li> <li>The dispatcher pushes incoming messages directly to the consumer stream.</li> <li>Acknowledgments are optional and do not affect delivery; if a consumer disconnects, messages in flight may be lost.</li> </ul>"},{"location":"concepts/dispatch_strategy/#reliable-dispatch-strategy","title":"Reliable Dispatch Strategy","text":"<p>This strategy ensures at-least-once delivery using a WAL + Cloud store-and-forward design. Messages are appended to a local Write-Ahead Log (WAL) and asynchronously uploaded to cloud object storage. Delivery is coordinated by the subscription engine, which tracks progress and acknowledgments per subscription.</p> <p>Writer path (producer)</p> <ul> <li>The producer sends a message to the broker for a reliable topic.</li> <li>The message is appended to the local WAL (durable on disk) and becomes eligible for dispatch.</li> <li>The dispatcher prepares the message for the subscription type (Exclusive/Shared/Failover) while the subscription engine records it as pending.</li> <li>A background uploader asynchronously persists WAL frames to cloud object storage; this does not block producers.</li> </ul> <p>Reader path (consumer)</p> <ul> <li>A consumer subscribes to a reliable topic; the broker attaches a stream and initializes subscription progress.</li> <li>The dispatcher delivers messages according to the subscription type and ordering guarantees.</li> <li>The consumer acknowledges processed messages; the subscription engine advances progress and triggers redelivery if needed.</li> <li>If the consumer is late or reconnects after a gap, historical data is replayed from the WAL or, if needed, from cloud storage, then seamlessly handed off to the live WAL tail.</li> </ul> <p>These strategies embody Danube\u2019s flexibility, letting you choose the right balance between performance and reliability per topic. You can run non-reliable and reliable topics side by side in the same cluster.</p>"},{"location":"concepts/messages/","title":"Message Structure in Danube","text":"<p>A message in Danube represents the fundamental unit of data transmission between producers and consumers. Each message contains both the payload and associated metadata for proper routing and processing.</p>"},{"location":"concepts/messages/#streammessage-structure","title":"StreamMessage Structure","text":""},{"location":"concepts/messages/#core-fields","title":"Core Fields","text":"<ul> <li>request_id (u64): Unique identifier for tracking the message request</li> <li>msg_id (MessageID): Identifier containing routing and location information</li> <li>payload (Vec): The binary message content <li>publish_time (u64): Timestamp when the message was published</li> <li>producer_name (String): Name of the producer that sent the message</li> <li>subscription_name (Option): Name of the subscription for consumer acknowledgment routing, when applicable <li>attributes (HashMap): User-defined key-value pairs for custom metadata"},{"location":"concepts/messages/#messageid-fields","title":"MessageID Fields","text":"<ul> <li>producer_id (u64): Unique identifier for the producer within a topic</li> <li>topic_name (String): Name of the topic the message belongs to</li> <li>broker_addr (String): Address of the broker that delivered the message to the consumer</li> <li>topic_offset (u64): Monotonic position of the message within the topic</li> </ul>"},{"location":"concepts/messages/#usage","title":"Usage","text":""},{"location":"concepts/messages/#producer-perspective","title":"Producer Perspective","text":"<p>Producers create messages by setting the payload and optional attributes. The system automatically generates and manages other fields like request_id, msg_id, and publish_time.</p>"},{"location":"concepts/messages/#consumer-perspective","title":"Consumer Perspective","text":"<p>Consumers receive the complete StreamMessage structure, providing access to both the message payload and all associated metadata for processing and acknowledgment handling.</p>"},{"location":"concepts/messages/#message-routing","title":"Message Routing","text":"<p>The MessageID structure enables efficient message routing and acknowledgment handling across the Danube messaging system, ensuring messages reach their intended destinations and can be properly tracked.</p>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/","title":"Pub/Sub vs Streaming: Concepts and Danube Support","text":"<p>This page compares Pub/Sub messaging and Streaming architectures, highlights key differences and use cases, and indicates what Danube supports. Use it as a conceptual guide aligned with Danube\u2019s Reliable and Non\u2011Reliable dispatch modes.</p>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#pubsub-messaging","title":"Pub/Sub messaging","text":""},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#purpose-and-use-cases","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system.</li> <li>Use Cases: Event-driven architectures, real-time notifications, decoupled microservices, and distributed systems. Suitable when low latency is critical and some message loss is acceptable (e.g., monitoring, telemetry, ephemeral chat).</li> </ul> <p>Danube support: Yes, via Non\u2011Reliable dispatch.</p> <ul> <li>Delivery: best\u2011effort (at\u2011most\u2011once).</li> <li>Persistence: none; messages are not stored.</li> <li>Ordering: per topic/partition within a dispatcher.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#architecture-and-design","title":"Architecture and Design","text":"<ul> <li>Components: Producers, Subscriptions (consumer groups), and the broker.</li> <li>Message Flow: Producers send messages to a broker, which distributes them to active subscribers according to subscription semantics.</li> <li>Scaling: Scales by adding more brokers or distributing load (topics / partitions) across multiple brokers.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#data-handling-and-processing-models","title":"Data Handling and Processing Models","text":""},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#pubsub-messaging-producers","title":"Pub/Sub messaging Producers","text":"<ul> <li>Low Latency: No disk persistence on the hot path; minimal overhead.</li> <li>Ordering: Preserved per topic/partition through the dispatcher.</li> <li>Delivery: Best\u2011effort; messages can be lost on failure.</li> <li>Acks: Immediate, based on in\u2011memory handling.</li> <li>No Active Consumers: Publishing is allowed; messages are dropped if no consumers are attached.</li> </ul> <p>Danube: Matches Non\u2011Reliable dispatch semantics.</p>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#pubsub-messaging-consumers","title":"Pub/Sub messaging Consumers","text":"<ul> <li>Real-time: Messages are delivered only to connected consumers.</li> <li>No Replay: No historical fetch; process as they arrive.</li> <li>Throughput: High, with low overhead.</li> <li>Delivery: Only to active subscribers; otherwise dropped.</li> </ul> <p>Danube: Supported via Exclusive, Shared, Failover subscriptions in Non\u2011Reliable mode.</p>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#order-of-operations-of-pubsub-messaging","title":"Order of Operations of Pub/Sub messaging","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Broker Receives Message: The broker processes the message.</li> <li>Consumer Availability Check: If consumers are available, the message is delivered to them in real-time.</li> <li>No Consumers: If no consumers are connected, the message is discarded.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#streaming-design-considerations","title":"Streaming design considerations","text":""},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#purpose-and-use-cases_1","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for processing and analyzing large volumes of data in real-time as it is generated.</li> <li>Use Cases: Real-time analytics, data pipelines, event sourcing, continuous data processing, and stream processing applications. Ideal when high reliability and durability are required (e.g., financial transactions, orders, audit logs).</li> </ul> <p>Danube support: Yes, via Reliable dispatch (WAL + Cloud persistence).</p> <ul> <li>Delivery: at\u2011least\u2011once with ack\u2011gating.</li> <li>Persistence: local WAL for hot path, background uploads to object storage for durability and replay.</li> <li>Replay: historical fetch from WAL or Cloud with seamless handoff to live tail.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#architecture-and-design_1","title":"Architecture and Design","text":"<ul> <li>Components: Producers, consumers, stream processors, and a durable log.</li> <li>Data Flow: Producers append to a log; consumers/processors read continuously with tracked progress.</li> <li>Scaling: Designed to handle high throughput and scale horizontally by partitioning data across multiple nodes in a cluster.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#data-handling-and-processing-models_1","title":"Data Handling and Processing Models","text":""},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#streaming-producers","title":"Streaming Producers","text":"<ul> <li>Durability: Messages are appended to a persistent log. In Danube, the hot path is a local WAL with asynchronous uploads to cloud object storage, enabling replay and recovery.</li> <li>Acknowledgements: Acks are returned once the message is durably appended to the WAL (replication depends on deployment/storage backend). Latency is higher than non\u2011reliable but optimized for sub\u2011second dispatch.</li> <li>Ordering: Preserved per topic/partition.</li> <li>Delivery Guarantees: At\u2011least\u2011once (with redelivery on failure).</li> <li>Publishing Without Consumers: Allowed; messages are stored and remain available for later consumption within retention.</li> <li>Processing: Compatible with stream processing patterns (e.g., windowing, aggregations) layered above the durable log.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#streaming-consumers","title":"Streaming Consumers","text":"<ul> <li>Replay &amp; Retention: Consumers can fetch historical data within retention. In Danube, reads come from WAL if available, otherwise from Cloud with seamless handoff to the live tail.</li> <li>Acknowledgements: Consumers ack processed messages; the broker tracks subscription progress and manages redelivery.</li> <li>Fault Tolerance: On crash/reconnect, consumers resume from the last acknowledged position.</li> <li>Availability: Messages remain available even with no active consumers, subject to retention policies.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#order-of-operations-of-streaming","title":"Order of Operations of Streaming","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Durable Append: The broker appends the message to the WAL (durable on disk); background tasks upload frames to cloud storage.</li> <li>Producer Acknowledgment: The broker acknowledges once the append is durable.</li> <li>Delivery to Consumers: Messages are dispatched to consumers, gated by acknowledgments for reliable delivery.</li> <li>No Consumers: Messages remain stored and available for later consumption within retention.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/","title":"Messaging Patterns: Queuing vs Pub/Sub (Fan\u2011out)","text":"<p>This page explains the difference between queuing (point\u2011to\u2011point) and pub/sub (fan\u2011out) and shows how to enable each pattern in Danube using subscriptions and dispatch modes.</p> <ul> <li>For subscription details, see <code>architecture/subscriptions.md</code>.</li> <li>For delivery semantics, see <code>architecture/dispatch_strategy.md</code>.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#concepts-at-a-glance","title":"Concepts at a Glance","text":"<p>Queuing (Point\u2011to\u2011Point)</p> <ul> <li>One message is delivered to exactly one consumer in a group.</li> <li>Suited for work distribution (orders, tasks, jobs).</li> <li>Ordering is per consumer, and per partition when topics are partitioned.</li> </ul> <p>Pub/Sub (Fan\u2011out)</p> <ul> <li>Every subscription receives every message.</li> <li>Suited for broadcast (notifications, analytics, multiple services reacting to events).</li> <li>Each subscription processes the full stream independently.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#how-to-enable-in-danube","title":"How to enable in Danube","text":"<p>Both patterns are built with Danube subscriptions on a topic.</p> <p>Queuing (Point\u2011to\u2011Point)</p> <ul> <li>Set subscription type to <code>Shared</code>.</li> <li>Use the same subscription name across all workers (e.g., <code>orders-workers</code>).</li> <li>Result: Each message is delivered to one consumer in the group (round\u2011robin, per partition).</li> </ul> <p>Pub/Sub (Fan\u2011out)</p> <ul> <li>Create distinct subscriptions per downstream consumer/team (unique subscription names), typically using <code>Exclusive</code> (or <code>Failover</code> for HA).</li> <li>Result: Each subscription receives every message; consumers do not contend with other subscriptions.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#dispatch-modes-and-guarantees","title":"Dispatch modes and guarantees","text":"<p>Non\u2011Reliable (Pub/Sub\u2011style, best\u2011effort)</p> <ul> <li>Lowest latency; no persistence or replay.</li> <li>Messages are delivered only to active consumers. Disconnections may cause loss.</li> </ul> <p>Reliable (Streaming\u2011style, at\u2011least\u2011once)</p> <ul> <li>Messages are appended to the local WAL and asynchronously uploaded to cloud storage.</li> <li>Consumers can replay historical data; acknowledgments drive redelivery.</li> </ul> <p>See <code>architecture/persistence.md</code> for WAL + Cloud details.</p>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#partitioning-behavior","title":"Partitioning behavior","text":"<p>With partitioned topics, both patterns apply per partition.</p> <ul> <li>Queuing: messages are distributed round\u2011robin within each partition to the consumers in that subscription.</li> <li>Pub/Sub: each subscription receives each partition\u2019s messages independently.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#quick-examples","title":"Quick examples","text":"<p>Build a worker queue</p> <ul> <li>Topic: <code>/default/orders</code></li> <li>Subscription: <code>orders-workers</code> (type <code>Shared</code>)</li> <li>Run N consumers with the same subscription name. Messages are load\u2011balanced.</li> </ul> <p>Broadcast to multiple services</p> <ul> <li>Topic: <code>/default/events</code></li> <li>Subscriptions: <code>billing</code> (Exclusive), <code>analytics</code> (Exclusive), <code>monitoring</code> (Failover)</li> <li>Each subscription receives the full event stream independently.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#notes-best-practices","title":"Notes &amp; best practices","text":"<ul> <li>Use Reliable mode for durability and replay; Non\u2011Reliable for minimal latency when loss is acceptable.</li> <li>Prefer <code>Failover</code> over <code>Exclusive</code> when you need quick takeover on consumer failure.</li> <li>Size partitions to match consumer parallelism for optimal throughput.</li> </ul>"},{"location":"concepts/subscriptions/","title":"Subscription","text":"<p>A Danube subscription is a named configuration rule that determines how messages are delivered to consumers. It is a lease on a topic established by a group of consumers.</p> <p>Danube permits multiple producers and subscribers to the same topic. The Subscription Types can be combined to obtain message queueing or fan-out pub-sub messaging patterns.</p> <p></p>"},{"location":"concepts/subscriptions/#exclusive","title":"Exclusive","text":"<p>The Exclusive type is a subscription type that only allows a single consumer to attach to the subscription. If multiple consumers subscribe to a topic using the same subscription, an error occurs. This consumer has exclusive access to all messages published to the topic or partition.</p>"},{"location":"concepts/subscriptions/#exclusive-subscription-on-non-partitioned-topic","title":"Exclusive subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumer</code>: Only one consumer can be attached to the topic with an Exclusive subscription.</li> <li><code>Message Handling</code>: The single consumer handles all messages from the topic, receiving every message published to that topic.</li> </ul>"},{"location":"concepts/subscriptions/#exclusive-subscription-on-partitioned-topic-multiple-partitions","title":"Exclusive subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumer</code>: One consumer is allowed to connect to the subscription across all partitions of the partitioned topic.</li> <li><code>Message Handling</code> : This single consumer processes messages from all partitions of the partitioned topic. If a topic is partitioned into multiple partitions, the exclusive consumer handles messages from every partition.</li> </ul>"},{"location":"concepts/subscriptions/#shared","title":"Shared","text":"<p>In Danube, the Shared subscription type allows multiple consumers to attach to the same subscription. Messages are delivered in a round-robin distribution across consumers, and any given message is delivered to only one consumer.</p>"},{"location":"concepts/subscriptions/#shared-subscription-on-non-partitioned-topic","title":"Shared subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumers</code>: Multiple consumers can subscribe to the same topic.</li> <li><code>Message Handling</code>: Messages are distributed among all consumers in a round-robin fashion.</li> </ul>"},{"location":"concepts/subscriptions/#shared-subscription-on-partitioned-topic-multiple-partitions","title":"Shared subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumers</code>: Multiple consumers can subscribe to the partitioned topic.</li> <li><code>Message Handling</code>: Messages are distributed across all partitions, and then among consumers in a round-robin fashion. Each message from any partition is delivered to only one consumer.</li> </ul>"},{"location":"concepts/subscriptions/#failover","title":"Failover","text":"<p>The Failover subscription type allows multiple consumers to attach to the same subscription, with one active consumer at a time. If the active consumer disconnects or becomes unhealthy, another consumer automatically takes over. This preserves ordering and minimizes downtime.</p>"},{"location":"concepts/subscriptions/#failover-subscription-on-non-partitioned-topic","title":"Failover subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumers</code>: One active consumer processes all messages; additional consumers are in standby.</li> <li><code>Message Handling</code>: If the active consumer fails, a standby consumer takes over and continues from the last acknowledged position.</li> </ul>"},{"location":"concepts/subscriptions/#failover-subscription-on-partitioned-topic-multiple-partitions","title":"Failover subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumers</code>: One active consumer per partition; other consumers remain on standby for each partition.</li> <li><code>Message Handling</code>: Failover occurs independently per partition, ensuring continuity and ordering within each partition.</li> </ul>"},{"location":"concepts/topics/","title":"Topic","text":"<p>A topic is a unit of storage that organizes messages into a stream. As in other messaging systems, topics are named channels for transmitting messages from producers to consumers. Topic names are URLs that have a well-defined structure:</p>"},{"location":"concepts/topics/#namespacetopic_name","title":"/{namespace}/{topic_name}","text":"<p>Example: /default/markets (where default is the namespace and markets the topic)</p>"},{"location":"concepts/topics/#partitioned-topics","title":"Partitioned Topics","text":"<p>Danube support both partitioned and non-partitioned topics. The non-partitioned topics are served by a single broker, while the partitioned topic has partitiones that are served by multiple brokers within the cluster, thus allowing for higher throughput.</p> <p>A partitioned topic is implemented as N internal topics, where N is the number of partitions. When publishing messages to a partitioned topic, each message is routed to one of several brokers. The distribution of partitions across brokers is handled automatically.</p> <p></p> <p>Messages for the topic are broadcast to two consumers. The routing mode determines each message should be published to which partition, while the subscription type determines which messages go to which consumers.</p>"},{"location":"concepts/topics/#benefits-of-the-partitioned-topics","title":"Benefits of the Partitioned topics","text":"<ul> <li><code>Scalability</code>: Partitioned topics enable horizontal scaling by distributing the load across multiple partitions. This is essential for high-throughput systems that need to handle large volumes of data efficiently.</li> <li><code>Parallel Processing</code>: It allows multiple consumers to process different partitions of the same topic concurrently, improving throughput and processing efficiency.</li> <li><code>Data Locality</code>: Partitioning can help in maintaining data locality and reducing processing latency, as consumers handle a specific subset of the data (key-shared distribution not yet supported).</li> </ul>"},{"location":"concepts/topics/#creation-of-partitioned-topics","title":"Creation of Partitioned Topics","text":"<p>Partitioned topics are created with a predefined number of partitions. When you create a partitioned topic, you specify the number of partitions it should have. This number remains fixed for the lifetime of the topic, although you can configure this number at topic creation time.</p>"},{"location":"concepts/topics/#producers","title":"Producers","text":"<p>The producers routing mechanism determine which messages go to which partition.</p>"},{"location":"concepts/topics/#routing-modes","title":"Routing modes","text":"<p>When publishing to partitioned topics, you must specify a routing mode. The routing mode determines each message should be published to which partition or which internal topic.</p> <ul> <li>RoundRobinPartition - The producer will publish messages across all partitions in round-robin fashion to achieve maximum throughput. If a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> <li>SinglePartition - If no key is provided, the producer will randomly pick one single partition and publish all the messages into that partition. While if a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> </ul>"},{"location":"concepts/topics/#consumers-subscriptions","title":"Consumers (subscriptions)","text":"<p>The subscription type determines which messages go to which consumers.</p> <p>Check the Subscription documentation for details on how messages are distributed to consumers based on the subscription type.</p>"},{"location":"contributing/dev_environment/","title":"Development Environment Setup for Danube Broker","text":"<p>This document guides you through setting up the development environment, running danube broker instances, and be able to effectively contribute to the code.</p>"},{"location":"contributing/dev_environment/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following installed:</p> <ul> <li> <p>Rust: Ensure you have Rust installed. You can download and install it from the Rust website.</p> </li> <li> <p>Docker: Install Docker if you haven\u2019t already. Follow the installation instructions on the Docker website.</p> </li> </ul>"},{"location":"contributing/dev_environment/#contributing-to-the-repository","title":"Contributing to the Repository","text":"<ol> <li> <p>Fork the Repository:</p> </li> <li> <p>Go to the Danube Broker GitHub repository.</p> </li> <li> <p>Click the \"Fork\" button on the top right corner of the page to create your own copy of the repository.</p> </li> <li> <p>Clone Your Fork:</p> </li> </ol> <p>Once you have forked the repository, clone your forked repository:</p> <pre><code>git clone https://github.com/&lt;your-username&gt;/danube.git\ncd danube\n</code></pre> <ol> <li>Add the Original Repository as a Remote (optional but recommended for keeping up-to-date):</li> </ol> <pre><code>git remote add upstream https://github.com/danube-messaging/danube.git\n</code></pre>"},{"location":"contributing/dev_environment/#building-the-project","title":"Building the Project","text":"<ol> <li>Build the Project:</li> </ol> <p>To build the Danube Broker:</p> <pre><code>cargo build \nor  \ncargo build --release\n</code></pre>"},{"location":"contributing/dev_environment/#running-etcd","title":"Running ETCD","text":"<ol> <li>Start ETCD:</li> </ol> <p>Use the Makefile to start an ETCD instance. This will run ETCD in a Docker container.</p> <pre><code>make etcd\n</code></pre> <ol> <li>Clean Up ETCD:</li> </ol> <p>To stop and remove the ETCD instance and its data:</p> <pre><code>make etcd-clean\n</code></pre>"},{"location":"contributing/dev_environment/#running-a-single-broker-instance","title":"Running a Single Broker Instance","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. If not, use the <code>make etcd</code> command to start it.</p> <ol> <li>Run the Broker:</li> </ol> <p>Use the following command to start a single broker instance:</p> <pre><code>RUST_LOG=danube_broker=info target/debug/danube-broker --config-file config/danube_broker.yml\n</code></pre>"},{"location":"contributing/dev_environment/#running-multiple-broker-instances","title":"Running Multiple Broker Instances","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. Use:</p> <pre><code>make etcd\n</code></pre> <ol> <li>Run Multiple Brokers:</li> </ol> <p>Use the following Makefile command to start multiple broker instances:</p> <pre><code>make brokers\n</code></pre> <p>This will start brokers on ports 6650, 6651, and 6652. Logs for each broker will be saved in <code>temp/</code> directory.</p> <ol> <li>Clean Up Broker Instances:</li> </ol> <p>To stop all running broker instances:</p> <pre><code>make brokers-clean\n</code></pre>"},{"location":"contributing/dev_environment/#reading-logs","title":"Reading Logs","text":"<p>Logs for each broker instance are stored in the <code>temp/</code> directory. You can view them using:</p> <pre><code>cat temp/broker_&lt;port&gt;.log\n</code></pre> <p>Replace <code>&lt;port&gt;</code> with the actual port number (6650, 6651, or 6652).</p>"},{"location":"contributing/dev_environment/#inspecting-etcd-metadata","title":"Inspecting ETCD Metadata","text":"<ol> <li>Set Up <code>etcdctl</code>:</li> </ol> <p>Export the following environment variables:</p> <pre><code>export ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=http://localhost:2379\n</code></pre> <ol> <li>Inspect Metadata:</li> </ol> <p>Use <code>etcdctl</code> commands to inspect metadata. For example, to list all keys:</p> <pre><code>etcdctl get \"\" --prefix\n</code></pre> <p>To get a specific key:</p> <pre><code>etcdctl get &lt;key&gt;\n</code></pre>"},{"location":"contributing/dev_environment/#makefile-targets-summary","title":"Makefile Targets Summary","text":"<ul> <li><code>make etcd</code>: Starts an ETCD instance in Docker.</li> <li><code>make etcd-clean</code>: Stops and removes the ETCD instance and its data.</li> <li><code>make brokers</code>: Builds and starts broker instances on predefined ports.</li> <li><code>make brokers-clean</code>: Stops and removes all running broker instances.</li> </ul>"},{"location":"contributing/dev_environment/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>ETCD Not Starting: Check Docker logs and ensure no other service is using port 2379.</li> <li>Broker Not Starting: Ensure ETCD is running and accessible at the specified address and port.</li> </ul>"},{"location":"contributing/internal_resources/","title":"Resources mapping","text":"<p>This document describes how the resources are organized in the Metadata store</p>"},{"location":"contributing/internal_resources/#metadatastore-and-localcache","title":"MetadataStore and LocalCache","text":"<p>Basically the entire configuration and the metadata for all the cluster's objects (topics, namespaces, etc) are stored in MetadataStorage (ETCD) and in the LocalCache to ensure fast retrieval for the local broker and to reduce the number of request to the metadata database.</p> <p>The pattern:</p> <ul> <li>Put / Delete requests should use MetadataStore (ETCD), to ensure consistency across cluster</li> <li>Get requests should be served from the Local Cache</li> </ul> <p>The LocalCache continuously update from 2 sources for increase consistency:</p> <ul> <li>the Watch operation on ETCD</li> <li>the Syncronizer topic, where all Put / Delete requests are published and read by the brokers.</li> </ul>"},{"location":"contributing/internal_resources/#resources-types","title":"Resources Types","text":""},{"location":"contributing/internal_resources/#cluster-resources","title":"Cluster Resources","text":"<p>Holds information about the cluster and the cluster's brokers. Mainly read and write by Danube Service.</p> <ul> <li>/cluster/cluster-name</li> <li>holds a String with the name of the cluster</li> <li>/cluster/register/{broker-id}</li> <li>the broker register once it join the cluster, contain the broker metadata (broker id &amp; socket addr)  </li> <li>/cluster/brokers/{broker-id}/{namespace}/{topic}</li> <li>topics served by the broker, with value ()</li> <li>Load Manager updates the path, with topic assignments to brokers</li> <li>Brokers should watch it's own path like (/cluster/brokers/1122334455) - and perform the neccesary actions on adding or removing a topic</li> <li>/cluster/unassigned/{namespace}/{topic}</li> <li>New unassigned topics created by Broker</li> <li>Load Manager should watch this path, add assign the topic to a broker</li> <li>/cluster/load/{broker-id}</li> <li>broker periodically reports its load metrics on this path</li> <li>Load Manager watch this path to calculate broker load rankings for the cluster</li> <li>/cluster/load_balance</li> <li>the load_balance updated decision, posted by the Load Manager, contain a HashMap with keys the broker_id and value the list of topic_name</li> <li>/cluster/leader</li> <li>the value posted by Leader Election service, it holds broker_id of the current Leader of the CLuster</li> </ul> <p>Example:</p> <ul> <li>/cluster/brokers/1122334455/markets/trade-events - value is ()</li> <li>/cluster/brokers/1122334455/markets/trade-events - value is ()</li> </ul>"},{"location":"contributing/internal_resources/#namespace-resources","title":"Namespace Resources","text":"<p>Holds information about the namespace policy and the namespace's topics</p> <ul> <li>/namespaces/{namespace}/policy</li> <li>/namespaces/{namespace}/topics/{namespace}/{topic}</li> </ul> <p>Example:</p> <ul> <li>/namespaces/markets/policy - the value stores a Json like { \"retentionTimeInMinutes\": 1440 }</li> <li>/namespaces/markets/topics/markets/trade-events - topics part of the namespace, value is ()</li> </ul>"},{"location":"contributing/internal_resources/#topic-resources","title":"Topic Resources","text":"<p>Holds information about the topic policy and the associated producers / subscriptions, including partitioned topic.</p> <ul> <li>/topics/{namespace}/{topic}/policy</li> <li>holds the topic policy, the value stores a Json</li> <li>/topics/{namespace}/{topic}/schema</li> <li>holds the topic schema, the value stores the schema</li> <li>/topics/{namespace}/{topic}/producers/{producer_id}</li> <li>holds the producer config</li> <li>/topics/{namespace}/{topic}/subscriptions/{subscription_name}</li> <li>holds the subscription config</li> </ul> <p>Example:</p> <ul> <li>/topics/markets/trade-events/producers/1122334455 - with value Producer Metadata</li> <li>/topics/markets/trade-events/subscriptions/my_subscription - with value Subscription Metadata</li> <li>/topics/markets/trade-events-part-1/policy - where /markets/trade-events-part-1 is the partitioned topic that stores partition policy</li> </ul>"},{"location":"contributing/internal_resources/#subscriptions-resources","title":"Subscriptions Resources","text":"<p>Holds information about the topic subscriptions, including associated consumers</p> <ul> <li>/subscriptions/{subscription_name}/{consumer_id}</li> <li>holds the consumer metadata</li> </ul> <p>Example:</p> <ul> <li>/subscriptions/my_subscription/23232323</li> </ul>"},{"location":"danube_clis/danube_admin/brokers/","title":"Brokers Management","text":"<p>Manage and view broker information in your Danube cluster.</p>"},{"location":"danube_clis/danube_admin/brokers/#overview","title":"Overview","text":"<p>The <code>brokers</code> command provides visibility and control over the brokers in your Danube cluster. Use it to:</p> <ul> <li>List all brokers with their status</li> <li>Identify the leader broker</li> <li>View broker namespaces</li> <li>Unload topics from brokers</li> <li>Activate brokers</li> </ul>"},{"location":"danube_clis/danube_admin/brokers/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/brokers/#list-all-brokers","title":"List All Brokers","text":"<p>Display all brokers in the cluster with their details.</p> <pre><code>danube-admin-cli brokers list\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text (default) - easy to read\ndanube-admin-cli brokers list\n\n# JSON format - for scripting/automation\ndanube-admin-cli brokers list --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Broker ID    \u2502 Address             \u2502 Role     \u2502 Admin Address       \u2502 Metrics Address     \u2502 Status \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 broker-001   \u2502 127.0.0.1:6650      \u2502 leader   \u2502 127.0.0.1:50051     \u2502 127.0.0.1:9090      \u2502 active \u2502\n\u2502 broker-002   \u2502 127.0.0.1:6651      \u2502 follower \u2502 127.0.0.1:50052     \u2502 127.0.0.1:9091      \u2502 active \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Example Output (JSON):</p> <pre><code>[\n  {\n    \"broker_id\": \"broker-001\",\n    \"broker_addr\": \"127.0.0.1:6650\",\n    \"broker_role\": \"leader\",\n    \"admin_addr\": \"127.0.0.1:50051\",\n    \"metrics_addr\": \"127.0.0.1:9090\",\n    \"broker_status\": \"active\"\n  },\n  {\n    \"broker_id\": \"broker-002\",\n    \"broker_addr\": \"127.0.0.1:6651\",\n    \"broker_role\": \"follower\",\n    \"admin_addr\": \"127.0.0.1:50052\",\n    \"metrics_addr\": \"127.0.0.1:9091\",\n    \"broker_status\": \"active\"\n  }\n]\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#get-leader-broker","title":"Get Leader Broker","text":"<p>Identify which broker is currently the cluster leader.</p> <pre><code>danube-admin-cli brokers leader\n</code></pre> <p>Example Output:</p> <pre><code>Leader: broker-001\n</code></pre> <p>Why This Matters:</p> <ul> <li>The leader broker coordinates cluster operations</li> <li>Useful for debugging cluster issues</li> <li>Important for understanding cluster topology</li> </ul>"},{"location":"danube_clis/danube_admin/brokers/#list-broker-namespaces","title":"List Broker Namespaces","text":"<p>View all namespaces managed by the cluster.</p> <pre><code>danube-admin-cli brokers namespaces\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text\ndanube-admin-cli brokers namespaces\n\n# JSON format\ndanube-admin-cli brokers namespaces --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Namespaces: [\"default\", \"analytics\", \"logs\"]\n</code></pre> <p>Example Output (JSON):</p> <pre><code>[\"default\", \"analytics\", \"logs\"]\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#unload-broker-topics","title":"Unload Broker Topics","text":"<p>Gracefully unload topics from a broker (useful for maintenance or rebalancing).</p> <pre><code>danube-admin-cli brokers unload &lt;BROKER_ID&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code># Unload all topics from broker-001\ndanube-admin-cli brokers unload broker-001\n\n# Dry-run to see what would be unloaded\ndanube-admin-cli brokers unload broker-001 --dry-run\n</code></pre> <p>Advanced Options:</p> <pre><code># Unload with custom parallelism\ndanube-admin-cli brokers unload broker-001 --max-parallel 5\n\n# Unload only specific namespaces\ndanube-admin-cli brokers unload broker-001 \\\n  --namespace-include default \\\n  --namespace-include analytics\n\n# Exclude certain namespaces\ndanube-admin-cli brokers unload broker-001 \\\n  --namespace-exclude system\n\n# Set custom timeout per topic (seconds)\ndanube-admin-cli brokers unload broker-001 --timeout 30\n</code></pre> <p>Options:</p> Option Description Default <code>--dry-run</code> Preview topics to be unloaded without making changes <code>false</code> <code>--max-parallel</code> Number of topics to unload concurrently <code>1</code> <code>--namespace-include</code> Only unload topics from these namespaces (repeatable) All <code>--namespace-exclude</code> Skip topics from these namespaces (repeatable) None <code>--timeout</code> Timeout in seconds for each topic unload <code>30</code> <p>Example Output:</p> <pre><code>Unload Started: true\nTotal Topics: 45\nSucceeded: 45\nFailed: 0\nPending: 0\n</code></pre> <p>Use Cases:</p> <ul> <li>Broker Maintenance: Drain topics before shutting down a broker</li> <li>Load Rebalancing: Move topics to other brokers</li> <li>Rolling Upgrades: Safely upgrade brokers one at a time</li> </ul>"},{"location":"danube_clis/danube_admin/brokers/#activate-broker","title":"Activate Broker","text":"<p>Mark a broker as active, allowing it to receive traffic.</p> <pre><code>danube-admin-cli brokers activate &lt;BROKER_ID&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli brokers activate broker-002\n</code></pre> <p>With Audit Reason:</p> <pre><code>danube-admin-cli brokers activate broker-002 \\\n  --reason \"Maintenance completed\"\n</code></pre> <p>Example Output:</p> <pre><code>Activated: true\n</code></pre> <p>Use Cases:</p> <ul> <li>After Maintenance: Re-enable a broker after maintenance</li> <li>After Unload: Activate broker to start receiving topics again</li> <li>Cluster Expansion: Activate newly added brokers</li> </ul>"},{"location":"danube_clis/danube_admin/brokers/#common-workflows","title":"Common Workflows","text":""},{"location":"danube_clis/danube_admin/brokers/#1-health-check","title":"1. Health Check","text":"<pre><code># Check cluster health\ndanube-admin-cli brokers list\ndanube-admin-cli brokers leader\n\n# Verify all brokers are active\ndanube-admin-cli brokers list | grep -c active\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#2-broker-maintenance","title":"2. Broker Maintenance","text":"<pre><code># Step 1: Dry-run to preview unload\ndanube-admin-cli brokers unload broker-001 --dry-run\n\n# Step 2: Unload topics\ndanube-admin-cli brokers unload broker-001\n\n# Step 3: Perform maintenance (external)\n# ...\n\n# Step 4: Reactivate broker\ndanube-admin-cli brokers activate broker-001 --reason \"Maintenance completed\"\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#3-cluster-expansion","title":"3. Cluster Expansion","text":"<pre><code># List current brokers\ndanube-admin-cli brokers list\n\n# Add new broker (external process)\n# ...\n\n# Activate new broker\ndanube-admin-cli brokers activate broker-003 --reason \"New broker added\"\n\n# Verify\ndanube-admin-cli brokers list\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#quick-reference","title":"Quick Reference","text":"<pre><code># List all brokers\ndanube-admin-cli brokers list\n\n# Get leader\ndanube-admin-cli brokers leader\n\n# List namespaces\ndanube-admin-cli brokers namespaces\n\n# Unload topics (dry-run)\ndanube-admin-cli brokers unload &lt;broker-id&gt; --dry-run\n\n# Unload topics (execute)\ndanube-admin-cli brokers unload &lt;broker-id&gt; --max-parallel 5\n\n# Activate broker\ndanube-admin-cli brokers activate &lt;broker-id&gt; --reason \"Ready\"\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/","title":"Namespaces Management","text":"<p>Organize your topics with namespaces in Danube.</p>"},{"location":"danube_clis/danube_admin/namespaces/#overview","title":"Overview","text":"<p>Namespaces provide logical isolation for topics in your Danube cluster. Use namespaces to:</p> <ul> <li>Organize topics by application, team, or environment</li> <li>Apply policies at the namespace level</li> <li>Control access and resource allocation</li> <li>Separate production, staging, and development workloads</li> </ul>"},{"location":"danube_clis/danube_admin/namespaces/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/namespaces/#list-topics-in-a-namespace","title":"List Topics in a Namespace","text":"<p>View all topics within a specific namespace.</p> <pre><code>danube-admin-cli namespaces topics &lt;NAMESPACE&gt;\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli namespaces topics default\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text (default)\ndanube-admin-cli namespaces topics default\n\n# JSON format - for automation\ndanube-admin-cli namespaces topics default --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Topics in namespace 'default':\n  /default/user-events\n  /default/payment-logs\n  /default/analytics\n</code></pre> <p>Example Output (JSON):</p> <pre><code>[\n  \"/default/user-events\",\n  \"/default/payment-logs\",\n  \"/default/analytics\"\n]\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#view-namespace-policies","title":"View Namespace Policies","text":"<p>Get the policies configured for a namespace.</p> <pre><code>danube-admin-cli namespaces policies &lt;NAMESPACE&gt;\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli namespaces policies default\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text (default) - pretty printed\ndanube-admin-cli namespaces policies default\n\n# JSON format\ndanube-admin-cli namespaces policies default --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Policies for namespace 'default':\n{\n  \"max_topics_per_namespace\": 1000,\n  \"max_producers_per_topic\": 100,\n  \"max_consumers_per_topic\": 100,\n  \"message_ttl_seconds\": 604800,\n  \"retention_policy\": \"time_based\"\n}\n</code></pre> <p>Example Output (JSON):</p> <pre><code>{\n  \"max_topics_per_namespace\": 1000,\n  \"max_producers_per_topic\": 100,\n  \"max_consumers_per_topic\": 100,\n  \"message_ttl_seconds\": 604800,\n  \"retention_policy\": \"time_based\"\n}\n</code></pre> <p>Common Policies:</p> Policy Description Typical Values <code>max_topics_per_namespace</code> Maximum number of topics <code>100</code> - <code>10000</code> <code>max_producers_per_topic</code> Maximum producers per topic <code>10</code> - <code>1000</code> <code>max_consumers_per_topic</code> Maximum consumers per topic <code>10</code> - <code>1000</code> <code>message_ttl_seconds</code> Message time-to-live <code>3600</code> (1h) - <code>604800</code> (7d) <code>retention_policy</code> How messages are retained <code>time_based</code>, <code>size_based</code>"},{"location":"danube_clis/danube_admin/namespaces/#create-a-namespace","title":"Create a Namespace","text":"<p>Create a new namespace in the cluster.</p> <pre><code>danube-admin-cli namespaces create &lt;NAMESPACE&gt;\n</code></pre> <p>Basic Usage:</p> <pre><code># Create namespace\ndanube-admin-cli namespaces create production\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Namespace created: production\n</code></pre> <p>Naming Guidelines:</p> <ul> <li>Use lowercase letters and hyphens</li> <li>Keep names descriptive: <code>production</code>, <code>staging</code>, <code>dev</code></li> <li>Avoid special characters</li> <li>Use consistent naming: <code>team-app-env</code> pattern</li> </ul> <p>Examples:</p> <pre><code># By environment\ndanube-admin-cli namespaces create production\ndanube-admin-cli namespaces create staging\ndanube-admin-cli namespaces create development\n\n# By team\ndanube-admin-cli namespaces create analytics-team\ndanube-admin-cli namespaces create platform-team\n\n# By application\ndanube-admin-cli namespaces create payment-service\ndanube-admin-cli namespaces create user-service\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#delete-a-namespace","title":"Delete a Namespace","text":"<p>Remove a namespace from the cluster.</p> <pre><code>danube-admin-cli namespaces delete &lt;NAMESPACE&gt;\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli namespaces delete old-namespace\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Namespace deleted: old-namespace\n</code></pre> <p>\u26a0\ufe0f Important Warnings:</p> <ol> <li>All Topics Deleted: Deleting a namespace removes ALL topics within it</li> <li>No Confirmation: This operation is immediate and irreversible</li> <li>Active Connections: Connected producers/consumers will be disconnected</li> <li>Data Loss: All messages in the namespace are permanently deleted</li> </ol> <p>Safety Checklist:</p> <pre><code># 1. List topics before deletion\ndanube-admin-cli namespaces topics my-namespace\n\n# 2. Verify no critical topics\ndanube-admin-cli namespaces topics my-namespace --output json | grep -i critical\n\n# 3. Check policies to understand impact\ndanube-admin-cli namespaces policies my-namespace\n\n# 4. Only then delete\ndanube-admin-cli namespaces delete my-namespace\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#common-workflows","title":"Common Workflows","text":""},{"location":"danube_clis/danube_admin/namespaces/#1-namespace-setup-for-new-application","title":"1. Namespace Setup for New Application","text":"<pre><code># Create namespace\ndanube-admin-cli namespaces create payment-service\n\n# Verify creation\ndanube-admin-cli brokers namespaces | grep payment-service\n\n# Check default policies\ndanube-admin-cli namespaces policies payment-service\n\n# Create topics in namespace\ndanube-admin-cli topics create /payment-service/transactions\ndanube-admin-cli topics create /payment-service/refunds\ndanube-admin-cli topics create /payment-service/notifications\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#2-multi-environment-setup","title":"2. Multi-Environment Setup","text":"<pre><code># Create environments\ndanube-admin-cli namespaces create production\ndanube-admin-cli namespaces create staging\ndanube-admin-cli namespaces create development\n\n# List all namespaces\ndanube-admin-cli brokers namespaces\n\n# Create same topics in each environment\nfor env in production staging development; do\n  danube-admin-cli topics create /$env/user-events\n  danube-admin-cli topics create /$env/order-events\ndone\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/","title":"Schema Registry Management","text":"<p>Manage schemas for data validation and evolution in Danube.</p>"},{"location":"danube_clis/danube_admin/schema_registry/#overview","title":"Overview","text":"<p>The Schema Registry provides centralized schema management for your Danube topics. It enables:</p> <ul> <li>Type Safety: Validate messages against defined schemas</li> <li>Schema Evolution: Track and manage schema versions over time</li> <li>Compatibility Checking: Ensure new schemas don't break existing consumers</li> <li>Documentation: Schemas serve as living documentation for your data</li> </ul>"},{"location":"danube_clis/danube_admin/schema_registry/#why-use-schema-registry","title":"Why Use Schema Registry?","text":"<p>Without Schema Registry:</p> <pre><code># No validation - anything goes\nproducer.send('{\"nam\": \"John\"}')  # Typo: \"nam\" instead of \"name\"\n# Message accepted \u274c - consumers break\n</code></pre> <p>With Schema Registry:</p> <pre><code># Schema enforces structure\nproducer.send('{\"nam\": \"John\"}')  # Typo detected\n# Error: Field 'name' is required \u2705\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/schema_registry/#register-a-schema","title":"Register a Schema","text":"<p>Register a new schema or create a new version of an existing schema.</p> <pre><code>danube-admin-cli schemas register &lt;SUBJECT&gt; [OPTIONS]\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#basic-schema-registration","title":"Basic Schema Registration","text":"<p>From File (Recommended):</p> <pre><code>danube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file schemas/user-events.json\n</code></pre> <p>Inline Schema:</p> <pre><code>danube-admin-cli schemas register simple-events \\\n  --schema-type json_schema \\\n  --schema '{\"type\": \"object\", \"properties\": {\"id\": {\"type\": \"string\"}}}'\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Registered new schema version\nSubject: user-events\nSchema ID: 12345\nVersion: 1\nFingerprint: sha256:abc123...\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#with-metadata","title":"With Metadata","text":"<p>Add Description and Tags:</p> <pre><code>danube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file schemas/user-events.json \\\n  --description \"User registration and login events\" \\\n  --tags users \\\n  --tags authentication \\\n  --tags analytics\n</code></pre> <p>Tags for Organization:</p> <ul> <li><code>production</code>, <code>staging</code>, <code>development</code> - Environment</li> <li><code>team-analytics</code>, <code>team-platform</code> - Ownership</li> <li><code>pii</code>, <code>sensitive</code> - Data classification</li> <li><code>v1</code>, <code>v2</code> - Version tracking</li> <li><code>deprecated</code> - Lifecycle status</li> </ul>"},{"location":"danube_clis/danube_admin/schema_registry/#schema-types","title":"Schema Types","text":"Type Description Use Cases Extension <code>json_schema</code> JSON Schema (Draft 7) Web APIs, JavaScript/TypeScript <code>.json</code> <code>avro</code> Apache Avro Big data, Kafka integration <code>.avsc</code> <code>protobuf</code> Protocol Buffers gRPC, high performance <code>.proto</code> <code>string</code> Plain string (no validation) Simple text messages <code>.txt</code> <code>bytes</code> Raw bytes (no validation) Binary data -"},{"location":"danube_clis/danube_admin/schema_registry/#json-schema-example","title":"JSON Schema Example","text":"<p>schemas/user-events.json:</p> <pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"UserEvent\",\n  \"type\": \"object\",\n  \"required\": [\"event_type\", \"user_id\", \"timestamp\"],\n  \"properties\": {\n    \"event_type\": {\n      \"type\": \"string\",\n      \"enum\": [\"login\", \"logout\", \"register\"]\n    },\n    \"user_id\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\"\n    },\n    \"timestamp\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\"\n    },\n    \"metadata\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"ip_address\": { \"type\": \"string\" },\n        \"user_agent\": { \"type\": \"string\" }\n      }\n    }\n  }\n}\n</code></pre> <p>Register:</p> <pre><code>danube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file schemas/user-events.json \\\n  --description \"User authentication events\" \\\n  --tags users authentication\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#avro-schema-example","title":"Avro Schema Example","text":"<p>schemas/payment.avsc:</p> <pre><code>{\n  \"type\": \"record\",\n  \"name\": \"Payment\",\n  \"namespace\": \"com.example.payments\",\n  \"fields\": [\n    {\"name\": \"payment_id\", \"type\": \"string\"},\n    {\"name\": \"amount\", \"type\": \"double\"},\n    {\"name\": \"currency\", \"type\": \"string\"},\n    {\"name\": \"timestamp\", \"type\": \"long\"}\n  ]\n}\n</code></pre> <p>Register:</p> <pre><code>danube-admin-cli schemas register payment-events \\\n  --schema-type avro \\\n  --file schemas/payment.avsc \\\n  --description \"Payment transaction events\"\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#get-a-schema","title":"Get a Schema","text":"<p>Retrieve schema details by subject or ID.</p> <pre><code>danube-admin-cli schemas get [OPTIONS]\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#by-subject-latest-version","title":"By Subject (Latest Version)","text":"<pre><code># Get latest version\ndanube-admin-cli schemas get --subject user-events\n</code></pre> <p>Example Output:</p> <pre><code>Schema ID: 12345\nVersion: 2\nSubject: user-events\nType: json_schema\nCompatibility Mode: BACKWARD\nDescription: User registration and login events\nTags: users, authentication, analytics\n\nSchema Definition:\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"UserEvent\",\n  \"type\": \"object\",\n  \"required\": [\"event_type\", \"user_id\", \"timestamp\"],\n  ...\n}\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#by-schema-id","title":"By Schema ID","text":"<pre><code># Get specific schema by ID\ndanube-admin-cli schemas get --id 12345\n\n# Get specific version of a schema\ndanube-admin-cli schemas get --id 12345 --version 1\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#json-output","title":"JSON Output","text":"<pre><code>danube-admin-cli schemas get --subject user-events --output json\n</code></pre> <p>Example JSON Output:</p> <pre><code>{\n  \"schema_id\": 12345,\n  \"version\": 2,\n  \"subject\": \"user-events\",\n  \"schema_type\": \"json_schema\",\n  \"schema_definition\": \"{ ... }\",\n  \"description\": \"User registration and login events\",\n  \"created_at\": 1704067200,\n  \"created_by\": \"admin\",\n  \"tags\": [\"users\", \"authentication\", \"analytics\"],\n  \"fingerprint\": \"sha256:abc123...\",\n  \"compatibility_mode\": \"BACKWARD\"\n}\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#list-schema-versions","title":"List Schema Versions","text":"<p>View all versions for a schema subject.</p> <pre><code>danube-admin-cli schemas versions &lt;SUBJECT&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli schemas versions user-events\n</code></pre> <p>Example Output:</p> <pre><code>Versions for subject 'user-events':\n  Version 1: schema_id=12344, fingerprint=sha256:old123...\n    Created by: alice\n    Description: Initial schema\n  Version 2: schema_id=12345, fingerprint=sha256:abc123...\n    Created by: bob\n    Description: Added email field\n  Version 3: schema_id=12346, fingerprint=sha256:new456...\n    Created by: charlie\n    Description: Made phone optional\n</code></pre> <p>JSON Output:</p> <pre><code>danube-admin-cli schemas versions user-events --output json\n</code></pre> <p>Example JSON:</p> <pre><code>[\n  {\n    \"version\": 1,\n    \"schema_id\": 12344,\n    \"created_at\": 1704067200,\n    \"created_by\": \"alice\",\n    \"description\": \"Initial schema\",\n    \"fingerprint\": \"sha256:old123...\"\n  },\n  {\n    \"version\": 2,\n    \"schema_id\": 12345,\n    \"created_at\": 1704153600,\n    \"created_by\": \"bob\",\n    \"description\": \"Added email field\",\n    \"fingerprint\": \"sha256:abc123...\"\n  }\n]\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#check-schema-compatibility","title":"Check Schema Compatibility","text":"<p>Verify if a new schema is compatible with existing versions.</p> <pre><code>danube-admin-cli schemas check &lt;SUBJECT&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli schemas check user-events \\\n  --file schemas/user-events-v2.json \\\n  --schema-type json_schema\n</code></pre> <p>Example Output (Compatible):</p> <pre><code>\u2705 Schema is compatible with subject 'user-events'\n</code></pre> <p>Example Output (Incompatible):</p> <pre><code>\u274c Schema is NOT compatible with subject 'user-events'\n\nCompatibility errors:\n  - Field 'user_id' was removed (breaking change)\n  - Field 'email' is now required (breaking change for existing consumers)\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#override-compatibility-mode","title":"Override Compatibility Mode","text":"<pre><code># Check with specific mode (overrides subject's default)\ndanube-admin-cli schemas check user-events \\\n  --file schemas/user-events-v2.json \\\n  --schema-type json_schema \\\n  --mode full\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#workflow-example","title":"Workflow Example","text":"<pre><code># Step 1: Create new schema version\nvim schemas/user-events-v2.json\n\n# Step 2: Check compatibility BEFORE registering\ndanube-admin-cli schemas check user-events \\\n  --file schemas/user-events-v2.json \\\n  --schema-type json_schema\n\n# Step 3: If compatible, register it\nif [ $? -eq 0 ]; then\n  danube-admin-cli schemas register user-events \\\n    --schema-type json_schema \\\n    --file schemas/user-events-v2.json \\\n    --description \"Added email field\"\nfi\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#set-compatibility-mode","title":"Set Compatibility Mode","text":"<p>Configure how schema evolution is enforced.</p> <pre><code>danube-admin-cli schemas set-compatibility &lt;SUBJECT&gt; --mode &lt;MODE&gt;\n</code></pre> <p>Compatibility Modes:</p> Mode Description Allows Use Case <code>none</code> No compatibility checks Any changes Development, testing <code>backward</code> New schema can read old data Add optional fields, remove fields Most common - new consumers, old producers <code>forward</code> Old schema can read new data Remove optional fields, add fields New producers, old consumers <code>full</code> Both backward and forward Add/remove optional fields only Strict compatibility <p>Examples:</p> <pre><code># Set backward compatibility (most common)\ndanube-admin-cli schemas set-compatibility user-events --mode backward\n\n# Set full compatibility (strict)\ndanube-admin-cli schemas set-compatibility payment-events --mode full\n\n# Disable compatibility (development only)\ndanube-admin-cli schemas set-compatibility test-events --mode none\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Compatibility mode set for subject 'user-events'\nMode: BACKWARD\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#backward-compatibility-recommended","title":"Backward Compatibility (Recommended)","text":"<p>Allows:</p> <ul> <li>\u2705 Adding optional fields</li> <li>\u2705 Removing fields</li> <li>\u2705 Adding enum values</li> </ul> <p>Prevents:</p> <ul> <li>\u274c Removing required fields</li> <li>\u274c Changing field types</li> <li>\u274c Making optional fields required</li> </ul> <p>Example:</p> <pre><code># Old schema\n{\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"name\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\", \"name\"]\n}\n\n# New schema (backward compatible)\n{\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"name\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\"}  // \u2705 Added optional field\n  },\n  \"required\": [\"user_id\", \"name\"]\n}\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#forward-compatibility","title":"Forward Compatibility","text":"<p>Allows:</p> <ul> <li>\u2705 Removing optional fields</li> <li>\u2705 Adding fields</li> </ul> <p>Prevents:</p> <ul> <li>\u274c Adding required fields</li> <li>\u274c Changing field types</li> </ul>"},{"location":"danube_clis/danube_admin/schema_registry/#full-compatibility-strictest","title":"Full Compatibility (Strictest)","text":"<p>Allows:</p> <ul> <li>\u2705 Only changes that are both backward AND forward compatible</li> <li>\u2705 Adding optional fields with defaults</li> <li>\u2705 Removing optional fields</li> </ul> <p>Prevents:</p> <ul> <li>\u274c Most breaking changes</li> <li>\u274c Required field modifications</li> </ul>"},{"location":"danube_clis/danube_admin/schema_registry/#delete-schema-version","title":"Delete Schema Version","text":"<p>Remove a specific version of a schema.</p> <pre><code>danube-admin-cli schemas delete &lt;SUBJECT&gt; --version &lt;VERSION&gt; --confirm\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli schemas delete user-events --version 1 --confirm\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Deleted version 1 of subject 'user-events'\n</code></pre> <p>\u26a0\ufe0f Important Notes:</p> <ol> <li>Requires Confirmation: Must use <code>--confirm</code> flag to prevent accidents</li> <li>Cannot Delete Active: Cannot delete version currently used by topics</li> <li>No Undo: Deletion is permanent</li> <li>Version History: Gaps in version numbers are normal after deletion</li> </ol> <p>Safety Checks:</p> <pre><code># Step 1: List all versions\ndanube-admin-cli schemas versions user-events\n\n# Step 2: Check which version is active\ndanube-admin-cli topics describe /production/events | grep \"Version:\"\n\n# Step 3: Only delete if not active and confirmed safe\ndanube-admin-cli schemas delete user-events --version 1 --confirm\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/","title":"Topics Management","text":"<p>Create and manage topics in your Danube cluster.</p>"},{"location":"danube_clis/danube_admin/topics/#overview","title":"Overview","text":"<p>Topics are the fundamental messaging primitive in Danube. They provide:</p> <ul> <li>Named channels for publishing and subscribing to messages</li> <li>Schema enforcement via Schema Registry</li> <li>Partitioning for horizontal scaling</li> <li>Reliable or non-reliable delivery modes</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/topics/#list-topics","title":"List Topics","text":"<p>View topics in a namespace or on a specific broker.</p> <pre><code>danube-admin-cli topics list [OPTIONS]\n</code></pre> <p>By Namespace:</p> <pre><code># List all topics in a namespace\ndanube-admin-cli topics list --namespace default\n\n# JSON output for automation\ndanube-admin-cli topics list --namespace default --output json\n</code></pre> <p>By Broker:</p> <pre><code># List topics on a specific broker\ndanube-admin-cli topics list --broker broker-001\n\n# JSON output\ndanube-admin-cli topics list --broker broker-001 --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Topics in namespace 'default':\n  /default/user-events\n  /default/payment-transactions\n  /default/analytics-stream\n</code></pre> <p>Example Output (JSON):</p> <pre><code>[\n  \"/default/user-events\",\n  \"/default/payment-transactions\",\n  \"/default/analytics-stream\"\n]\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#create-a-topic","title":"Create a Topic","text":"<p>Create a new topic with optional schema validation.</p> <pre><code>danube-admin-cli topics create &lt;TOPIC&gt; [OPTIONS]\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#basic-topic-creation","title":"Basic Topic Creation","text":"<p>Simple Topic (No Schema):</p> <pre><code># Create topic without schema\ndanube-admin-cli topics create /default/logs\n\n# Create with reliable delivery\ndanube-admin-cli topics create /default/events --dispatch-strategy reliable\n</code></pre> <p>Using Namespace Flag:</p> <pre><code># Specify namespace separately\ndanube-admin-cli topics create my-topic --namespace default\n\n# Equivalent to\ndanube-admin-cli topics create /default/my-topic\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#schema-validated-topics","title":"Schema-Validated Topics","text":"<p>With Schema Registry:</p> <pre><code># First, register a schema\ndanube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file user-schema.json\n\n# Create topic with schema validation\ndanube-admin-cli topics create /default/user-events \\\n  --schema-subject user-events \\\n  --dispatch-strategy reliable\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Topic created: /default/user-events\n   Schema subject: user-events\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#partitioned-topics","title":"Partitioned Topics","text":"<p>Create with Partitions:</p> <pre><code># Create partitioned topic (3 partitions)\ndanube-admin-cli topics create /default/high-throughput \\\n  --partitions 3\n\n# With schema and partitions\ndanube-admin-cli topics create /default/user-events \\\n  --partitions 5 \\\n  --schema-subject user-events \\\n  --dispatch-strategy reliable\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Partitioned topic created: /default/high-throughput\n   Schema subject: user-events\n   Partitions: 5\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#options-reference","title":"Options Reference","text":"Option Description Default Example <code>--namespace</code> Namespace (if not in topic path) - <code>--namespace default</code> <code>--partitions</code> Number of partitions 1 <code>--partitions 3</code> <code>--schema-subject</code> Schema subject from registry None <code>--schema-subject user-events</code> <code>--dispatch-strategy</code> Delivery mode <code>non_reliable</code> <code>--dispatch-strategy reliable</code> <p>Dispatch Strategies:</p> <ul> <li>non_reliable: Fast, at-most-once delivery (fire-and-forget)</li> <li>Use for: Logs, metrics, non-critical events</li> <li>Pros: Low latency, high throughput</li> <li> <p>Cons: Messages may be lost</p> </li> <li> <p>reliable: Slower, at-least-once delivery (with acknowledgments)</p> </li> <li>Use for: Transactions, orders, critical events</li> <li>Pros: Guaranteed delivery</li> <li>Cons: Higher latency</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#describe-a-topic","title":"Describe a Topic","text":"<p>View detailed information about a topic including schema and subscriptions.</p> <pre><code>danube-admin-cli topics describe &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli topics describe /default/user-events\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text (default) - human-readable\ndanube-admin-cli topics describe /default/user-events\n\n# JSON format - for automation\ndanube-admin-cli topics describe /default/user-events --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Topic: /default/user-events\nBroker ID: broker-001\nDelivery: Reliable\n\n\ud83d\udccb Schema Registry:\n  Subject: user-events\n  Schema ID: 12345\n  Version: 2\n  Type: json_schema\n  Compatibility: BACKWARD\n\nSubscriptions: [\"analytics-consumer\", \"audit-logger\"]\n</code></pre> <p>Example Output (JSON):</p> <pre><code>{\n  \"topic\": \"/default/user-events\",\n  \"broker_id\": \"broker-001\",\n  \"delivery\": \"Reliable\",\n  \"schema_subject\": \"user-events\",\n  \"schema_id\": 12345,\n  \"schema_version\": 2,\n  \"schema_type\": \"json_schema\",\n  \"compatibility_mode\": \"BACKWARD\",\n  \"subscriptions\": [\n    \"analytics-consumer\",\n    \"audit-logger\"\n  ]\n}\n</code></pre> <p>Without Schema:</p> <pre><code>Topic: /default/logs\nBroker ID: broker-002\nDelivery: NonReliable\n\n\ud83d\udccb Schema: None\n\nSubscriptions: []\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#list-subscriptions","title":"List Subscriptions","text":"<p>View all active subscriptions for a topic.</p> <pre><code>danube-admin-cli topics subscriptions &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli topics subscriptions /default/user-events\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text\ndanube-admin-cli topics subscriptions /default/user-events\n\n# JSON format\ndanube-admin-cli topics subscriptions /default/user-events --output json\n</code></pre> <p>Example Output:</p> <pre><code>Subscriptions: [\"consumer-1\", \"consumer-2\", \"analytics-team\"]\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#delete-a-topic","title":"Delete a Topic","text":"<p>Permanently remove a topic and all its messages.</p> <pre><code>danube-admin-cli topics delete &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli topics delete /default/old-topic\n</code></pre> <p>With Namespace:</p> <pre><code>danube-admin-cli topics delete old-topic --namespace default\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Topic deleted: /default/old-topic\n</code></pre> <p>\u26a0\ufe0f Important Warnings:</p> <ol> <li>Data Loss: All messages in the topic are permanently deleted</li> <li>No Confirmation: Operation is immediate and irreversible</li> <li>Active Subscriptions: All consumers will be disconnected</li> <li>Schema Intact: The schema in the registry is NOT deleted</li> </ol> <p>Safety Checklist:</p> <pre><code># 1. Check subscriptions\ndanube-admin-cli topics subscriptions /default/my-topic\n\n# 2. Verify topic details\ndanube-admin-cli topics describe /default/my-topic\n\n# 3. Backup if needed (application-level)\n\n# 4. Delete topic\ndanube-admin-cli topics delete /default/my-topic\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#unsubscribe","title":"Unsubscribe","text":"<p>Remove a specific subscription from a topic.</p> <pre><code>danube-admin-cli topics unsubscribe &lt;TOPIC&gt; --subscription &lt;NAME&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli topics unsubscribe /default/user-events \\\n  --subscription old-consumer\n</code></pre> <p>With Namespace:</p> <pre><code>danube-admin-cli topics unsubscribe my-topic \\\n  --namespace default \\\n  --subscription old-consumer\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Unsubscribed: true\n</code></pre> <p>Use Cases:</p> <ul> <li>Remove inactive consumers</li> <li>Clean up test subscriptions</li> <li>Force consumer reconnection</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#unload-a-topic","title":"Unload a Topic","text":"<p>Gracefully unload a topic from its current broker.</p> <pre><code>danube-admin-cli topics unload &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli topics unload /default/user-events\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Topic unloaded: /default/user-events\n</code></pre> <p>Use Cases:</p> <ul> <li>Rebalance topics across brokers</li> <li>Prepare for broker maintenance</li> <li>Move topic to different broker</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#common-workflows","title":"Common Workflows","text":""},{"location":"danube_clis/danube_admin/topics/#1-create-topic-with-schema-validation","title":"1. Create Topic with Schema Validation","text":"<p>Step-by-step:</p> <pre><code># Step 1: Register schema\ndanube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file schemas/user-events.json \\\n  --description \"User event schema\" \\\n  --tags users analytics\n\n# Step 2: Verify schema\ndanube-admin-cli schemas get --subject user-events\n\n# Step 3: Create topic\ndanube-admin-cli topics create /production/user-events \\\n  --schema-subject user-events \\\n  --dispatch-strategy reliable \\\n  --partitions 5\n\n# Step 4: Verify topic\ndanube-admin-cli topics describe /production/user-events\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#2-schema-evolution","title":"2. Schema Evolution","text":"<p>Update schema for existing topic:</p> <pre><code># Step 1: Check current compatibility mode\ndanube-admin-cli schemas get --subject user-events\n\n# Step 2: Test new schema compatibility\ndanube-admin-cli schemas check user-events \\\n  --file schemas/user-events-v2.json \\\n  --schema-type json_schema\n\n# Step 3: If compatible, register new version\ndanube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file schemas/user-events-v2.json \\\n  --description \"Added email field\"\n\n# Step 4: Verify topic picked up new version\ndanube-admin-cli topics describe /production/user-events\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#3-multi-environment-deployment","title":"3. Multi-Environment Deployment","text":"<p>Create same topics across environments:</p> <pre><code># Production\ndanube-admin-cli topics create /production/user-events \\\n  --schema-subject user-events \\\n  --dispatch-strategy reliable \\\n  --partitions 10\n\n# Staging\ndanube-admin-cli topics create /staging/user-events \\\n  --schema-subject user-events \\\n  --dispatch-strategy reliable \\\n  --partitions 3\n\n# Development\ndanube-admin-cli topics create /development/user-events \\\n  --schema-subject user-events-dev \\\n  --partitions 1\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#4-topic-migration","title":"4. Topic Migration","text":"<p>Move topic to new namespace:</p> <pre><code># Step 1: Create new namespace\ndanube-admin-cli namespaces create new-namespace\n\n# Step 2: Get old topic schema\nOLD_SCHEMA=$(danube-admin-cli topics describe /old/topic --output json | jq -r '.schema_subject')\n\n# Step 3: Create new topic with same schema\ndanube-admin-cli topics create /new-namespace/topic \\\n  --schema-subject $OLD_SCHEMA \\\n  --dispatch-strategy reliable\n\n# Step 4: Migrate consumers (application-level)\n\n# Step 5: Delete old topic\ndanube-admin-cli topics delete /old/topic\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#related-commands","title":"Related Commands","text":"<ul> <li><code>danube-admin-cli schemas register</code> - Register schemas for validation</li> <li><code>danube-admin-cli schemas get</code> - View schema details</li> <li><code>danube-admin-cli namespaces create</code> - Create namespaces for topics</li> <li><code>danube-admin-cli brokers list</code> - View broker topology</li> </ul>"},{"location":"danube_clis/danube_cli/consumer/","title":"Consumer Guide","text":"<p>Learn how to consume messages from Danube topics. \ud83d\udce5</p>"},{"location":"danube_clis/danube_cli/consumer/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic Usage</li> <li>Subscription Types</li> <li>Schema-Based Consumption</li> <li>Advanced Patterns</li> <li>Practical Examples</li> </ul>"},{"location":"danube_clis/danube_cli/consumer/#basic-usage","title":"Basic Usage","text":""},{"location":"danube_clis/danube_cli/consumer/#simple-consumption","title":"Simple Consumption","text":"<pre><code>danube-cli consume \\\n  --service-addr http://localhost:6650 \\\n  --subscription my-subscription\n</code></pre> <p>Consumes from the default topic <code>/default/test_topic</code> with a shared subscription.</p>"},{"location":"danube_clis/danube_cli/consumer/#custom-topic","title":"Custom Topic","text":"<pre><code>danube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  -m order-processors\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#using-short-flags","title":"Using Short Flags","text":"<pre><code>danube-cli consume -s http://localhost:6650 -t /default/events -m event-sub\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#custom-consumer-name","title":"Custom Consumer Name","text":"<pre><code>danube-cli consume \\\n  -s http://localhost:6650 \\\n  -n order-consumer-1 \\\n  -m order-subscription\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#subscription-types","title":"Subscription Types","text":""},{"location":"danube_clis/danube_cli/consumer/#shared-default","title":"Shared (Default)","text":"<p>Multiple consumers share message processing. Messages are distributed across consumers.</p> <pre><code># Consumer 1\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  -m shared-sub \\\n  --sub-type shared\n\n# Consumer 2 (run in parallel)\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  -m shared-sub \\\n  --sub-type shared\n</code></pre> <p>Use case: Load balancing, parallel processing</p>"},{"location":"danube_clis/danube_cli/consumer/#exclusive","title":"Exclusive","text":"<p>Only one consumer can be active at a time. Ensures ordered processing.</p> <pre><code>danube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  -m exclusive-sub \\\n  --sub-type exclusive\n</code></pre> <p>Use case: Ordered message processing, single consumer workflows</p>"},{"location":"danube_clis/danube_cli/consumer/#failover","title":"Failover","text":"<p>Multiple consumers but only one is active. Others act as standby.</p> <pre><code># Primary consumer\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/critical \\\n  -m ha-sub \\\n  --sub-type fail-over\n\n# Standby consumer (automatically takes over if primary fails)\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/critical \\\n  -m ha-sub \\\n  --sub-type fail-over\n</code></pre> <p>Use case: High availability, ordered processing with failover</p>"},{"location":"danube_clis/danube_cli/consumer/#schema-based-consumption","title":"Schema-Based Consumption","text":""},{"location":"danube_clis/danube_cli/consumer/#auto-detection","title":"Auto-Detection","text":"<p>The consumer automatically detects and validates against the topic's schema:</p> <pre><code>danube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  -m order-sub\n</code></pre> <p>Output shows schema validation:</p> <pre><code>\ud83d\udd0d Checking for schema associated with topic...\n\u2705 Topic has schema: orders (json_schema, version 1)\n\ud83d\udce5 Consuming with schema validation...\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#without-schema","title":"Without Schema","text":"<p>If the topic has no schema:</p> <pre><code>danube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/raw-data \\\n  -m raw-sub\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd0d Checking for schema associated with topic...\n\u2139\ufe0f  Topic has no schema - consuming raw bytes\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#schema-evolution","title":"Schema Evolution","text":"<p>Consumers automatically handle schema evolution:</p> <pre><code># Producer sends message with v1 schema\ndanube-cli produce \\\n  -t /default/users \\\n  --schema-subject users \\\n  -m '{\"user_id\":\"123\",\"name\":\"Alice\"}'\n\n# Schema evolves to v2 (adds optional \"email\" field)\ndanube-cli schema register users \\\n  --schema-type json_schema \\\n  --file users-v2.json\n\n# Consumer automatically uses latest schema\ndanube-cli consume \\\n  -t /default/users \\\n  -m user-processors\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"danube_clis/danube_cli/consumer/#fan-out-pattern","title":"Fan-Out Pattern","text":"<p>Multiple subscriptions on the same topic for different purposes:</p> <pre><code># Subscription 1: Process orders\ndanube-cli consume -t /default/orders -m order-processing &amp;\n\n# Subscription 2: Analytics\ndanube-cli consume -t /default/orders -m order-analytics &amp;\n\n# Subscription 3: Notifications\ndanube-cli consume -t /default/orders -m order-notifications &amp;\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#worker-pool-pattern","title":"Worker Pool Pattern","text":"<p>Multiple consumers in a shared subscription for parallel processing:</p> <pre><code># Start 4 workers\nfor i in {1..4}; do\n  danube-cli consume \\\n    -s http://localhost:6650 \\\n    -t /default/tasks \\\n    -n \"worker-$i\" \\\n    -m task-workers \\\n    --sub-type shared &amp;\ndone\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#multi-stage-pipeline","title":"Multi-Stage Pipeline","text":"<p>Process messages through multiple stages:</p> <pre><code># Stage 1: Consume from source\ndanube-cli consume -t /pipeline/raw-events -m stage1 &amp;\n\n# Stage 2: Consume from enriched\ndanube-cli consume -t /pipeline/enriched-events -m stage2 &amp;\n\n# Stage 3: Consume from processed\ndanube-cli consume -t /pipeline/processed-events -m stage3 &amp;\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#practical-examples","title":"Practical Examples","text":""},{"location":"danube_clis/danube_cli/consumer/#e-commerce-order-processing","title":"E-Commerce Order Processing","text":"<pre><code># Multiple workers processing orders\nfor i in {1..3}; do\n  danube-cli consume \\\n    -s http://localhost:6650 \\\n    -t /default/orders \\\n    -n \"order-worker-$i\" \\\n    -m order-processors \\\n    --sub-type shared &amp;\ndone\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#real-time-analytics","title":"Real-Time Analytics","text":"<pre><code># Exclusive consumer for ordered analytics\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /analytics/events \\\n  -m analytics-processor \\\n  --sub-type exclusive\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#event-driven-microservices","title":"Event-Driven Microservices","text":"<pre><code># Service 1: User service\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/user-events \\\n  -m user-service &amp;\n\n# Service 2: Email service\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/user-events \\\n  -m email-service &amp;\n\n# Service 3: Analytics service\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/user-events \\\n  -m analytics-service &amp;\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#iot-data-collection","title":"IoT Data Collection","text":"<pre><code># Consume sensor data with high availability\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /iot/sensors \\\n  -m sensor-processor \\\n  --sub-type fail-over\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#log-aggregation","title":"Log Aggregation","text":"<pre><code># Consume logs from multiple sources\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /logs/application \\\n  -m log-aggregator \\\n  --sub-type shared\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#notification-service","title":"Notification Service","text":"<pre><code># Process notifications with worker pool\nfor i in {1..5}; do\n  danube-cli consume \\\n    -s http://localhost:6650 \\\n    -t /notifications/queue \\\n    -n \"notification-worker-$i\" \\\n    -m notification-processors \\\n    --sub-type shared &amp;\ndone\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#message-output-format","title":"Message Output Format","text":""},{"location":"danube_clis/danube_cli/consumer/#text-messages","title":"Text Messages","text":"<pre><code>Received message: Hello, World!\nSize: 13 bytes, Total received: 13 bytes\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#json-messages","title":"JSON Messages","text":"<pre><code>Received message: {\"user_id\":\"123\",\"action\":\"login\"}\nSize: 35 bytes, Total received: 35 bytes\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#binary-data","title":"Binary Data","text":"<pre><code>Received message: [binary data - 1024 bytes]\nSize: 1024 bytes, Total received: 1024 bytes\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#with-schema-validation","title":"With Schema Validation","text":"<pre><code>\u2705 Message validated against schema 'orders' (version 1)\nReceived message: {\"order_id\":\"ord_123\",\"amount\":99.99}\nSize: 42 bytes, Total received: 42 bytes\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#command-reference","title":"Command Reference","text":""},{"location":"danube_clis/danube_cli/consumer/#all-consumer-flags","title":"All Consumer Flags","text":"Flag Short Description Default <code>--service-addr</code> <code>-s</code> Broker URL Required <code>--topic</code> <code>-t</code> Topic name <code>/default/test_topic</code> <code>--subscription</code> <code>-m</code> Subscription name Required <code>--consumer-name</code> <code>-n</code> Consumer name <code>consumer_pubsub</code> <code>--sub-type</code> - Subscription type <code>shared</code>"},{"location":"danube_clis/danube_cli/consumer/#subscription-types_1","title":"Subscription Types","text":"Type Description Use Case <code>shared</code> Load balanced across consumers Parallel processing <code>exclusive</code> Single active consumer Ordered processing <code>fail-over</code> Active/standby with failover HA ordered processing"},{"location":"danube_clis/danube_cli/consumer/#scripting-with-consumers","title":"Scripting with Consumers","text":""},{"location":"danube_clis/danube_cli/consumer/#basic-shell-script","title":"Basic Shell Script","text":"<pre><code>#!/bin/bash\n\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  -m event-processor | \\\nwhile IFS= read -r line; do\n  echo \"Processing: $line\"\n  # Your processing logic here\ndone\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#filter-messages","title":"Filter Messages","text":"<pre><code>#!/bin/bash\n\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  -m event-filter | \\\nwhile IFS= read -r line; do\n  # Process only specific messages\n  if echo \"$line\" | grep -q \"error\"; then\n    echo \"Error detected: $line\"\n    # Send alert\n  fi\ndone\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/","title":"Danube CLI Documentation","text":"<p>Welcome to the Danube CLI - command-line companion for interacting with Danube messaging system! \ud83d\ude80</p>"},{"location":"danube_clis/danube_cli/getting_started/#what-is-danube-cli","title":"What is Danube CLI?","text":"<p>Danube CLI is a powerful, easy-to-use command-line tool that lets you:</p> <ul> <li>\ud83d\udce4 Produce messages to topics with schema validation</li> <li>\ud83d\udce5 Consume messages from topics with automatic schema detection</li> <li>\ud83d\udccb Manage schemas in the schema registry</li> <li>\ud83d\udd04 Test your Danube deployment end-to-end</li> <li>\ud83d\udee0\ufe0f Develop and debug messaging workflows</li> </ul>"},{"location":"danube_clis/danube_cli/getting_started/#core-concepts","title":"Core Concepts","text":""},{"location":"danube_clis/danube_cli/getting_started/#topics","title":"Topics","text":"<p>Topics are logical channels where messages are published and consumed. Topic names follow a hierarchical structure:</p> <pre><code>/namespace/topic-name\n</code></pre> <p>Example: <code>/default/user-events</code>, <code>/production/orders</code></p>"},{"location":"danube_clis/danube_cli/getting_started/#producers","title":"Producers","text":"<p>Producers send messages to topics. They can:</p> <ul> <li>Send messages with or without schemas</li> <li>Configure partitioning for scalability</li> <li>Enable reliable delivery for critical messages</li> </ul>"},{"location":"danube_clis/danube_cli/getting_started/#consumers","title":"Consumers","text":"<p>Consumers receive messages from topics via subscriptions. They support:</p> <ul> <li>Multiple subscription types (Exclusive, Shared, Failover)</li> <li>Automatic schema validation</li> <li>Message acknowledgment</li> </ul>"},{"location":"danube_clis/danube_cli/getting_started/#schema-registry","title":"Schema Registry","text":"<p>The schema registry provides:</p> <ul> <li>Centralized schema management</li> <li>Schema evolution with compatibility checking</li> <li>Automatic validation for producers and consumers</li> </ul> <p>Whether you're testing a new deployment, debugging message flows, or building automation scripts, Danube CLI has you covered!</p>"},{"location":"danube_clis/danube_cli/getting_started/#quick-start","title":"Quick Start","text":"<p>Download the latest release for your system from Danube Releases:</p> <pre><code># Linux\nwget https://github.com/danube-messaging/danube/releases/download/v0.6.0/danube-cli-linux\nchmod +x danube-cli-linux\n\n# macOS (Apple Silicon)\nwget https://github.com/danube-messaging/danube/releases/download/v0.6.0/danube-cli-macos\nchmod +x danube-cli-macos\n\n# Windows\n# Download danube-cli-windows.exe from the releases page\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#add-to-path-optional","title":"Add to PATH (Optional)","text":"<p>For easier access, add the binary to your PATH:</p> <pre><code># Option 1: Copy to a directory in your PATH\nsudo cp danube-cli-linux /usr/local/bin/danube-cli\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#verify-installation","title":"Verify Installation","text":"<pre><code>danube-cli --version\ndanube-cli --help\n</code></pre> <p>You should see the CLI version and help information!</p> <p>Let's send and receive a simple message!</p>"},{"location":"danube_clis/danube_cli/getting_started/#your-first-message","title":"Your First Message","text":""},{"location":"danube_clis/danube_cli/getting_started/#step-1-produce-messages","title":"Step 1: Produce Messages","text":"<p>First, let's produce some messages (this also creates the topic):</p> <pre><code>danube-cli produce \\\n  --service-addr http://localhost:6650 \\\n  --topic /default/getting-started \\\n  --message \"Hello from Danube CLI!\" \\\n  --count 5\n</code></pre> <p>You should see:</p> <pre><code>\u2705 Producer 'test_producer' created successfully\n\ud83d\udce4 Message 1/5 sent successfully (ID: ...)\n\ud83d\udce4 Message 2/5 sent successfully (ID: ...)\n\ud83d\udce4 Message 3/5 sent successfully (ID: ...)\n\ud83d\udce4 Message 4/5 sent successfully (ID: ...)\n\ud83d\udce4 Message 5/5 sent successfully (ID: ...)\n\ud83d\udcca Summary:\n   \u2705 Success: 5\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#step-2-start-a-consumer","title":"Step 2: Start a Consumer","text":"<p>Now open a new terminal and start consuming the messages:</p> <pre><code>danube-cli consume \\\n  --service-addr http://localhost:6650 \\\n  --topic /default/getting-started \\\n  --subscription my-first-subscription\n</code></pre> <p>You should see:</p> <pre><code>\ud83d\udd0d Checking for schema associated with topic...\n\u2139\ufe0f  Topic has no schema - consuming raw bytes\nReceived message: Hello from Danube CLI!\nSize: 24 bytes, Total received: 24 bytes\nReceived message: Hello from Danube CLI!\nSize: 24 bytes, Total received: 48 bytes\n...\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#example-workflows","title":"Example Workflows","text":""},{"location":"danube_clis/danube_cli/getting_started/#add-a-schema","title":"Add a Schema","text":"<p>Schemas ensure your messages have the right structure:</p> <pre><code># 1. Create a simple schema file\ncat &gt; /tmp/user-schema.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"action\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\", \"action\"]\n}\nEOF\n\n# 2. Register the schema\ndanube-cli schema register user-events \\\n  --schema-type json_schema \\\n  --file /tmp/user-schema.json\n\n# 3. Produce with schema validation\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/user-events \\\n  --schema-subject user-events \\\n  -m '{\"user_id\":\"user_123\",\"action\":\"login\"}' \\\n  --count 5\n\n# 4. Consume with automatic validation\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/user-events \\\n  -m user-subscription\n</code></pre> <p>The consumer will automatically validate messages against the schema!</p>"},{"location":"danube_clis/danube_cli/getting_started/#send-multiple-messages","title":"Send Multiple Messages","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/test \\\n  -m \"Message\" \\\n  --count 20 \\\n  --interval 500\n</code></pre> <p>This sends 20 messages with a 500ms delay between each.</p>"},{"location":"danube_clis/danube_cli/getting_started/#use-different-subscription-types","title":"Use Different Subscription Types","text":"<pre><code># Exclusive: Only one consumer at a time\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/test \\\n  -m my-exclusive \\\n  --sub-type exclusive\n\n# Shared: Multiple consumers share messages\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/test \\\n  -m my-shared \\\n  --sub-type shared\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#common-patterns","title":"Common Patterns","text":""},{"location":"danube_clis/danube_cli/getting_started/#pattern-1-quick-test-message","title":"Pattern 1: Quick Test Message","text":"<pre><code># Shortest way to send a message\ndanube-cli produce -s http://localhost:6650 -m \"test\"\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#pattern-2-binary-files","title":"Pattern 2: Binary Files","text":"<pre><code># Send a file as a message\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  --file /path/to/data.bin\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#pattern-3-messages-with-metadata","title":"Pattern 3: Messages with Metadata","text":"<pre><code># Add attributes for routing/filtering\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Alert!\" \\\n  --attributes \"priority:high,region:us-west\"\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#pattern-4-partitioned-topics","title":"Pattern 4: Partitioned Topics","text":"<pre><code># Create a topic with partitions\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  --partitions 4 \\\n  -m \"Partitioned message\"\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#pattern-5-reliable-delivery","title":"Pattern 5: Reliable Delivery","text":"<pre><code># Guarantee message delivery\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Important message\" \\\n  --reliable\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#tips-for-success","title":"Tips for Success","text":""},{"location":"danube_clis/danube_cli/getting_started/#1-check-examples-in-help","title":"1. Check Examples in Help","text":"<p>Every command has examples built-in:</p> <pre><code>danube-cli produce --help     # See producer examples\ndanube-cli consume --help     # See consumer examples\ndanube-cli schema --help      # See schema examples\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#2-json-output-for-scripting","title":"2. JSON Output for Scripting","text":"<p>Use <code>--output json</code> for programmatic parsing:</p> <pre><code>danube-cli schema get user-events --output json | jq .\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#3-descriptive-names","title":"3. Descriptive Names","text":"<p>Use meaningful names for easier debugging:</p> <pre><code>danube-cli produce \\\n  --producer-name order-service-producer \\\n  --topic /production/orders \\\n  -m '{\"order_id\":\"123\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#whats-next","title":"What's Next?","text":"<p>You can explore more advanced features:</p> <ol> <li>\ud83d\udce4 Producer Guide - Message production</li> <li>\ud83d\udce5 Consumer Guide - Message consumption</li> <li>\ud83d\udccb Schema Registry - Schema management</li> </ol>"},{"location":"danube_clis/danube_cli/producer/","title":"Producer Guide","text":"<p>Learn how to produce messages to Danube topics. \ud83d\udce4</p>"},{"location":"danube_clis/danube_cli/producer/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic Usage</li> <li>Message Content Types</li> <li>Multiple Messages</li> <li>Message Attributes</li> <li>Schema-Based Production</li> <li>Partitioned Topics</li> <li>Reliable Delivery</li> </ul>"},{"location":"danube_clis/danube_cli/producer/#basic-usage","title":"Basic Usage","text":""},{"location":"danube_clis/danube_cli/producer/#simple-message","title":"Simple Message","text":"<pre><code>danube-cli produce \\\n  --service-addr http://localhost:6650 \\\n  --message \"Hello, World!\"\n</code></pre> <p>Sends one message to the default topic <code>/default/test_topic</code>.</p>"},{"location":"danube_clis/danube_cli/producer/#custom-topic","title":"Custom Topic","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  -m \"Order received\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#using-short-flags","title":"Using Short Flags","text":"<pre><code>danube-cli produce -s http://localhost:6650 -t /default/events -m \"Quick message\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#message-content-types","title":"Message Content Types","text":""},{"location":"danube_clis/danube_cli/producer/#text-messages","title":"Text Messages","text":"<pre><code>danube-cli produce -s http://localhost:6650 -m \"Simple text message\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#json-messages","title":"JSON Messages","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -m '{\"user_id\":\"123\",\"action\":\"login\",\"timestamp\":\"2024-01-15T10:30:00Z\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#binary-files","title":"Binary Files","text":"<pre><code># Send an image\ndanube-cli produce -s http://localhost:6650 --file image.png\n\n# Send any binary file\ndanube-cli produce -s http://localhost:6650 --file data.bin\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#multiple-messages","title":"Multiple Messages","text":""},{"location":"danube_clis/danube_cli/producer/#send-multiple-times","title":"Send Multiple Times","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Repeated message\" \\\n  --count 10\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#with-interval","title":"With Interval","text":"<pre><code># Send 100 messages with 500ms delay between each\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Message\" \\\n  --count 100 \\\n  --interval 500\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#rapid-fire","title":"Rapid Fire","text":"<pre><code># Minimum interval is 100ms\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Fast message\" \\\n  --count 1000 \\\n  --interval 100\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#message-attributes","title":"Message Attributes","text":""},{"location":"danube_clis/danube_cli/producer/#single-attribute","title":"Single Attribute","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Alert!\" \\\n  --attributes \"priority:high\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#multiple-attributes","title":"Multiple Attributes","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"User action\" \\\n  --attributes \"user_id:123,region:us-west,priority:high\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#schema-based-production","title":"Schema-Based Production","text":""},{"location":"danube_clis/danube_cli/producer/#with-pre-registered-schema","title":"With Pre-Registered Schema","text":"<pre><code># First, register the schema (one time)\ndanube-cli schema register orders \\\n  --schema-type json_schema \\\n  --file order-schema.json\n\n# Produce with schema validation\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  --schema-subject orders \\\n  -m '{\"order_id\":\"ord_123\",\"amount\":99.99,\"currency\":\"USD\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#auto-register-schema","title":"Auto-Register Schema","text":"<pre><code># Schema will be registered automatically if it doesn't exist\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  --schema-file event-schema.json \\\n  --schema-type json_schema \\\n  -m '{\"event\":\"user_signup\",\"user_id\":\"user_456\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#schema-types","title":"Schema Types","text":"<p>JSON Schema:</p> <pre><code>danube-cli produce \\\n  --schema-subject my-schema \\\n  --schema-type json_schema \\\n  -m '{\"key\":\"value\"}'\n</code></pre> <p>Avro:</p> <pre><code>danube-cli produce \\\n  --schema-subject my-avro-schema \\\n  --schema-type avro \\\n  --file message.avro\n</code></pre> <p>Protobuf:</p> <pre><code>danube-cli produce \\\n  --schema-subject my-proto-schema \\\n  --schema-type protobuf \\\n  --file message.pb\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#partitioned-topics","title":"Partitioned Topics","text":""},{"location":"danube_clis/danube_cli/producer/#create-partitioned-topic","title":"Create Partitioned Topic","text":"<pre><code># Specify number of partitions\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  --partitions 8 \\\n  -m \"Partitioned message\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#high-throughput-example","title":"High Throughput Example","text":"<pre><code># 16 partitions for parallel processing\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/high-volume \\\n  --partitions 16 \\\n  --schema-subject events \\\n  -m '{\"event\":\"data\"}' \\\n  --count 10000 \\\n  --interval 100\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#reliable-delivery","title":"Reliable Delivery","text":""},{"location":"danube_clis/danube_cli/producer/#enable-reliable-delivery","title":"Enable Reliable Delivery","text":"<pre><code># Messages are persisted to disk before acknowledgment\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Critical message\" \\\n  --reliable\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#reliable-partitioned","title":"Reliable + Partitioned","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/transactions \\\n  --partitions 4 \\\n  --reliable \\\n  --schema-subject transactions \\\n  -m '{\"tx_id\":\"tx_789\",\"amount\":1000.00}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#practical-examples","title":"Practical Examples","text":""},{"location":"danube_clis/danube_cli/producer/#e-commerce-orders","title":"E-Commerce Orders","text":"<pre><code># Register order schema\ndanube-cli schema register orders \\\n  --schema-type json_schema \\\n  --file order-schema.json\n\n# Send order\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  --schema-subject orders \\\n  --reliable \\\n  -m '{\n    \"order_id\":\"ord_123\",\n    \"customer_id\":\"cust_456\",\n    \"items\":[{\"sku\":\"ITEM1\",\"qty\":2,\"price\":29.99}],\n    \"total\":59.98\n  }'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#event-streaming","title":"Event Streaming","text":"<pre><code># High-volume event stream\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /analytics/events \\\n  --partitions 16 \\\n  --schema-subject user-events \\\n  -m '{\"event\":\"page_view\",\"user_id\":\"user_789\",\"page\":\"/home\"}' \\\n  --count 100000 \\\n  --interval 100\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#iot-sensor-data","title":"IoT Sensor Data","text":"<pre><code># Send sensor readings\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /iot/sensors \\\n  --attributes \"sensor_id:temp_001,location:warehouse-a\" \\\n  -m '{\"temperature\":22.5,\"humidity\":45,\"timestamp\":\"2024-01-15T10:30:00Z\"}' \\\n  --count 1440 \\\n  --interval 60000  # Every minute\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#log-aggregation","title":"Log Aggregation","text":"<pre><code># Send application logs\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /logs/application \\\n  --attributes \"app:api-server,env:production,level:error\" \\\n  -m '{\"timestamp\":\"2024-01-15T10:30:00Z\",\"message\":\"Database connection failed\",\"stack_trace\":\"...\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#command-reference","title":"Command Reference","text":""},{"location":"danube_clis/danube_cli/producer/#all-producer-flags","title":"All Producer Flags","text":"Flag Short Description Default <code>--service-addr</code> <code>-s</code> Broker URL <code>http://127.0.0.1:6650</code> <code>--topic</code> <code>-t</code> Topic name <code>/default/test_topic</code> <code>--message</code> <code>-m</code> Message content Required* <code>--file</code> <code>-f</code> Binary file path - <code>--producer-name</code> <code>-n</code> Producer name <code>test_producer</code> <code>--schema-subject</code> - Schema subject - <code>--schema-file</code> - Schema file (auto-register) - <code>--schema-type</code> - Schema type - <code>--count</code> <code>-c</code> Number of messages <code>1</code> <code>--interval</code> <code>-i</code> Interval in ms <code>500</code> <code>--partitions</code> <code>-p</code> Number of partitions - <code>--attributes</code> <code>-a</code> Message attributes - <code>--reliable</code> - Reliable delivery <code>false</code> <p>*Required unless <code>--file</code> is provided</p>"},{"location":"danube_clis/danube_cli/schema_registry/","title":"Schema Registry Guide","text":"<p>Master schema management for reliable, validated messaging! \ud83d\udccb</p>"},{"location":"danube_clis/danube_cli/schema_registry/#table-of-contents","title":"Table of Contents","text":"<ul> <li>What is a Schema Registry?</li> <li>Schema Management</li> <li>Schema Types</li> <li>Schema Evolution</li> <li>Compatibility Modes</li> <li>Complete Workflows</li> </ul>"},{"location":"danube_clis/danube_cli/schema_registry/#what-is-a-schema-registry","title":"What is a Schema Registry?","text":"<p>The Schema Registry is a centralized repository that stores and manages schemas for your messages.</p>"},{"location":"danube_clis/danube_cli/schema_registry/#why-use-schemas","title":"Why Use Schemas?","text":"<p>Without Schemas:</p> <pre><code># Producer sends anything\ndanube-cli produce -s http://localhost:6650 -m '{\"user\":123}'  # number\ndanube-cli produce -s http://localhost:6650 -m '{\"user\":\"abc\"}' # string\ndanube-cli produce -s http://localhost:6650 -m '{\"usr\":\"xyz\"}'  # typo!\n\n# Consumer has no idea what to expect! \u274c\n</code></pre> <p>With Schemas:</p> <pre><code># Schema defines the contract\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\", \"email\"]\n}\n\n# Only valid messages are accepted \u2705\n# Consumers know exactly what to expect \u2705\n# Breaking changes are prevented \u2705\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#benefits","title":"Benefits","text":"Benefit Description Type Safety Prevent invalid data at the source Documentation Schema serves as living documentation Evolution Safe schema updates with compatibility checking Validation Automatic validation for producers and consumers Versioning Track schema changes over time"},{"location":"danube_clis/danube_cli/schema_registry/#schema-management","title":"Schema Management","text":""},{"location":"danube_clis/danube_cli/schema_registry/#register-a-schema","title":"Register a Schema","text":"<pre><code>danube-cli schema register &lt;subject&gt; \\\n  --schema-type &lt;schema-type&gt; \\\n  --file &lt;schema-file&gt;\n</code></pre> <p>Example:</p> <pre><code># Create a JSON schema file\ncat &gt; user-schema.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\", \"format\": \"email\"},\n    \"age\": {\"type\": \"integer\", \"minimum\": 0}\n  },\n  \"required\": [\"user_id\", \"email\"]\n}\nEOF\n\n# Register the schema\ndanube-cli schema register user-events \\\n  --schema-type json_schema \\\n  --file user-schema.json\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udce4 Registering schema 'user-events' (type: JsonSchema)...\n\u2705 Schema registered successfully!\n   Subject: user-events\n   Schema ID: 1\n   Version: 1\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#get-schema-details","title":"Get Schema Details","text":"<pre><code>danube-cli schema get &lt;subject&gt;\n</code></pre> <p>Example:</p> <pre><code>danube-cli schema get user-events\n</code></pre> <p>Output:</p> <pre><code>\u2705 Schema Details\n==================================================\nSubject:       user-events\nVersion:       1\nSchema ID:     1\nType:          json_schema\n==================================================\nSchema Definition:\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\", \"format\": \"email\"},\n    \"age\": {\"type\": \"integer\", \"minimum\": 0}\n  },\n  \"required\": [\"user_id\", \"email\"]\n}\n==================================================\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#list-schema-versions","title":"List Schema Versions","text":"<pre><code>danube-cli schema versions &lt;subject&gt;\n</code></pre> <p>Example:</p> <pre><code>danube-cli schema versions user-events\n</code></pre> <p>Output:</p> <pre><code>\u2705 Schema Versions for 'user-events'\n==================================================\nVersion 1 (ID: 1) - Current\nVersion 2 (ID: 2)\nVersion 3 (ID: 3) - Latest\n==================================================\nTotal versions: 3\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#check-compatibility","title":"Check Compatibility","text":"<p>Before registering a new version, check compatibility:</p> <pre><code>danube-cli schema check &lt;subject&gt; \\\n  --schema-type &lt;schema-type&gt; \\\n  --file &lt;new-schema-file&gt;\n</code></pre> <p>Example:</p> <pre><code># Create updated schema (v2)\ncat &gt; user-schema-v2.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\", \"format\": \"email\"},\n    \"age\": {\"type\": \"integer\", \"minimum\": 0},\n    \"name\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\", \"email\"]\n}\nEOF\n\n# Check compatibility\ndanube-cli schema check user-events \\\n  --schema-type json_schema \\\n  --file user-schema-v2.json\n</code></pre> <p>Output (Compatible):</p> <pre><code>\u2705 Schema is compatible!\n   Subject: user-events\n   Compatibility Mode: backward\n\nSchema can be safely registered.\n</code></pre> <p>Output (Incompatible):</p> <pre><code>\u274c Schema is NOT compatible!\n   Subject: user-events\n   Compatibility Mode: backward\n\nCompatibility errors:\n- Required field 'name' added (breaks backward compatibility)\n\nCannot register this schema version.\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#schema-types","title":"Schema Types","text":""},{"location":"danube_clis/danube_cli/schema_registry/#json-schema","title":"JSON Schema","text":"<p>Most common for JSON messages.</p> <p>Create Schema:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"event_type\": {\"type\": \"string\"},\n    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"},\n    \"user_id\": {\"type\": \"string\"}\n  },\n  \"required\": [\"event_type\", \"timestamp\"]\n}\n</code></pre> <p>Register:</p> <pre><code>danube-cli schema register events \\\n  --schema-type json_schema \\\n  --file events-schema.json\n</code></pre> <p>Use:</p> <pre><code># Produce with validation\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  --schema-subject events \\\n  -m '{\"event_type\":\"login\",\"timestamp\":\"2024-01-01T10:00:00Z\",\"user_id\":\"u123\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#avro-schema","title":"Avro Schema","text":"<p>For compact binary serialization.</p> <p>Create Schema:</p> <pre><code>{\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"namespace\": \"com.example\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"string\"},\n    {\"name\": \"email\", \"type\": \"string\"},\n    {\"name\": \"age\", \"type\": [\"null\", \"int\"], \"default\": null}\n  ]\n}\n</code></pre> <p>Register:</p> <pre><code>danube-cli schema register users \\\n  --schema-type avro \\\n  --file user-schema.avsc\n</code></pre> <p>Use:</p> <pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  --schema-subject users \\\n  -m '{\"id\":\"u123\",\"email\":\"user@example.com\",\"age\":25}'\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#protobuf-schema","title":"Protobuf Schema","text":"<p>For Google Protocol Buffers.</p> <p>Create Schema (message.proto):</p> <pre><code>syntax = \"proto3\";\n\nmessage User {\n  string id = 1;\n  string email = 2;\n  int32 age = 3;\n}\n</code></pre> <p>Register:</p> <pre><code>danube-cli schema register users \\\n  --schema-type protobuf \\\n  --file message.proto\n</code></pre> <p>Use:</p> <pre><code># Send compiled protobuf binary\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  --schema-subject users \\\n  --file compiled-message.bin\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#schema-evolution","title":"Schema Evolution","text":""},{"location":"danube_clis/danube_cli/schema_registry/#evolution-scenarios","title":"Evolution Scenarios","text":""},{"location":"danube_clis/danube_cli/schema_registry/#adding-optional-fields-safe","title":"Adding Optional Fields (Safe)","text":"<p>V1:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\"]\n}\n</code></pre> <p>V2 (Add optional field):</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\"]\n}\n</code></pre> <p>\u2705 Backward compatible - Old consumers can read new messages \u2705 Forward compatible - New consumers can read old messages</p>"},{"location":"danube_clis/danube_cli/schema_registry/#removing-optional-fields-safe","title":"Removing Optional Fields (Safe)","text":"<p>V1:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"temp_field\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\"]\n}\n</code></pre> <p>V2 (Remove optional field):</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\"]\n}\n</code></pre> <p>\u2705 Backward compatible - Old consumers still work</p>"},{"location":"danube_clis/danube_cli/schema_registry/#adding-required-fields-unsafe","title":"Adding Required Fields (Unsafe)","text":"<p>V1:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\"]\n}\n</code></pre> <p>V2 (Add required field):</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\", \"email\"]\n}\n</code></pre> <p>\u274c NOT backward compatible - Old producers can't provide required field</p>"},{"location":"danube_clis/danube_cli/schema_registry/#safe-evolution-workflow","title":"Safe Evolution Workflow","text":"<pre><code># Step 1: Check current schema\ndanube-cli schema get orders\n\n# Step 2: Create new schema version\ncat &gt; orders-v2.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"order_id\": {\"type\": \"string\"},\n    \"amount\": {\"type\": \"number\"},\n    \"currency\": {\"type\": \"string\", \"default\": \"USD\"}\n  },\n  \"required\": [\"order_id\", \"amount\"]\n}\nEOF\n\n# Step 3: Check compatibility\ndanube-cli schema check orders \\\n  --schema-type json_schema \\\n  --file orders-v2.json\n\n# Step 4: If compatible, register\ndanube-cli schema register orders \\\n  --schema-type json_schema \\\n  --file orders-v2.json\n\n# Step 5: Verify versions\ndanube-cli schema versions orders\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#compatibility-modes","title":"Compatibility Modes","text":"<p>Compatibility modes control how schemas can evolve.</p>"},{"location":"danube_clis/danube_cli/schema_registry/#backward-default","title":"Backward (Default)","text":"<p>New schema can read data written with old schema.</p> <p>Use when: Consumers are upgraded before producers</p> <pre><code># Check backward compatibility\ndanube-cli schema check orders \\\n  --schema-type json_schema \\\n  --file orders-v2.json\n</code></pre> <p>Allowed changes:</p> <ul> <li>\u2705 Add optional fields</li> <li>\u2705 Remove required fields</li> </ul> <p>Forbidden changes:</p> <ul> <li>\u274c Add required fields</li> <li>\u274c Remove optional fields</li> </ul>"},{"location":"danube_clis/danube_cli/schema_registry/#forward","title":"Forward","text":"<p>Old schema can read data written with new schema.</p> <p>Use when: Producers are upgraded before consumers</p> <p>Allowed changes:</p> <ul> <li>\u2705 Remove optional fields</li> <li>\u2705 Add required fields</li> </ul> <p>Forbidden changes:</p> <ul> <li>\u274c Add optional fields</li> <li>\u274c Remove required fields</li> </ul>"},{"location":"danube_clis/danube_cli/schema_registry/#full","title":"Full","text":"<p>Both backward and forward compatible.</p> <p>Use when: Consumers and producers upgrade independently</p> <p>Allowed changes:</p> <ul> <li>\u2705 Add optional fields with defaults</li> <li>\u2705 Remove optional fields</li> </ul> <p>Forbidden changes:</p> <ul> <li>\u274c Add required fields</li> <li>\u274c Remove required fields</li> <li>\u274c Change field types</li> </ul>"},{"location":"danube_clis/danube_cli/schema_registry/#none","title":"None","text":"<p>No compatibility checking.</p> <p>Use when: Breaking changes are acceptable</p> <p>\u26a0\ufe0f Warning: Can break consumers!</p>"},{"location":"danube_clis/danube_cli/schema_registry/#complete-workflows","title":"Complete Workflows","text":""},{"location":"danube_clis/danube_cli/schema_registry/#workflow-1-new-schema-from-scratch","title":"Workflow 1: New Schema from Scratch","text":"<pre><code># Step 1: Create schema file\ncat &gt; payment-events.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"payment_id\": {\"type\": \"string\"},\n    \"amount\": {\"type\": \"number\", \"minimum\": 0},\n    \"currency\": {\"type\": \"string\"},\n    \"status\": {\"type\": \"string\", \"enum\": [\"pending\", \"completed\", \"failed\"]}\n  },\n  \"required\": [\"payment_id\", \"amount\", \"currency\", \"status\"]\n}\nEOF\n\n# Step 2: Register schema\ndanube-cli schema register payment-events \\\n  --schema-type json_schema \\\n  --file payment-events.json\n\n# Step 3: Verify registration\ndanube-cli schema get payment-events\n\n# Step 4: Start producer with schema\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /production/payments \\\n  --schema-subject payment-events \\\n  -m '{\"payment_id\":\"pay_123\",\"amount\":99.99,\"currency\":\"USD\",\"status\":\"completed\"}'\n\n# Step 5: Start consumer (automatic validation)\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /production/payments \\\n  -m payment-processor\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#workflow-2-schema-evolution","title":"Workflow 2: Schema Evolution","text":"<pre><code># Step 1: Check current schema\ndanube-cli schema get user-events\ndanube-cli schema versions user-events\n\n# Step 2: Create new schema version\ncat &gt; user-events-v2.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"event\": {\"type\": \"string\"},\n    \"timestamp\": {\"type\": \"string\"},\n    \"metadata\": {\"type\": \"object\"}\n  },\n  \"required\": [\"user_id\", \"event\", \"timestamp\"]\n}\nEOF\n\n# Step 3: Check compatibility\ndanube-cli schema check user-events \\\n  --schema-type json_schema \\\n  --file user-events-v2.json\n\n# Step 4: Register if compatible\ndanube-cli schema register user-events \\\n  --schema-type json_schema \\\n  --file user-events-v2.json\n\n# Step 5: Verify new version\ndanube-cli schema versions user-events\n\n# Step 6: Test with new schema\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  --schema-subject user-events \\\n  -m '{\"user_id\":\"u123\",\"event\":\"login\",\"timestamp\":\"2024-01-01T10:00:00Z\",\"metadata\":{\"ip\":\"127.0.0.1\"}}'\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#troubleshooting","title":"Troubleshooting","text":""},{"location":"danube_clis/danube_cli/schema_registry/#schema-not-found","title":"Schema Not Found","text":"<pre><code># Check if schema is registered\ndanube-cli schema get my-subject\n\n# If not found, register it\ndanube-cli schema register my-subject --schema-type json_schema --file schema.json\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#validation-failures","title":"Validation Failures","text":"<pre><code># Get current schema\ndanube-cli schema get my-subject --output json\n\n# Verify your message matches the schema\n# Check required fields, types, formats\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#compatibility-issues","title":"Compatibility Issues","text":"<pre><code># Check what compatibility mode is set\ndanube-cli schema get my-subject\n\n# Check compatibility\ndanube-cli schema check my-subject \\\n  --schema-type json_schema \\\n  --file new-schema.json\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#json-output-for-automation","title":"JSON Output for Automation","text":"<p>All schema commands support JSON output:</p> <pre><code># Get schema as JSON\ndanube-cli schema get user-events --output json | jq .\n\n# List versions as JSON\ndanube-cli schema versions user-events --output json | jq .\n\n# Check compatibility with JSON output\ndanube-cli schema check user-events \\\n  --schema-type json_schema \\\n  --file new-schema.json \\\n  --output json | jq .\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/","title":"Run Danube with Docker Compose","text":"<p>This guide provides instructions on how to run Danube Messaging using Docker and Docker Compose. It sets up ETCD for metadata storage and MinIO for topic persistence storage.</p>"},{"location":"getting_started/Danube_docker_compose/#architecture-overview","title":"Architecture Overview","text":"<p>The setup includes:</p> <ul> <li>2 Danube Brokers: High-availability message brokers with load balancing</li> <li>ETCD: Distributed metadata store for cluster coordination</li> <li>MinIO: S3-compatible object storage for persistent message storage</li> <li>MinIO Client (MC): Automatic bucket creation and configuration</li> </ul> <p></p>"},{"location":"getting_started/Danube_docker_compose/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine 20.10+</li> <li>Docker Compose 2.0+</li> <li>At least 4GB RAM available for containers</li> <li>Ports 2379, 2380, 6650-6651, 9000-9001, 9040-9041, 50051-50052 available</li> </ul>"},{"location":"getting_started/Danube_docker_compose/#quick-start","title":"Quick Start","text":""},{"location":"getting_started/Danube_docker_compose/#step-1-setup-choose-one-option","title":"Step 1: Setup (Choose One Option)","text":"<p>Option 1: Download Docker Compose Files (Recommended for running the broker)</p> <p>Create a directory and download the required files:</p> <pre><code>mkdir danube-docker &amp;&amp; cd danube-docker\n</code></pre> <p>Download the docker-compose file:</p> <pre><code>curl -O https://raw.githubusercontent.com/danube-messaging/danube/main/docker/docker-compose.yml\n</code></pre> <p>Download the broker configuration file:</p> <pre><code>curl -O https://raw.githubusercontent.com/danube-messaging/danube/main/docker/danube_broker.yml\n</code></pre> <p>Option 2: Clone Repository (Recommended for development and building from source)</p> <pre><code>git clone https://github.com/danube-messaging/danube.git\ncd danube/docker\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#step-2-start-the-cluster","title":"Step 2: Start the Cluster","text":"<p>Start the entire cluster:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#step-3-verify-all-services-are-healthy","title":"Step 3: Verify all services are healthy","text":"<p>Verify all services are running:</p> <pre><code>docker-compose ps\n</code></pre> <p>Expected output:</p> <pre><code>\u2717 docker-compose ps\nNAME        IMAGE        COMMAND       SERVICE         CREATED        STATUS       PORTS\n\ndanube-broker1   docker-broker1                             \"/usr/local/bin/danu\u2026\"   broker1      About a minute ago   Up 6 seconds (health: starting)   0.0.0.0:6650-&gt;6650/tcp, [::]:6650-&gt;6650/tcp, 0.0.0.0:9040-&gt;9040/tcp, [::]:9040-&gt;9040/tcp, 0.0.0.0:50051-&gt;50051/tcp, [::]:50051-&gt;50051/tcp\n\ndanube-broker2   docker-broker2                             \"/usr/local/bin/danu\u2026\"   broker2      About a minute ago   Up 6 seconds (health: starting)   0.0.0.0:6651-&gt;6650/tcp, [::]:6651-&gt;6650/tcp, 0.0.0.0:9041-&gt;9040/tcp, [::]:9041-&gt;9040/tcp, 0.0.0.0:50052-&gt;50051/tcp, [::]:50052-&gt;50051/tcp\n\ndanube-cli       docker-danube-cli                          \"sleep infinity\"         danube-cli   About a minute ago   Up 6 seconds                      \n\ndanube-etcd      quay.io/coreos/etcd:v3.5.9                 \"/usr/local/bin/etcd\"    etcd         About a minute ago   Up 12 seconds (healthy)           0.0.0.0:2379-2380-&gt;2379-2380/tcp, [::]:2379-2380-&gt;2379-2380/tcp\n\ndanube-mc        minio/mc:RELEASE.2024-09-16T17-43-14Z      \"/bin/sh -c ' echo '\u2026\"   mc           About a minute ago   Up About a minute                 \n\ndanube-minio     minio/minio:RELEASE.2025-07-23T15-54-02Z   \"/usr/bin/docker-ent\u2026\"   minio        About a minute ago   Up About a minute (healthy)       0.0.0.0:9000-9001-&gt;9000-9001/tcp, [::]:9000-9001-&gt;9000-9001/tcp\n</code></pre> <p>Check logs (optional):</p> <pre><code># View all logs\ndocker-compose logs -f\n\n# View specific service logs\ndocker-compose logs -f broker1\ndocker-compose logs -f broker2\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#service-endpoints","title":"Service Endpoints","text":"Service Endpoint Purpose Danube Broker 1 <code>localhost:6650</code> gRPC messaging Danube Broker 2 <code>localhost:6651</code> gRPC messaging Admin API 1 <code>localhost:50051</code> Broker administration Admin API 2 <code>localhost:50052</code> Broker administration Prometheus 1 <code>localhost:9040</code> Metrics and monitoring Prometheus 2 <code>localhost:9041</code> Metrics and monitoring MinIO API <code>localhost:9000</code> S3-compatible storage MinIO Console <code>localhost:9001</code> Web UI (minioadmin/minioadmin123) ETCD <code>localhost:2379</code> Metadata store"},{"location":"getting_started/Danube_docker_compose/#testing-with-danube-cli","title":"Testing with Danube CLI","text":""},{"location":"getting_started/Danube_docker_compose/#using-the-cli-container","title":"Using the CLI Container","text":"<p>The Docker Compose setup includes a <code>danube-cli</code> container with both <code>danube-cli</code> and <code>danube-admin-cli</code> tools pre-installed. This eliminates the need to build or install Rust locally.</p> <p>No local installation required - use the containerized CLI tools directly.</p>"},{"location":"getting_started/Danube_docker_compose/#reliable-messaging-with-s3-storage","title":"Reliable Messaging with S3 Storage","text":"<p>Test the cloud-ready persistent storage capabilities:</p> <p>Produce with reliable delivery and S3 persistence:</p> <pre><code>docker exec -it danube-cli danube-cli produce \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/persistent-topic\" \\\n  --count 1000 \\\n  --message \"Persistent message\" \\\n  --reliable\n</code></pre> <p>Consume persistent messages:</p> <pre><code>docker exec -it danube-cli danube-cli consume \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/persistent-topic\" \\\n  --subscription \"persistent-sub\" \\\n  --sub-type exclusive\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#non-reliable-message-flow-testing","title":"Non-Reliable Message Flow Testing","text":""},{"location":"getting_started/Danube_docker_compose/#basic-string-messages","title":"Basic string messages","text":"<p>Produce basic string messages:</p> <pre><code>docker exec -it danube-cli danube-cli produce \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/test-topic\" \\\n  --count 100 \\\n  --message \"Hello from Danube Docker!\"\n</code></pre> <p>Consume from shared subscription:</p> <pre><code>docker exec -it danube-cli danube-cli consume \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/test-topic\" \\\n  --subscription \"shared-sub\" \\\n  --consumer \"docker-consumer\"\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#json-schema-messages","title":"JSON schema messages","text":"<p>Produce JSON messages with schema:</p> <pre><code>docker exec -it danube-cli danube-cli produce \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/json-topic\" \\\n  --count 100 \\\n  --schema json \\\n  --json-schema '{\"type\":\"object\",\"properties\":{\"message\":{\"type\":\"string\"},\"timestamp\":{\"type\":\"number\"}}}' \\\n  --message '{\"message\":\"Hello JSON\",\"timestamp\":1640995200}'\n</code></pre> <p>Consume JSON messages:</p> <pre><code>docker exec -it danube-cli danube-cli consume \\\n  --service-addr http://broker2:6650 \\\n  --topic \"/default/json-topic\" \\\n  --subscription \"json-sub\" \\\n  --consumer \"json-consumer\"\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#admin-cli-operations","title":"Admin CLI Operations","text":"<p>Use danube-admin-cli for cluster management:</p> <pre><code># List active brokers\ndocker exec -it danube-cli danube-admin-cli brokers list\n\n# List namespaces in cluster\ndocker exec -it danube-cli danube-admin-cli brokers namespaces\n\n# List topics in a namespace\ndocker exec -it danube-cli danube-admin-cli topics list default\n\n# List subscriptions on a topic\ndocker exec -it danube-cli danube-admin-cli topics subscriptions /default/test-topic\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"getting_started/Danube_docker_compose/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>Access broker metrics for monitoring:</p> <pre><code># Broker 1 metrics\ncurl http://localhost:9040/metrics\n\n# Broker 2 metrics  \ncurl http://localhost:9041/metrics\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#minio-console","title":"MinIO Console","text":"<ol> <li>Open http://localhost:9001 in your browser</li> <li>Login with credentials: <code>minioadmin</code> / <code>minioadmin123</code></li> <li>Navigate to \"Buckets\" to see:</li> <li><code>danube-messages</code>: Persistent message storage</li> <li><code>danube-wal</code>: Write-ahead log storage</li> </ol>"},{"location":"getting_started/Danube_docker_compose/#etcd-inspection","title":"ETCD Inspection","text":"<pre><code># List all keys in ETCD\ndocker exec danube-etcd etcdctl --endpoints=http://127.0.0.1:2379 get --prefix \"\"\n\n# Watch for changes\ndocker exec danube-etcd etcdctl --endpoints=http://127.0.0.1:2379 watch --prefix \"\"\n\n# Check broker registrations\ndocker exec danube-etcd etcdctl --endpoints=http://127.0.0.1:2379 get --prefix \"/cluster/register\"\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#configuration","title":"Configuration","text":""},{"location":"getting_started/Danube_docker_compose/#broker-configuration","title":"Broker Configuration","text":"<p>The <code>danube_broker.yml</code> file is optimized for:</p> <ul> <li>S3 Storage: MinIO integration with automatic credential management</li> <li>High Performance: Optimized WAL rotation and batch sizes</li> <li>Development: Relaxed security and unlimited resource policies</li> <li>Monitoring: Prometheus metrics enabled on all brokers</li> </ul>"},{"location":"getting_started/Danube_docker_compose/#environment-variables","title":"Environment Variables","text":"<p>Key environment variables used:</p> <ul> <li><code>AWS_ACCESS_KEY_ID=minioadmin</code></li> <li><code>AWS_SECRET_ACCESS_KEY=minioadmin123</code></li> <li><code>AWS_REGION=us-east-1</code></li> <li><code>RUST_LOG=danube_broker=info,danube_core=info</code></li> </ul>"},{"location":"getting_started/Danube_docker_compose/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting_started/Danube_docker_compose/#common-issues","title":"Common Issues","text":"<ol> <li>Port conflicts: Ensure all required ports are available</li> <li>Memory issues: Increase Docker memory allocation if containers fail to start</li> <li>Storage issues: Check MinIO bucket creation in logs: <code>docker-compose logs mc</code></li> </ol>"},{"location":"getting_started/Danube_docker_compose/#reset-environment","title":"Reset Environment","text":"<pre><code># Stop and remove all containers, networks, and volumes\ndocker-compose down -v\n\n# Remove all Danube-related Docker resources\ndocker volume prune -f\ndocker network prune -f\n\n# Restart fresh\ndocker-compose up -d\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#production-considerations","title":"Production Considerations","text":"<p>This setup demonstrates Danube's cloud-ready capabilities. For production deployment:</p> <ol> <li>Replace MinIO with AWS S3, Google Cloud Storage, or Azure Blob Storage</li> <li>Enable TLS/SSL authentication in broker configuration</li> <li>Configure resource limits and health checks appropriately</li> <li>Set up monitoring with Prometheus and Grafana</li> <li>Implement backup strategies for ETCD and persistent storage</li> <li>Use container orchestration like Kubernetes for scaling</li> </ol>"},{"location":"getting_started/Danube_docker_compose/#aws-s3-migration","title":"AWS S3 Migration","text":"<p>To migrate from MinIO to AWS S3, update <code>danube_broker.yml</code>:</p> <pre><code>wal_cloud:\n  cloud:\n    backend: \"s3\"\n    root: \"s3://your-production-bucket/danube-cluster\"\n    region: \"us-west-2\"\n    # Remove endpoint for AWS S3\n    # endpoint: \"http://minio:9000\"  \n    # Use IAM roles or environment variables for credentials\n</code></pre> <p>This Docker Compose setup showcases Danube's architecture with cloud-native storage.</p>"},{"location":"getting_started/Danube_kubernetes/","title":"Run Danube messaging on Kubernetes","text":"<p>This documentation covers the instalation of the Danube cluster on the kubernetes. The Helm chart deploys the Danube Cluster with ETCD as metadata storage in the same namespace.</p> <p>The documentation assumes that you have a Kubernetes cluster running and that you have installed the Helm package manager. For local testing you can use kind.</p>"},{"location":"getting_started/Danube_kubernetes/#install-the-ngnix-ingress-controller","title":"Install the Ngnix Ingress controller","text":"<p>Using the Official NGINX Ingress Helm Chart. This is required in order to route traffic to each broker service in the cluster. The Broker configuration is provisioned in the danube_helm, you can tweak the values.yaml per your needs.</p> <p>The Danube messaging has no dependency on ngnix, can work with any ingress controller of your choice.</p> <p>Install the NGINX Ingress Controller using Helm:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#run-the-nginx-ingress-controller-on-your-local-kubernetes-cluster","title":"Run the NGINX Ingress Controller on your local kubernetes cluster","text":"<p>For local testing the NGINX Ingress Controller can be exposed using a NodePort service so that the traffic from the local machine (outside the cluster) can reach the Ingress controller.</p> <pre><code>helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.service.type=NodePort\n</code></pre> <p>You can find out which port is assigned by running</p> <pre><code>kubectl get svc\n\nNAME                                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nkubernetes                                         ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                      4m17s\nnginx-ingress-ingress-nginx-controller             NodePort    10.96.245.118   &lt;none&gt;        80:30115/TCP,443:30294/TCP   2m58s\nnginx-ingress-ingress-nginx-controller-admission   ClusterIP   10.96.169.82    &lt;none&gt;        443/TCP                      2m58s\n</code></pre> <p>If ngnix is running as NodePort (usually for testing), you need local port in this case 30115, in order to provide to danube_helm installation.</p>"},{"location":"getting_started/Danube_kubernetes/#run-the-nginx-ingress-controller-in-a-remote-cluster-cloud","title":"Run the NGINX Ingress Controller in a remote cluster (cloud)","text":"<pre><code>helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.publishService.enabled=true\n</code></pre> <ul> <li>The publishService feature enables the Ingress controller to publish information about itself (such as its external IP or hostname) in a Kubernetes Service resource.</li> <li>This is particularly useful when you are running the Ingress controller in a cloud environment (like AWS, GCP, or Azure) and need it to publish its external IP address to handle incoming traffic</li> </ul>"},{"location":"getting_started/Danube_kubernetes/#install-danube-messaging-brokers","title":"Install Danube Messaging Brokers","text":"<p>First, add the repository to your Helm client:</p> <pre><code>helm repo add danube https://danube-messaging.github.io/danube_helm\nhelm repo update\n</code></pre> <p>You can install the chart with the release name <code>my-danube-cluster</code> using the below command. This will deploy the Danube Broker and an ETCD instance with the default configuration.</p>"},{"location":"getting_started/Danube_kubernetes/#running-on-the-local-kubernetes-cluster","title":"Running on the local kubernetes cluster","text":"<pre><code>helm install my-danube-cluster danube/danube-helm-chart --set broker.service.advertisedPort=30115\n</code></pre> <p>The advertisedPort is used to allow the client to reach the brokers, through the ingress NodePort.</p>"},{"location":"getting_started/Danube_kubernetes/#running-on-the-remote-cluster-cloud","title":"Running on the remote cluster (cloud)","text":"<p>The Danube cluster configuration from the values.yaml file has to be adjusted for your needs.</p> <p>You can override the default values by providing a custom <code>values.yaml</code> file:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart -f custom-values.yaml\n</code></pre> <p>Alternatively, you can specify individual values using the <code>--set</code> flag:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart --set broker.service.type=\"ClusterIP\"\n</code></pre> <p>You can further customize the installation, check the readme file. The default configuration is running 3 Danube Brokers in cluster.</p>"},{"location":"getting_started/Danube_kubernetes/#check-the-install","title":"Check the install","text":"<p>Make sure that the brokers, etcd and the ngnix ingress are running properly in the cluster.</p>"},{"location":"getting_started/Danube_kubernetes/#example-running-on-local-kubernetes-cluster","title":"Example running on local kubernetes cluster","text":"<pre><code>kubectl get all\n\nNAME                                                          READY   STATUS    RESTARTS   AGE\npod/my-danube-cluster-danube-broker1-766665d6f4-qdbf6         1/1     Running   0          12s\npod/my-danube-cluster-danube-broker2-5774ff4dd6-dvx66         1/1     Running   0          12s\npod/my-danube-cluster-danube-broker3-6db6b5fccd-dkr2k         1/1     Running   0          12s\npod/my-danube-cluster-etcd-867f5b85f8-g4m9m                   1/1     Running   0          12s\npod/nginx-ingress-ingress-nginx-controller-7bc7c7776d-wqc5g   1/1     Running   0          47m\n\nNAME                                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                       AGE\nservice/kubernetes                                         ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                       48m\nservice/my-danube-cluster-danube-broker1                   ClusterIP   10.96.40.244    &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-danube-broker2                   ClusterIP   10.96.204.21    &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-danube-broker3                   ClusterIP   10.96.46.5      &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-etcd                             ClusterIP   10.96.232.70    &lt;none&gt;        2379/TCP                      12s\nservice/nginx-ingress-ingress-nginx-controller             NodePort    10.96.245.118   &lt;none&gt;        80:30115/TCP,443:30294/TCP    47m\nservice/nginx-ingress-ingress-nginx-controller-admission   ClusterIP   10.96.169.82    &lt;none&gt;        443/TCP                       47m\n\nNAME                                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/my-danube-cluster-danube-broker1         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-danube-broker2         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-danube-broker3         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-etcd                   1/1     1            1           12s\ndeployment.apps/nginx-ingress-ingress-nginx-controller   1/1     1            1           47m\n\nNAME                                                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/my-danube-cluster-danube-broker1-766665d6f4         1         1         1       12s\nreplicaset.apps/my-danube-cluster-danube-broker2-5774ff4dd6         1         1         1       12s\nreplicaset.apps/my-danube-cluster-danube-broker3-6db6b5fccd         1         1         1       12s\nreplicaset.apps/my-danube-cluster-etcd-867f5b85f8                   1         1         1       12s\nreplicaset.apps/nginx-ingress-ingress-nginx-controller-7bc7c7776d   1         1         1       47m\n</code></pre> <p>Validate that the brokers have started correctly:</p> <pre><code>kubectl logs pod/my-danube-cluster-danube-broker1-766665d6f4-qdbf6\n\ninitializing metrics exporter\n2024-08-28T04:30:22.969462Z  INFO danube_broker: Use ETCD storage as metadata persistent store\n2024-08-28T04:30:22.969598Z  INFO danube_broker: Start the Danube Service\n2024-08-28T04:30:22.969612Z  INFO danube_broker::danube_service: Setting up the cluster MY_CLUSTER\n2024-08-28T04:30:22.971978Z  INFO danube_broker::danube_service::local_cache: Initial cache populated\n2024-08-28T04:30:22.972013Z  INFO danube_broker::danube_service: Started the Local Cache service.\n2024-08-28T04:30:22.990763Z  INFO danube_broker::danube_service::broker_register: Broker 14150019297734190044 registered in the cluster\n2024-08-28T04:30:22.991620Z  INFO danube_broker::danube_service: Namespace default already exists.\n2024-08-28T04:30:22.991926Z  INFO danube_broker::danube_service: Namespace system already exists.\n2024-08-28T04:30:22.992480Z  INFO danube_broker::danube_service: Namespace default already exists.\n2024-08-28T04:30:22.992490Z  INFO danube_broker::danube_service: cluster metadata setup completed\n2024-08-28T04:30:22.992551Z  INFO danube_broker::danube_service:  Started the Broker GRPC server\n2024-08-28T04:30:22.992563Z  INFO danube_broker::broker_server: Server is listening on address: 0.0.0.0:6650\n2024-08-28T04:30:22.992605Z  INFO danube_broker::danube_service: Started the Leader Election service\n2024-08-28T04:30:22.993050Z  INFO danube_broker::danube_service: Started the Load Manager service.\n2024-08-28T04:30:22.993143Z  INFO danube_broker::danube_service:  Started the Danube Admin GRPC server\n2024-08-28T04:30:22.993274Z  INFO danube_broker::admin: Admin is listening on address: 0.0.0.0:50051\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#setup-in-order-to-communicate-with-cluster-danube-brokers","title":"Setup in order to communicate with cluster danube brokers","text":"<p>If you would like to communicate to the messaging sytem by using the danube-cli tool, or your own danube clients running locally, you can do the following:</p> <pre><code>kubectl get nodes -o wide\nNAME                 STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION       CONTAINER-RUNTIME\nkind-control-plane   Ready    control-plane   53m   v1.30.0   172.20.0.2    &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   5.15.0-118-generic   containerd://1.7.15\n</code></pre> <p>Use the INTERNAL-IP to route the traffic to broker hosts. Add the following in the hosts file, but make sure you match the number and the name of the brokers from the helm values.yaml file.</p> <pre><code>cat /etc/hosts\n172.20.0.2 broker1.example.com broker2.example.com broker3.example.com\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#inspect-the-etcd-instance","title":"Inspect the etcd instance","text":"<p>If you want to connect from your local machine, use kubectl port-forward to forward the etcd port to your local machine:</p> <p>Port Forward etcd Service:</p> <pre><code>kubectl port-forward service/my-danube-cluster-etcd 2379:2379\n</code></pre> <p>Once port forwarding is set up, you can run etcdctl commands from your local machine:</p> <pre><code>etcdctl --endpoints=http://localhost:2379 watch --prefix /\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#cleanup","title":"Cleanup","text":"<p>To uninstall the <code>my-danube-cluster</code> release:</p> <pre><code>helm uninstall my-danube-cluster\n</code></pre> <p>This command removes all the Kubernetes components associated with the chart and deletes the release.</p>"},{"location":"getting_started/Danube_local/","title":"Run Danube Broker on your local machine","text":""},{"location":"getting_started/Danube_local/#start-metadata-storage-etcd","title":"Start Metadata Storage (ETCD)","text":"<p>Danube uses ETCD for metadata storage to provide high availability and scalability. Run ETCD using Docker:</p> <pre><code>docker run -d --name etcd-danube -p 2379:2379 quay.io/coreos/etcd:latest etcd --advertise-client-urls http://0.0.0.0:2379 --listen-client-urls http://0.0.0.0:2379\n</code></pre> <p>Verify ETCD is running:</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS          PORTS                                                 NAMES\n27792bce6077   quay.io/coreos/etcd:latest   \"etcd --advertise-cl\u2026\"   35 seconds ago   Up 34 seconds   0.0.0.0:2379-&gt;2379/tcp, :::2379-&gt;2379/tcp, 2380/tcp   etcd-danube\n</code></pre>"},{"location":"getting_started/Danube_local/#configure-and-run-danube-broker","title":"Configure and Run Danube Broker","text":""},{"location":"getting_started/Danube_local/#create-and-configure-broker-config","title":"Create and configure broker config","text":"<p>Create a local config file, use the sample config file as a reference.</p> <pre><code>touch danube_broker.yml\n</code></pre>"},{"location":"getting_started/Danube_local/#download-and-run-the-danube-broker","title":"Download and run the Danube Broker","text":"<p>Download the latest binary from the releases page.</p> <p>If you would like to run Danube Brokers in cluster, you need to upload the binary to each machine and use the same cluster configuration name.</p> <p>Run the Danube Broker:</p> <pre><code>touch broker.log\n</code></pre> <pre><code>RUST_LOG=info ./danube-broker-linux --config-file danube_broker.yml --broker-addr \"0.0.0.0:6650\" --admin-addr \"0.0.0.0:50051\" &gt; broker.log 2&gt;&amp;1 &amp;\n</code></pre> <p>Check the logs:</p> <pre><code>tail -n 100 -f broker.log\n</code></pre> <pre><code>2025-01-12T06:15:53.705416Z  INFO danube_broker: Use ETCD storage as metadata persistent store\n2025-01-12T06:15:53.705665Z  INFO danube_broker: Start the Danube Service\n2025-01-12T06:15:53.705679Z  INFO danube_broker::danube_service: Setting up the cluster MY_CLUSTER\n2025-01-12T06:15:53.707988Z  INFO danube_broker::danube_service::local_cache: Initial cache populated\n2025-01-12T06:15:53.709521Z  INFO danube_broker::danube_service: Started the Local Cache service.\n2025-01-12T06:15:53.713329Z  INFO danube_broker::danube_service::broker_register: Broker 15139934490483381581 registered in the cluster\n2025-01-12T06:15:53.714977Z  INFO danube_broker::danube_service: Namespace default already exists.\n2025-01-12T06:15:53.716405Z  INFO danube_broker::danube_service: Namespace system already exists.\n2025-01-12T06:15:53.717979Z  INFO danube_broker::danube_service: Namespace default already exists.\n2025-01-12T06:15:53.718012Z  INFO danube_broker::danube_service: cluster metadata setup completed\n2025-01-12T06:15:53.718092Z  INFO danube_broker::danube_service:  Started the Broker GRPC server\n2025-01-12T06:15:53.718116Z  INFO danube_broker::broker_server: Server is listening on address: 0.0.0.0:6650\n2025-01-12T06:15:53.718191Z  INFO danube_broker::danube_service: Started the Leader Election service\n2025-01-12T06:15:53.722454Z  INFO danube_broker::danube_service: Started the Load Manager service.\n2025-01-12T06:15:53.724727Z  INFO danube_broker::danube_service:  Started the Danube Admin GRPC server\n2025-01-12T06:15:53.724727Z  INFO danube_broker::admin: Admin is listening on address: 0.0.0.0:50051\n</code></pre>"},{"location":"getting_started/Danube_local/#use-danube-cli-to-publish-and-consume-messages","title":"Use Danube CLI to Publish and Consume Messages","text":"<p>Download the latest Danube CLI binary from the releases page and run it:</p> <pre><code>./danube-cli-linux produce -s http://127.0.0.1:6650 -t /default/demo_topic -c 1000 -m \"Hello, Danube!\"\n</code></pre> <pre><code>Message sent successfully with ID: 9\nMessage sent successfully with ID: 10\nMessage sent successfully with ID: 11\nMessage sent successfully with ID: 12\n</code></pre> <p>Open a new terminal and run the below command to consume the messages:</p> <pre><code>./danube-cli-linux consume -s http://127.0.0.1:6650 -t /default/demo_topic -m my_subscription\n</code></pre> <pre><code>Received bytes message: 9, with payload: Hello, Danube!\nReceived bytes message: 10, with payload: Hello, Danube!\nReceived bytes message: 11, with payload: Hello, Danube!\nReceived bytes message: 12, with payload: Hello, Danube!\n</code></pre>"},{"location":"getting_started/Danube_local/#validate","title":"Validate","text":"<p>Ensure ETCD is running and accessible. You can check its status by accessing <code>http://&lt;ETCD_SERVER_IP&gt;:2379</code> from a browser or using <code>curl</code>:</p> <pre><code>curl http://&lt;ETCD_SERVER_IP&gt;:2379/v3/version\n</code></pre> <p>Ensure each broker instance is running and listening on the specified port. You can check this with <code>netstat</code> or <code>ss</code>:</p> <pre><code>netstat -tuln | grep 6650\n</code></pre> <p>For debugging, check the logs of each Danube broker instance.</p>"},{"location":"getting_started/Danube_local/#cleanup","title":"Cleanup","text":"<p>Stop the Danube Broker:</p> <pre><code>pkill danube-broker\n</code></pre> <p>Stop and remove ETCD container</p> <pre><code>docker stop etcd-danube\n</code></pre> <pre><code>docker rm -f etcd-danube\n</code></pre> <p>Verify cleanup</p> <pre><code>ps aux | grep danube-broker\ndocker ps | grep etcd-danube\n</code></pre>"}]}