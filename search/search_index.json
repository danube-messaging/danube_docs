{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Danube Messaging","text":"<p>Danube is a lightweight, cloud\u2011native messaging platform built in Rust. It delivers sub\u2011second dispatch with cloud economics by combining a Write\u2011Ahead Log (WAL) with object storage, so you get low\u2011latency pub/sub and durable streaming\u2014on one broker.</p> <p>Danube enables one or many producers publish to topics, and multiple consumers receive messages via named subscriptions. Choose Non\u2011Reliable (best\u2011effort pub/sub) or Reliable (at\u2011least\u2011once streaming) per topic to match your workload.</p> <p>For design details, see the Architecture.</p>"},{"location":"#try-danube-in-minutes","title":"Try Danube in minutes","text":"<p>Docker Compose Quickstart: Use the provided Docker Compose setup with MinIO and ETCD.</p>"},{"location":"#danube-capabilities","title":"Danube capabilities","text":""},{"location":"#cluster-broker-characteristics","title":"\ud83c\udfd7\ufe0f Cluster &amp; Broker Characteristics","text":"<ul> <li>Stateless brokers: Metadata in ETCD and data in WAL/Object Storage</li> <li>Horizontal scaling: Add brokers in seconds; partitions rebalance automatically</li> <li>Leader election &amp; HA: Automatic failover and coordination via ETCD</li> <li>Rolling upgrades: Restart or replace brokers with minimal disruption</li> <li>Multi-tenancy: Isolated namespaces with policy controls</li> <li>Security-ready: TLS/mTLS support in Admin and data paths</li> </ul> <p>Cloud-Native by Design - Danube's architecture separates compute from storage, enabling:</p>"},{"location":"#write-ahead-log-cloud-persistence","title":"\ud83c\udf29\ufe0f Write-Ahead Log + Cloud Persistence","text":"<ul> <li>Sub-millisecond producer acknowledgments via local WAL</li> <li>Asynchronous background uploads to S3/GCS/Azure object storage</li> <li>Automatic failover with shared cloud state</li> <li>Infinite retention without local disk constraints</li> </ul>"},{"location":"#performance-scalability","title":"\u26a1 Performance &amp; Scalability","text":"<ul> <li>Hot path optimization: Messages served from in-memory WAL cache</li> <li>Stream per subscription: WAL + cloud storage from selected offset </li> <li>Multi-cloud support: AWS S3, Google Cloud Storage, Azure Blob, MinIO</li> </ul>"},{"location":"#core-features","title":"Core features","text":"<p>Topics</p> <ul> <li>Non\u2011partitioned: served by a single broker.</li> <li>Partitioned: split across brokers for scale and HA.</li> </ul> <p>Dispatch strategies</p> <ul> <li>Non\u2011Reliable: in\u2011memory, best\u2011effort delivery, lowest latency.</li> <li>Reliable: WAL + Cloud persistence with acknowledgments and replay.</li> </ul> <p>Persistence (WAL + Cloud)</p> <ul> <li>Write\u2011Ahead Log on local disk for fast durable writes.</li> <li>Background uploads to object storage for durability and replay at cloud cost.</li> <li>Seamless handoff from historical replay to live tail.</li> </ul> <p>Subscriptions</p> <ul> <li>Exclusive, Shared, Failover patterns for queueing and fan\u2011out.</li> </ul> <p>Message schemas</p> <ul> <li>Bytes, String, Int64, JSON.</li> </ul> <p>Concept guides</p> <ul> <li>Messaging Modes (Pub/Sub vs Streaming)</li> <li>Messaging Patterns (Queuing vs Pub/Sub)</li> </ul>"},{"location":"#crates-in-the-workspace","title":"Crates in the workspace","text":"<p>Repository: https://github.com/danube-messaging/danube</p> <ul> <li>danube-broker \u2013 The broker service (topics, producers, consumers, subscriptions).</li> <li>danube-core \u2013 Core types, protocol, and shared logic.</li> <li>danube-metadata-store \u2013 Metadata storage and cluster coordination.</li> <li>danube-persistent-storage \u2013 WAL and cloud persistence backends.</li> </ul> <p>CLIs and client libraries:</p> <ul> <li>danube-client \u2013 Async Rust client library.</li> <li>danube-cli \u2013 Publish/consume client CLI.</li> <li>danube-admin-cli \u2013 Admin CLI for cluster management.</li> </ul>"},{"location":"#client-libraries","title":"Client libraries","text":"<ul> <li>danube-client (Rust)</li> <li>danube-go (Go)</li> </ul> <p>Contributions for other languages (Python, Java, etc.) are welcome.</p>"},{"location":"architecture/architecture/","title":"Danube Messaging Architecture","text":"<p>The Danube messaging system is a distributed messaging system, based on a publish-subscribe model, aiming to provide high throughput and low latency.</p> <p>The Danube Messaging system's architecture is designed for flexibility and scalability, making it suitable for event-driven and cloud-native applications. Its decoupled and pluggable architecture allows for independent scaling and easy integration of various storage backends. Using the dispatch strategies and the subscription models the system can accommodate different messaging patterns.</p> <p></p>"},{"location":"architecture/architecture/#brokers","title":"Brokers","text":"<p>Brokers are the core of the Danube Messaging system, responsible for routing and distributing messages, managing client connections and subscriptions, and implementing both reliable and non-reliable dispatch strategies. They act as the main entry point for publishers and subscribers, ensuring efficient and effective message flow.</p> <p>The Producers and Consumers connect to the Brokers to publish and consume messages, and use the subscription and dispatch mechanisms to accommodate various messaging patterns and reliability requirements.</p>"},{"location":"architecture/architecture/#metadata-storage","title":"Metadata Storage","text":"<p>The ETCD cluster serves as the metadata storage for the system by maintaining configuration data, topic information, and broker coordination and load-balancing, ensuring the entire system operates with high availability and consistent state management across all nodes.</p>"},{"location":"architecture/architecture/#storage-layer","title":"Storage Layer","text":"<p>The Storage Layer is responsible for message durability and replay. Danube now uses a cloud-native model based on a local Write-Ahead Log (WAL) for the hot path and background persistence to cloud object storage via OpenDAL. This keeps publish/dispatch latency low while enabling durable, elastic storage across providers (S3, GCS, Azure Blob, local FS, memory).</p> <p>Readers use tiered access: if data is within local WAL retention it is served from WAL/cache; otherwise historical data is streamed from cloud objects (using ETCD metadata) and seamlessly handed off to the WAL tail.</p>"},{"location":"architecture/architecture/#non-reliable-dispatch","title":"Non-Reliable Dispatch","text":"<p>Non-Reliable Dispatch operates with zero storage overhead, as messages flow directly from publishers to subscribers without intermediate persistence. This mode delivers maximum performance and lowest latency, making it ideal for scenarios where occasional message loss is acceptable, such as real-time metrics or live streaming data.</p>"},{"location":"architecture/architecture/#reliable-dispatch","title":"Reliable Dispatch","text":"<p>Reliable Dispatch offers guaranteed message delivery backed by the WAL + cloud persistence:</p> <ul> <li>WAL on local disk for low-latency appends and fast replay from in-memory cache and WAL files.</li> <li>Cloud object storage via OpenDAL (S3/GCS/Azure/FS/Memory) for durable, scalable historical data, uploaded asynchronously by a background uploader with resumable checkpoints.</li> <li>ETCD metadata tracks cloud objects and sparse indexes for efficient historical reads.</li> </ul> <p>The ability to choose between these dispatch modes gives users the flexibility to optimize their messaging infrastructure based on their specific requirements for performance, reliability, and resource utilization.</p> <p>For details and provider-specific configuration, see Persistence (WAL + Cloud).</p>"},{"location":"architecture/architecture/#design-considerations","title":"Design Considerations","text":""},{"location":"architecture/architecture/#decoupled-architecture","title":"Decoupled Architecture","text":"<p>The Danube Messaging system features a decoupled architecture where components are loosely coupled, allowing for independent scaling, easy maintenance and upgrades, and failure isolation.</p>"},{"location":"architecture/architecture/#plugin-architecture","title":"Plugin Architecture","text":"<p>With a plugin architecture, the system supports flexible storage backend options, making it easy to extend and customize according to different use cases. This adaptability ensures that the system can meet diverse application requirements and is cloud-native ready.</p>"},{"location":"architecture/architecture/#event-driven-focus","title":"Event-Driven Focus","text":"<p>Optimized for event-driven systems, the Danube Messaging system supports various message delivery patterns and scalable message processing. Its design is well-suited for microservices, providing efficient and scalable handling of event-driven workloads.</p>"},{"location":"architecture/dispatch_strategy/","title":"Dispatch Strategy","text":"<p>The dispatch strategies in Danube represent two distinct approaches to message delivery, each serving different use cases:</p>"},{"location":"architecture/dispatch_strategy/#non-reliable-dispatch-strategy","title":"Non-Reliable Dispatch Strategy","text":"<p>This strategy prioritizes speed and minimal resource usage by delivering messages directly from producers to subscribers without persistence. Messages flow through the broker in a \"fire and forget\" manner, achieving the lowest possible latency. It fits real-time metrics, live telemetry, or any workload where occasional loss is acceptable.</p> <p>Writer path (producer)</p> <ul> <li>The producer sends a message to the broker specifying the topic.</li> <li>The broker validates and routes the message to the topic\u2019s dispatcher.</li> <li>Depending on subscription type (Exclusive/Shared/Failover), the dispatcher selects the target consumer(s).</li> <li>The message is immediately forwarded to consumer channels. There is no on-disk persistence and no acknowledgment gating.</li> </ul> <p>Reader path (consumer)</p> <ul> <li>A consumer subscribes to a topic under an existing subscription (Exclusive/Shared/Failover).</li> <li>The broker registers the consumer and attaches a live message stream to it.</li> <li>The dispatcher pushes incoming messages directly to the consumer stream.</li> <li>Acknowledgments are optional and do not affect delivery; if a consumer disconnects, messages in flight may be lost.</li> </ul>"},{"location":"architecture/dispatch_strategy/#reliable-dispatch-strategy","title":"Reliable Dispatch Strategy","text":"<p>This strategy ensures at-least-once delivery using a WAL + Cloud store-and-forward design. Messages are appended to a local Write-Ahead Log (WAL) and asynchronously uploaded to cloud object storage. Delivery is coordinated by the subscription engine, which tracks progress and acknowledgments per subscription.</p> <p>Writer path (producer)</p> <ul> <li>The producer sends a message to the broker for a reliable topic.</li> <li>The message is appended to the local WAL (durable on disk) and becomes eligible for dispatch.</li> <li>The dispatcher prepares the message for the subscription type (Exclusive/Shared/Failover) while the subscription engine records it as pending.</li> <li>A background uploader asynchronously persists WAL frames to cloud object storage; this does not block producers.</li> </ul> <p>Reader path (consumer)</p> <ul> <li>A consumer subscribes to a reliable topic; the broker attaches a stream and initializes subscription progress.</li> <li>The dispatcher delivers messages according to the subscription type and ordering guarantees.</li> <li>The consumer acknowledges processed messages; the subscription engine advances progress and triggers redelivery if needed.</li> <li>If the consumer is late or reconnects after a gap, historical data is replayed from the WAL or, if needed, from cloud storage, then seamlessly handed off to the live WAL tail.</li> </ul> <p>These strategies embody Danube\u2019s flexibility, letting you choose the right balance between performance and reliability per topic. You can run non-reliable and reliable topics side by side in the same cluster.</p>"},{"location":"architecture/internal_danube_services/","title":"Danube Cluster Services Role","text":"<p>This document enumerates the principal internal components of the Danube Broker.</p>"},{"location":"architecture/internal_danube_services/#danube-service-components","title":"Danube Service Components","text":"<p>The Broker owns the topics and manages their lifecycle. It also facilitates the creation of producers, subscriptions, and consumers, ensuring that producers can publish messages to topics and consumers can consume messages from topics.</p>"},{"location":"architecture/internal_danube_services/#leader-election-service","title":"Leader Election Service","text":"<p>The Leader Election Service selects one broker from the cluster to act as the Leader. The Broker Leader is responsible for making decisions. This service is used by the Load Manager, ensuring only one broker in the cluster posts the cluster aggregated Load Report.</p> <p>Leader Election Flow:</p> <ul> <li>The first broker registered in the cluster becomes the Leader by registering itself on \"/cluster/leader\".</li> <li>The field is registered with a lease, so the leader broker must periodically renew its lease to maintain leadership.</li> <li>Subsequent brokers attempt to become leaders but become Followers if the path is already in use.</li> <li>All brokers periodically check the leader path. If there is no change, the state is maintained; otherwise, brokers attempt to become the leader.</li> </ul>"},{"location":"architecture/internal_danube_services/#load-manager-service","title":"Load Manager Service","text":"<p>The Load Manager monitors and distributes load across brokers by managing topic and partition assignments. It implements rebalancing logic to redistribute topics/partitions when brokers join or leave the cluster and is responsible for failover mechanisms to handle broker failures.</p> <p>Load Manager Flow:</p> <ul> <li>All brokers periodically post their Load Reports on the path \"/cluster/brokers/load/{broker-id}\".</li> <li>The leader broker watches for load reports from all brokers in the cluster.</li> <li>It calculates rankings using the selected Load Balance algorithm.</li> <li>It posts its calculations for the cluster on the \"/cluster/load_balance\" path.</li> </ul> <p>Creation of a New Topic:</p> <ul> <li>A broker registers the Topic on the \"/cluster/unassigned\" path.</li> <li>The Load Manager of the leader Broker watches this path and assigns the broker with the least load to host the new topic by posting the topic to the \"/cluster/brokers/{broker-id}/{topic_name}\" path.</li> <li>Each broker watches its own path: \"/cluster/brokers/{broker-id}\". For any event on that path, such as the addition or deletion of topics, it acts accordingly by creating a new topic locally or deleting the topic it owned and all related resources.</li> <li>On topic creation, the broker checks if the topic already exists locally. If not, it retrieves all data about the topic, including subscriptions and producers, from the Local Metadata Cache.</li> <li>On topic removal, the broker handles the disconnections of producers and consumers and removes the locally allocated resources.</li> </ul> <p>For further consideration: We may want the broker to ask the Load Manager to get the next broker and initiate topic creation. Either it just posts the topic on the \"/cluster/unassigned\" path, or if it is the selected broker, it also creates the topic locally.</p>"},{"location":"architecture/internal_danube_services/#local-metadata-cache","title":"Local Metadata Cache","text":"<p>This cache stores various types of metadata required by Danube brokers, such as topic and namespace data, which are frequently accessed during message production and consumption. This reduces the need for frequent queries to the central metadata store, ETCD.</p> <p>The docs/internal_resources.md document describes how the resources are organized in the Metadata Store.</p> <p>Updates/events are received via ETCD Watch events and/or the metadata event synchronizer.</p>"},{"location":"architecture/internal_danube_services/#syncronizer","title":"Syncronizer","text":"<p>Not yet implemented.</p> <p>The synchronizer ensures that metadata and configuration settings across different brokers remain consistent. It propagates changes to metadata and configuration settings using client Producers and Consumers.</p> <p>This is in addition to Metadata Storage watch events, allowing brokers to process metadata updates even if there was a communication glitch or the broker was unavailable for a short period, potentially missing the Store Watch events. The synchronizer allows for dynamic updates to configuration settings without requiring a broker service restart.</p>"},{"location":"architecture/messages/","title":"Message Structure in Danube","text":"<p>A message in Danube represents the fundamental unit of data transmission between producers and consumers. Each message contains both the payload and associated metadata for proper routing and processing.</p>"},{"location":"architecture/messages/#streammessage-structure","title":"StreamMessage Structure","text":""},{"location":"architecture/messages/#core-fields","title":"Core Fields","text":"<ul> <li>request_id (u64): Unique identifier for tracking the message request</li> <li>msg_id (MessageID): Identifier containing routing and location information</li> <li>payload (Vec): The binary message content <li>publish_time (u64): Timestamp when the message was published</li> <li>producer_name (String): Name of the producer that sent the message</li> <li>subscription_name (Option): Name of the subscription for consumer acknowledgment routing, when applicable <li>attributes (HashMap): User-defined key-value pairs for custom metadata"},{"location":"architecture/messages/#messageid-fields","title":"MessageID Fields","text":"<ul> <li>producer_id (u64): Unique identifier for the producer within a topic</li> <li>topic_name (String): Name of the topic the message belongs to</li> <li>broker_addr (String): Address of the broker that delivered the message to the consumer</li> <li>topic_offset (u64): Monotonic position of the message within the topic</li> </ul>"},{"location":"architecture/messages/#usage","title":"Usage","text":""},{"location":"architecture/messages/#producer-perspective","title":"Producer Perspective","text":"<p>Producers create messages by setting the payload and optional attributes. The system automatically generates and manages other fields like request_id, msg_id, and publish_time.</p>"},{"location":"architecture/messages/#consumer-perspective","title":"Consumer Perspective","text":"<p>Consumers receive the complete StreamMessage structure, providing access to both the message payload and all associated metadata for processing and acknowledgment handling.</p>"},{"location":"architecture/messages/#message-routing","title":"Message Routing","text":"<p>The MessageID structure enables efficient message routing and acknowledgment handling across the Danube messaging system, ensuring messages reach their intended destinations and can be properly tracked.</p>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/","title":"Pub/Sub vs Streaming: Concepts and Danube Support","text":"<p>This page compares Pub/Sub messaging and Streaming architectures, highlights key differences and use cases, and indicates what Danube supports. Use it as a conceptual guide aligned with Danube\u2019s Reliable and Non\u2011Reliable dispatch modes.</p>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#pubsub-messaging","title":"Pub/Sub messaging","text":""},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#purpose-and-use-cases","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system.</li> <li>Use Cases: Event-driven architectures, real-time notifications, decoupled microservices, and distributed systems. Suitable when low latency is critical and some message loss is acceptable (e.g., monitoring, telemetry, ephemeral chat).</li> </ul> <p>Danube support: Yes, via Non\u2011Reliable dispatch.</p> <ul> <li>Delivery: best\u2011effort (at\u2011most\u2011once).</li> <li>Persistence: none; messages are not stored.</li> <li>Ordering: per topic/partition within a dispatcher.</li> </ul>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#architecture-and-design","title":"Architecture and Design","text":"<ul> <li>Components: Producers, Subscriptions (consumer groups), and the broker.</li> <li>Message Flow: Producers send messages to a broker, which distributes them to active subscribers according to subscription semantics.</li> <li>Scaling: Scales by adding more brokers or distributing load (topics / partitions) across multiple brokers.</li> </ul>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#data-handling-and-processing-models","title":"Data Handling and Processing Models","text":""},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#pubsub-messaging-producers","title":"Pub/Sub messaging Producers","text":"<ul> <li>Low Latency: No disk persistence on the hot path; minimal overhead.</li> <li>Ordering: Preserved per topic/partition through the dispatcher.</li> <li>Delivery: Best\u2011effort; messages can be lost on failure.</li> <li>Acks: Immediate, based on in\u2011memory handling.</li> <li>No Active Consumers: Publishing is allowed; messages are dropped if no consumers are attached.</li> </ul> <p>Danube: Matches Non\u2011Reliable dispatch semantics.</p>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#pubsub-messaging-consumers","title":"Pub/Sub messaging Consumers","text":"<ul> <li>Real-time: Messages are delivered only to connected consumers.</li> <li>No Replay: No historical fetch; process as they arrive.</li> <li>Throughput: High, with low overhead.</li> <li>Delivery: Only to active subscribers; otherwise dropped.</li> </ul> <p>Danube: Supported via Exclusive, Shared, Failover subscriptions in Non\u2011Reliable mode.</p>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#order-of-operations-of-pubsub-messaging","title":"Order of Operations of Pub/Sub messaging","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Broker Receives Message: The broker processes the message.</li> <li>Consumer Availability Check: If consumers are available, the message is delivered to them in real-time.</li> <li>No Consumers: If no consumers are connected, the message is discarded.</li> </ul>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#streaming-design-considerations","title":"Streaming design considerations","text":""},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#purpose-and-use-cases_1","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for processing and analyzing large volumes of data in real-time as it is generated.</li> <li>Use Cases: Real-time analytics, data pipelines, event sourcing, continuous data processing, and stream processing applications. Ideal when high reliability and durability are required (e.g., financial transactions, orders, audit logs).</li> </ul> <p>Danube support: Yes, via Reliable dispatch (WAL + Cloud persistence).</p> <ul> <li>Delivery: at\u2011least\u2011once with ack\u2011gating.</li> <li>Persistence: local WAL for hot path, background uploads to object storage for durability and replay.</li> <li>Replay: historical fetch from WAL or Cloud with seamless handoff to live tail.</li> </ul>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#architecture-and-design_1","title":"Architecture and Design","text":"<ul> <li>Components: Producers, consumers, stream processors, and a durable log.</li> <li>Data Flow: Producers append to a log; consumers/processors read continuously with tracked progress.</li> <li>Scaling: Designed to handle high throughput and scale horizontally by partitioning data across multiple nodes in a cluster.</li> </ul>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#data-handling-and-processing-models_1","title":"Data Handling and Processing Models","text":""},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#streaming-producers","title":"Streaming Producers","text":"<ul> <li>Durability: Messages are appended to a persistent log. In Danube, the hot path is a local WAL with asynchronous uploads to cloud object storage, enabling replay and recovery.</li> <li>Acknowledgements: Acks are returned once the message is durably appended to the WAL (replication depends on deployment/storage backend). Latency is higher than non\u2011reliable but optimized for sub\u2011second dispatch.</li> <li>Ordering: Preserved per topic/partition.</li> <li>Delivery Guarantees: At\u2011least\u2011once (with redelivery on failure).</li> <li>Publishing Without Consumers: Allowed; messages are stored and remain available for later consumption within retention.</li> <li>Processing: Compatible with stream processing patterns (e.g., windowing, aggregations) layered above the durable log.</li> </ul>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#streaming-consumers","title":"Streaming Consumers","text":"<ul> <li>Replay &amp; Retention: Consumers can fetch historical data within retention. In Danube, reads come from WAL if available, otherwise from Cloud with seamless handoff to the live tail.</li> <li>Acknowledgements: Consumers ack processed messages; the broker tracks subscription progress and manages redelivery.</li> <li>Fault Tolerance: On crash/reconnect, consumers resume from the last acknowledged position.</li> <li>Availability: Messages remain available even with no active consumers, subject to retention policies.</li> </ul>"},{"location":"architecture/messaging_modes_pubsub_vs_streaming/#order-of-operations-of-streaming","title":"Order of Operations of Streaming","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Durable Append: The broker appends the message to the WAL (durable on disk); background tasks upload frames to cloud storage.</li> <li>Producer Acknowledgment: The broker acknowledges once the append is durable.</li> <li>Delivery to Consumers: Messages are dispatched to consumers, gated by acknowledgments for reliable delivery.</li> <li>No Consumers: Messages remain stored and available for later consumption within retention.</li> </ul>"},{"location":"architecture/messaging_patterns_queuing_vs_pubsub/","title":"Messaging Patterns: Queuing vs Pub/Sub (Fan\u2011out)","text":"<p>This page explains the difference between queuing (point\u2011to\u2011point) and pub/sub (fan\u2011out) and shows how to enable each pattern in Danube using subscriptions and dispatch modes.</p> <ul> <li>For subscription details, see <code>architecture/subscriptions.md</code>.</li> <li>For delivery semantics, see <code>architecture/dispatch_strategy.md</code>.</li> </ul>"},{"location":"architecture/messaging_patterns_queuing_vs_pubsub/#concepts-at-a-glance","title":"Concepts at a Glance","text":"<p>Queuing (Point\u2011to\u2011Point)</p> <ul> <li>One message is delivered to exactly one consumer in a group.</li> <li>Suited for work distribution (orders, tasks, jobs).</li> <li>Ordering is per consumer, and per partition when topics are partitioned.</li> </ul> <p>Pub/Sub (Fan\u2011out)</p> <ul> <li>Every subscription receives every message.</li> <li>Suited for broadcast (notifications, analytics, multiple services reacting to events).</li> <li>Each subscription processes the full stream independently.</li> </ul>"},{"location":"architecture/messaging_patterns_queuing_vs_pubsub/#how-to-enable-in-danube","title":"How to enable in Danube","text":"<p>Both patterns are built with Danube subscriptions on a topic.</p> <p>Queuing (Point\u2011to\u2011Point)</p> <ul> <li>Set subscription type to <code>Shared</code>.</li> <li>Use the same subscription name across all workers (e.g., <code>orders-workers</code>).</li> <li>Result: Each message is delivered to one consumer in the group (round\u2011robin, per partition).</li> </ul> <p>Pub/Sub (Fan\u2011out)</p> <ul> <li>Create distinct subscriptions per downstream consumer/team (unique subscription names), typically using <code>Exclusive</code> (or <code>Failover</code> for HA).</li> <li>Result: Each subscription receives every message; consumers do not contend with other subscriptions.</li> </ul>"},{"location":"architecture/messaging_patterns_queuing_vs_pubsub/#dispatch-modes-and-guarantees","title":"Dispatch modes and guarantees","text":"<p>Non\u2011Reliable (Pub/Sub\u2011style, best\u2011effort)</p> <ul> <li>Lowest latency; no persistence or replay.</li> <li>Messages are delivered only to active consumers. Disconnections may cause loss.</li> </ul> <p>Reliable (Streaming\u2011style, at\u2011least\u2011once)</p> <ul> <li>Messages are appended to the local WAL and asynchronously uploaded to cloud storage.</li> <li>Consumers can replay historical data; acknowledgments drive redelivery.</li> </ul> <p>See <code>architecture/persistence.md</code> for WAL + Cloud details.</p>"},{"location":"architecture/messaging_patterns_queuing_vs_pubsub/#partitioning-behavior","title":"Partitioning behavior","text":"<p>With partitioned topics, both patterns apply per partition.</p> <ul> <li>Queuing: messages are distributed round\u2011robin within each partition to the consumers in that subscription.</li> <li>Pub/Sub: each subscription receives each partition\u2019s messages independently.</li> </ul>"},{"location":"architecture/messaging_patterns_queuing_vs_pubsub/#quick-examples","title":"Quick examples","text":"<p>Build a worker queue</p> <ul> <li>Topic: <code>/default/orders</code></li> <li>Subscription: <code>orders-workers</code> (type <code>Shared</code>)</li> <li>Run N consumers with the same subscription name. Messages are load\u2011balanced.</li> </ul> <p>Broadcast to multiple services</p> <ul> <li>Topic: <code>/default/events</code></li> <li>Subscriptions: <code>billing</code> (Exclusive), <code>analytics</code> (Exclusive), <code>monitoring</code> (Failover)</li> <li>Each subscription receives the full event stream independently.</li> </ul>"},{"location":"architecture/messaging_patterns_queuing_vs_pubsub/#notes-best-practices","title":"Notes &amp; best practices","text":"<ul> <li>Use Reliable mode for durability and replay; Non\u2011Reliable for minimal latency when loss is acceptable.</li> <li>Prefer <code>Failover</code> over <code>Exclusive</code> when you need quick takeover on consumer failure.</li> <li>Size partitions to match consumer parallelism for optimal throughput.</li> </ul>"},{"location":"architecture/persistence/","title":"Danube Persistence Architecture (WAL + Cloud)","text":"<p>Danube's persistence layer has been recently revamped to make the platform cloud\u2011native, aiming for sub\u2011second dispatch with cloud economics. </p> <p>Danube uses a cloud\u2011native persistence architecture that combines a local Write\u2011Ahead Log (WAL) for the hot path with background uploads to cloud object storage. This keeps publish/dispatch latency low while providing durable, elastic, and cost\u2011effective storage in the cloud.</p> <p>This page explains the main concepts, how the system works, why it scales in cloud environments, and how to configure it for major providers (S3, GCS, Azure). Danube relies on OpenDAL as a storage abstraction, so it can be plugged into any storage backend.</p>"},{"location":"architecture/persistence/#key-benefits","title":"Key Benefits","text":"<ul> <li>Fast hot path: Producers append to a local WAL and consumers read from memory/WAL; no remote writes on the critical path.</li> <li>Cloud-native durability: A background uploader persists WAL frames to cloud object storage.</li> <li>Seamless reader experience: Readers transparently stream historical data from cloud, then live data from WAL.</li> <li>Cost-effective and elastic: Object storage provides massive scale, lifecycle policies, and low TCO.</li> <li>Portable by design: Built on OpenDAL, enabling S3, GCS, Azure Blob, local FS, memory, and more.</li> </ul>"},{"location":"architecture/persistence/#high-level-architecture","title":"High-level Architecture","text":"<ul> <li>Local WAL: Append-only log with an in-memory cache for ultra-fast reads. Files periodically fsync and rotate.</li> <li>Cloud Uploader: Periodically streams complete WAL frames to cloud objects. Writes object descriptors and sparse indexes to ETCD.</li> <li>Cloud Reader: Reads historical messages from cloud using ETCD metadata, then hands off to local WAL for live data.</li> <li>ETCD: Tracks object descriptors, offsets, and indexes for efficient range reads.</li> </ul>"},{"location":"architecture/persistence/#core-concepts-and-components","title":"Core Concepts and Components","text":"<p>WalStorageFactory creates per-topic <code>WalStorage</code>, starts one uploader and one deleter (retention) task per topic, and normalizes each topic to its own WAL directory under the configured root.</p> <p>Wal is an append-only log with an in-memory ordered cache; records are framed as <code>[u64 offset][u32 len][u32 crc][bytes]</code> with CRC32 validation, and a background writer batches and fsyncs data, supporting rotation by size and/or time.</p> <p>WalStorage implements the <code>PersistentStorage</code> trait; when creating readers it applies tiered logic to serve from WAL if possible, otherwise transparently chains a Cloud\u2192WAL stream to cover historical then live data.</p> <p>Uploader streams safe frame prefixes from WAL files to cloud, finalizes objects atomically, builds sparse offset indexes for efficient seeks, and operates one object per cycle with resumable checkpoints.</p> <p>CloudReader reads objects referenced in ETCD metadata, uses sparse indexes to seek efficiently to the requested range, and validates each frame by CRC during decoding.</p> <p>Retention/Deleter enforces time/size retention on WAL files after they are safely uploaded to cloud, advancing the WAL <code>start_offset</code> accordingly.</p> <p>For persistent storage implementation details, check the source code.</p>"},{"location":"architecture/persistence/#how-reads-work-cloudwal-handoff","title":"How Reads Work (Cloud\u2192WAL Handoff)","text":"<ol> <li>A subscription requests a reader at a <code>StartPosition</code> (latest or from a specific offset).</li> <li><code>WalStorage</code> checks the WAL\u2019s local <code>start_offset</code>.</li> <li>If <code>start_offset</code> is older than requested, it:    Streams historical range from cloud objects via <code>CloudReader</code>.    Seamlessly chains into WAL tail for fresh/live data.</li> <li>Consumers see a single ordered stream with no gaps or duplicates.</li> </ol>"},{"location":"architecture/persistence/#configuration-overview","title":"Configuration Overview","text":"<p>Broker configuration is under the <code>wal_cloud</code> section of <code>config/danube_broker.yml</code>:</p> <pre><code>wal_cloud:\n  wal:\n    dir: \"./danube-data/wal\"\n    file_name: \"wal.log\"\n    cache_capacity: 1024\n    file_sync:\n      interval_ms: 5000         # fsync cadence (checkpoint frequency)\n      max_batch_bytes: 10485760 # 10 MiB write batch\n    rotation:\n      max_bytes: 536870912      # 512 MiB WAL file size\n      # max_hours: 24           # optional time-based rotation\n    retention:\n      time_minutes: 2880        # prune locally after 48h\n      size_mb: 20480            # or when size exceeds 20 GiB\n      check_interval_minutes: 5 # deleter tick interval\n\n  uploader:\n    enabled: true\n    interval_seconds: 300       # one upload cycle every 5 minutes\n    root_prefix: \"/danube-data\"\n    max_object_mb: 1024         # optional cap per object\n\n  # Cloud storage backend - MinIO S3 configuration\n  cloud:\n    backend: \"s3\"\n    root: \"s3://danube-messages/cluster-data\"  # S3 bucket and prefix\n    region: \"us-east-1\"\n    endpoint: \"http://minio:9000\"     # MinIO endpoint\n    access_key: \"minioadmin\"         # From environment variable\n    secret_key: \"minioadmin123\"     # From environment variable\n    anonymous: false\n    virtual_host_style: false\n\n  metadata:\n    etcd_endpoint: \"127.0.0.1:2379\"\n    in_memory: false\n</code></pre> <ul> <li>WAL: local durability + replay cache for hot reads.</li> <li>Uploader: cadence and object sizing knobs.</li> <li>Cloud: provider/backend specific settings (see below).</li> <li>Metadata: ETCD endpoint used to store object descriptors and indexes.</li> </ul>"},{"location":"architecture/persistence/#provider-examples-opendal-powered","title":"Provider Examples (OpenDAL-powered)","text":"<p>Danube uses OpenDAL under the hood. Switch providers by changing <code>wal_cloud.cloud</code>.</p> <ul> <li>Amazon S3</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"s3\"\n    root: \"s3://my-bucket/prefix\"\n    region: \"us-east-1\"\n    endpoint: \"https://s3.us-east-1.amazonaws.com\"   # optional\n    access_key: \"${AWS_ACCESS_KEY_ID}\"               # or depend on env/IMDS\n    secret_key: \"${AWS_SECRET_ACCESS_KEY}\"\n    profile: null\n    role_arn: null\n    session_token: null\n    anonymous: false\n</code></pre> <ul> <li>Google Cloud Storage (GCS)</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"gcs\"\n    root: \"gcs://my-bucket/prefix\"\n    project: \"my-gcp-project\"\n    credentials_json: null                 # inline JSON string\n    credentials_path: \"/path/to/creds.json\" # or path to file\n</code></pre> <ul> <li>Azure Blob Storage</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"azblob\"\n    root: \"my-container/prefix\"                       # container[/prefix]\n    endpoint: \"https://&lt;account&gt;.blob.core.windows.net\" # or Azurite endpoint\n    account_name: \"&lt;account_name&gt;\"\n    account_key: \"&lt;account_key&gt;\"\n</code></pre> <ul> <li>In-memory (development)</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"memory\"\n    root: \"mem-prefix\" # namespace-only; persisted in memory\n</code></pre> <ul> <li>Local filesystem</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"fs\"\n    root: \"./object_root\"   # local directory for persisted objects\n</code></pre>"},{"location":"architecture/persistence/#tuning-and-best-practices","title":"Tuning and Best Practices","text":"<ul> <li>WAL cache size (<code>wal.cache_capacity</code>): increase for higher consumer hit rates; memory-bound.</li> <li>Flush cadence (<code>wal.file_sync.interval_ms</code>): smaller values improve checkpoint freshness but increase fsync pressure.</li> <li>Rotation thresholds (<code>wal.rotation.*</code>): tune file sizes for smoother uploads and operational hygiene.</li> <li>Uploader interval (<code>uploader.interval_seconds</code>): shorter intervals reduce RPO and speed up historical availability in cloud.</li> <li>Object size (<code>uploader.max_object_mb</code>): bigger objects reduce listing overhead; </li> <li>Retention: ensure local retention is safely larger than upload interval so deleter never prunes data not yet uploaded.</li> <li>Credentials: prefer environment-based credentials for cloud providers</li> </ul>"},{"location":"architecture/persistence/#operational-notes","title":"Operational Notes","text":"<ul> <li>At-least-once delivery: With reliable topics, dispatch uses the WAL to guarantee at-least-once delivery; cloud persistence is asynchronous and does not block producers/consumers.</li> <li>Resilience: Uploader uses precise checkpoints <code>(file_seq, byte_pos)</code> and never re-uploads confirmed bytes; CloudReader validates CRCs.</li> <li>Observability: Checkpoints and rotation metadata are stored under the per-topic WAL directory; ETCD keeps object descriptors.</li> <li>Extensibility: Because Danube uses OpenDAL, adding a new backend typically means adding backend-specific options in config; no broker code changes needed.</li> </ul>"},{"location":"architecture/subscriptions/","title":"Subscription","text":"<p>A Danube subscription is a named configuration rule that determines how messages are delivered to consumers. It is a lease on a topic established by a group of consumers.</p> <p>Danube permits multiple producers and subscribers to the same topic. The Subscription Types can be combined to obtain message queueing or fan-out pub-sub messaging patterns.</p> <p></p>"},{"location":"architecture/subscriptions/#exclusive","title":"Exclusive","text":"<p>The Exclusive type is a subscription type that only allows a single consumer to attach to the subscription. If multiple consumers subscribe to a topic using the same subscription, an error occurs. This consumer has exclusive access to all messages published to the topic or partition.</p>"},{"location":"architecture/subscriptions/#exclusive-subscription-on-non-partitioned-topic","title":"Exclusive subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumer</code>: Only one consumer can be attached to the topic with an Exclusive subscription.</li> <li><code>Message Handling</code>: The single consumer handles all messages from the topic, receiving every message published to that topic.</li> </ul>"},{"location":"architecture/subscriptions/#exclusive-subscription-on-partitioned-topic-multiple-partitions","title":"Exclusive subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumer</code>: One consumer is allowed to connect to the subscription across all partitions of the partitioned topic.</li> <li><code>Message Handling</code> : This single consumer processes messages from all partitions of the partitioned topic. If a topic is partitioned into multiple partitions, the exclusive consumer handles messages from every partition.</li> </ul>"},{"location":"architecture/subscriptions/#shared","title":"Shared","text":"<p>In Danube, the Shared subscription type allows multiple consumers to attach to the same subscription. Messages are delivered in a round-robin distribution across consumers, and any given message is delivered to only one consumer.</p>"},{"location":"architecture/subscriptions/#shared-subscription-on-non-partitioned-topic","title":"Shared subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumers</code>: Multiple consumers can subscribe to the same topic.</li> <li><code>Message Handling</code>: Messages are distributed among all consumers in a round-robin fashion.</li> </ul>"},{"location":"architecture/subscriptions/#shared-subscription-on-partitioned-topic-multiple-partitions","title":"Shared subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumers</code>: Multiple consumers can subscribe to the partitioned topic.</li> <li><code>Message Handling</code>: Messages are distributed across all partitions, and then among consumers in a round-robin fashion. Each message from any partition is delivered to only one consumer.</li> </ul>"},{"location":"architecture/subscriptions/#failover","title":"Failover","text":"<p>The Failover subscription type allows multiple consumers to attach to the same subscription, with one active consumer at a time. If the active consumer disconnects or becomes unhealthy, another consumer automatically takes over. This preserves ordering and minimizes downtime.</p>"},{"location":"architecture/subscriptions/#failover-subscription-on-non-partitioned-topic","title":"Failover subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumers</code>: One active consumer processes all messages; additional consumers are in standby.</li> <li><code>Message Handling</code>: If the active consumer fails, a standby consumer takes over and continues from the last acknowledged position.</li> </ul>"},{"location":"architecture/subscriptions/#failover-subscription-on-partitioned-topic-multiple-partitions","title":"Failover subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumers</code>: One active consumer per partition; other consumers remain on standby for each partition.</li> <li><code>Message Handling</code>: Failover occurs independently per partition, ensuring continuity and ordering within each partition.</li> </ul>"},{"location":"architecture/topics/","title":"Topic","text":"<p>A topic is a unit of storage that organizes messages into a stream. As in other messaging systems, topics are named channels for transmitting messages from producers to consumers. Topic names are URLs that have a well-defined structure:</p>"},{"location":"architecture/topics/#namespacetopic_name","title":"/{namespace}/{topic_name}","text":"<p>Example: /default/markets (where default is the namespace and markets the topic)</p>"},{"location":"architecture/topics/#partitioned-topics","title":"Partitioned Topics","text":"<p>Danube support both partitioned and non-partitioned topics. The non-partitioned topics are served by a single broker, while the partitioned topic has partitiones that are served by multiple brokers within the cluster, thus allowing for higher throughput.</p> <p>A partitioned topic is implemented as N internal topics, where N is the number of partitions. When publishing messages to a partitioned topic, each message is routed to one of several brokers. The distribution of partitions across brokers is handled automatically.</p> <p></p> <p>Messages for the topic are broadcast to two consumers. The routing mode determines each message should be published to which partition, while the subscription type determines which messages go to which consumers.</p>"},{"location":"architecture/topics/#benefits-of-the-partitioned-topics","title":"Benefits of the Partitioned topics","text":"<ul> <li><code>Scalability</code>: Partitioned topics enable horizontal scaling by distributing the load across multiple partitions. This is essential for high-throughput systems that need to handle large volumes of data efficiently.</li> <li><code>Parallel Processing</code>: It allows multiple consumers to process different partitions of the same topic concurrently, improving throughput and processing efficiency.</li> <li><code>Data Locality</code>: Partitioning can help in maintaining data locality and reducing processing latency, as consumers handle a specific subset of the data (key-shared distribution not yet supported).</li> </ul>"},{"location":"architecture/topics/#creation-of-partitioned-topics","title":"Creation of Partitioned Topics","text":"<p>Partitioned topics are created with a predefined number of partitions. When you create a partitioned topic, you specify the number of partitions it should have. This number remains fixed for the lifetime of the topic, although you can configure this number at topic creation time.</p>"},{"location":"architecture/topics/#producers","title":"Producers","text":"<p>The producers routing mechanism determine which messages go to which partition.</p>"},{"location":"architecture/topics/#routing-modes","title":"Routing modes","text":"<p>When publishing to partitioned topics, you must specify a routing mode. The routing mode determines each message should be published to which partition or which internal topic.</p> <ul> <li>RoundRobinPartition - The producer will publish messages across all partitions in round-robin fashion to achieve maximum throughput. If a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> <li>SinglePartition - If no key is provided, the producer will randomly pick one single partition and publish all the messages into that partition. While if a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> </ul>"},{"location":"architecture/topics/#consumers-subscriptions","title":"Consumers (subscriptions)","text":"<p>The subscription type determines which messages go to which consumers.</p> <p>Check the Subscription documentation for details on how messages are distributed to consumers based on the subscription type.</p>"},{"location":"client_libraries/clients/","title":"Danube client library","text":"<p>Currently, the supported clients are the Rust Client and Go Client clients. However, the community is encouraged to contribute by developing clients in other programming languages.</p>"},{"location":"client_libraries/clients/#rust-client","title":"Rust client","text":"<p>The Rust danube-client is an asynchronous Rust client library. To start using the <code>danube-client</code> library in your Rust project, you need to add it as a dependency. You can do this by running the following command:</p> <pre><code>cargo add danube-client\n</code></pre> <p>This command will add danube-client to your <code>Cargo.toml</code> file. Once added, you can import and use the library in your Rust code to interact with the Danube Pub/Sub messaging platform.</p>"},{"location":"client_libraries/clients/#go-client","title":"Go client","text":"<p>To start using the danube-go library in your Go project, you need to add it as a dependency. You can do this by running the following command:</p> <pre><code>go get github.com/danube-messaging/danube-go\n</code></pre> <p>This command will fetch the <code>danube-go</code> library and add it to your <code>go.mod</code> file. Once added, you can import and use the library in your Go code to interact with the Danube Pub/Sub messaging platform.</p>"},{"location":"client_libraries/clients/#community-danube-clients","title":"Community Danube clients","text":"<p>TBD</p>"},{"location":"client_libraries/consumer/","title":"Consumer","text":"<p>A consumer is a process that attaches to a topic via a subscription and then receives messages.</p> <p>Subscription Types - describe the way the consumers receive the messages from topics</p> <ul> <li>Exclusive -  Only one consumer can subscribe, guaranteeing message order.</li> <li>Shared -  Multiple consumers can subscribe, messages are delivered round-robin, offering good scalability but no order guarantee.</li> <li>Failover - Similar to shared subscriptions, but multiple consumers can subscribe, and one actively receives messages.</li> </ul>"},{"location":"client_libraries/consumer/#example","title":"Example","text":"RustGo <pre><code>let topic = \"/default/test_topic\";\n\n  // Create the Exclusive consumer\nlet mut consumer = danube_client\n    .new_consumer()\n    .with_topic(topic.to_string())\n    .with_consumer_name(consumer_name.to_string())\n    .with_subscription(format!(\"test_subscription_{}\", consumer_name))\n    .with_subscription_type(SubType::Exclusive)\n    .build();\n\n// Subscribe to the topic\nconsumer.subscribe().await?;\n\n// Start receiving messages\nlet mut message_stream = consumer.receive().await?;\n\n if let Some(stream_message) = message_stream.recv().await {\n\n    //process the message and ack for receive\n    consumer.ack(&amp;stream_message).await?\n\n }\n</code></pre> <pre><code>ctx := context.Background()\ntopic := \"/default/topic_test\"\nconsumerName := \"consumer_test\"\nsubscriptionName := \"subscription_test\"\nsubType := danube.Exclusive\n\nconsumer, err := client.NewConsumer(ctx).\n    WithConsumerName(consumerName).\n    WithTopic(topic).\n    WithSubscription(subscriptionName).\n    WithSubscriptionType(subType).\n    Build()\nif err != nil {\n    log.Fatalf(\"Failed to initialize the consumer: %v\", err)\n}\n\nif err := consumer.Subscribe(ctx); err != nil {\n    log.Fatalf(\"Failed to subscribe: %v\", err)\n}\nlog.Printf(\"The Consumer %s was created\", consumerName)\n\n\n\n// Receiving messages\nstream, err := consumer.Receive(ctx)\nif err != nil {\n    log.Fatalf(\"Failed to receive messages: %v\", err)\n}\n\nfor msg := range stream {\n    fmt.Printf(\"Received message: %+v\\n\", string(msg.GetPayload()))\n\n    if _, err := consumer.Ack(ctx, msg); err != nil {\n        log.Fatalf(\"Failed to acknowledge message: %v\", err)\n    }\n}\n</code></pre>"},{"location":"client_libraries/consumer/#complete-example","title":"Complete example","text":"<p>For complete code examples of using producers and consumers, check the links:</p> <ul> <li>Rust Examples</li> <li>Go Examples</li> </ul>"},{"location":"client_libraries/producer/","title":"Producer","text":"<p>A producer is a process that attaches to a topic and publishes messages to a Danube broker. The Danube broker processes the messages.</p> <p>Access Mode is a mechanism to determin the permissions of producers on topics.</p> <ul> <li>Shared - Multiple producers can publish on a topic.</li> <li>Exclusive - If there is already a producer connected, other producers trying to publish on this topic get errors immediately.</li> </ul> <p>Before an application creates a producer/consumer, the  client library needs to initiate a setup phase including two steps:</p> <ul> <li>The client attempts to determine the owner of the topic by sending a Lookup request to Broker.  </li> <li>Once the client library has the broker address, it creates a RPC connection (or reuses an existing connection from the pool) and (in later stage authenticates it ).</li> <li>Within this connection, the clients (producer, consumer) and brokers exchange RPC commands. At this point, the client sends a command to create producer/consumer to the broker, which will comply after doing some validation checks.</li> </ul>"},{"location":"client_libraries/producer/#create-producer","title":"Create Producer","text":"RustGo <pre><code>let topic = \"/default/topic_test\";\nlet producer_name = \"producer_test\";\n\n// Create the producer\nlet mut producer = danube_client\n    .new_producer()\n    .with_topic(topic)\n    .with_name(producer_name)\n    .with_schema(\"my_schema\".into(), SchemaType::String)\n    .build();\n\nproducer.create().await?;\n\nproducer\n    .send(\"Hello Danube\".as_bytes().into(), None)\n    .await?;\n</code></pre> <pre><code>topic := \"/default/topic_test\"\nproducerName := \"producer_test\"\n\nproducer, err := client.NewProducer(ctx).\n    WithName(producerName).\n    WithTopic(topic).\n    WithSchema(\"test_schema\", danube.SchemaType_STRING).\n    Build()\nif err != nil {\n    log.Fatalf(\"unable to initialize the producer: %v\", err)\n}\n\nif err := producer.Create(ctx); err != nil {\n    log.Fatalf(\"Failed to create producer: %v\", err)\n}\n\npayload := fmt.Sprintf(\"Hello Danube %d\", i)\n// Convert string to bytes\nbytes_payload := []byte(payload)\n\nmessageID, err := producer.Send(ctx, bytes_payload , nil)\nif err != nil {\n    log.Fatalf(\"Failed to send message: %v\", err)\n}\n</code></pre>"},{"location":"client_libraries/producer/#create-producer-with-partitioned-topic","title":"Create Producer with partitioned topic","text":"<p>Here we create a producer with a partitioned topic. A partitioned topic is implemented as N internal topics, where N is the number of partitions. When publishing messages to a partitioned topic, each message is routed to one of several brokers. The distribution of partitions across brokers is handled automatically.</p> RustGo <pre><code>let topic = \"/default/topic_test\";\nlet producer_name = \"producer_test\";\nlet partitions = 3\n\n// Create the producer\nlet mut producer = danube_client\n    .new_producer()\n    .with_topic(topic)\n    .with_name(producer_name)\n    .with_schema(\"my_schema\".into(), SchemaType::String)\n    .with_partitions(partitions)\n    .build();\n\nproducer.create().await?;\n\nproducer\n    .send(\"Hello Danube\".as_bytes().into(), None)\n    .await?;\n</code></pre> <pre><code>topic := \"/default/topic_test\"\nproducerName := \"producer_test\"\n\nproducer, err := client.NewProducer(ctx).\n    WithName(producerName).\n    WithTopic(topic).\n    WithSchema(\"test_schema\", danube.SchemaType_STRING).\n    WithPartitions(3).\n    Build()\nif err != nil {\n    log.Fatalf(\"unable to initialize the producer: %v\", err)\n}\n\nif err := producer.Create(ctx); err != nil {\n    log.Fatalf(\"Failed to create producer: %v\", err)\n}\n\npayload := fmt.Sprintf(\"Hello Danube %d\", i)\n// Convert string to bytes\nbytes_payload := []byte(payload)\n\nmessageID, err := producer.Send(ctx, bytes_payload , nil)\nif err != nil {\n    log.Fatalf(\"Failed to send message: %v\", err)\n}\n</code></pre>"},{"location":"client_libraries/producer/#create-producer-with-reliable-dispatch-topic","title":"Create Producer with reliable dispatch topic","text":"<p>Here we create Producer with reliable dispatch topic. This strategy ensures guaranteed message delivery by implementing a store-and-forward mechanism. When a message arrives, it's first stored in the chosen storage backend before being dispatched to subscribers.</p> RustGo <pre><code>use danube_client::{DanubeClient, SchemaType };\n\nlet topic = \"/default/topic_test\";\nlet producer_name = \"producer_test\";\n\n\n// Create the producer\nlet mut producer = danube_client\n    .new_producer()\n    .with_topic(topic)\n    .with_name(producer_name)\n    .with_schema(\"my_schema\".into(), SchemaType::String)\n    .with_reliable_dispatch()\n    .build();\n\nproducer.create().await?;\n\nproducer\n    .send(\"Hello Danube\".as_bytes().into(), None)\n    .await?;\n</code></pre> <pre><code>topic := \"/default/topic_test\"\nproducerName := \"producer_test\"\n\n// For reliable strategy\nreliableOpts := danube.NewReliableOptions(\n    10, // 10MB segment size\n    danube.RetainUntilExpire,\n    3600, // retention period in seconds\n    )\nreliableStrategy := danube.NewReliableDispatchStrategy(reliableOpts)\n\nproducer, err := client.NewProducer(ctx).\n    WithName(producerName).\n    WithTopic(topic).\n    WithSchema(\"test_schema\", danube.SchemaType_STRING).\n    WithDispatchStrategy(reliableStrategy).\n    Build()\nif err != nil {\n    log.Fatalf(\"unable to initialize the producer: %v\", err)\n}\n\nif err := producer.Create(ctx); err != nil {\n    log.Fatalf(\"Failed to create producer: %v\", err)\n}\n\npayload := fmt.Sprintf(\"Hello Danube %d\", i)\n// Convert string to bytes\nbytes_payload := []byte(payload)\n\nmessageID, err := producer.Send(ctx, bytes_payload , nil)\nif err != nil {\n    log.Fatalf(\"Failed to send message: %v\", err)\n}\n</code></pre>"},{"location":"client_libraries/producer/#complete-examples","title":"Complete examples","text":"<p>For complete code examples of using producers and consumers, check the links:</p> <ul> <li>Rust Examples</li> <li>Go Examples</li> </ul>"},{"location":"client_libraries/setup/","title":"Configure Danube Client","text":"<p>First you need to create the <code>DanubeClient</code>. The method <code>service_url</code> configures the base URI, that is used for connecting to the Danube Messaging System. The URI should include the protocol and address of the Danube service.</p> RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    // Setup tracing\n    tracing_subscriber::fmt::init();\n\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n}\n</code></pre> <pre><code>import \"github.com/danube-messaging/danube-go\"\n\nfunc main() {\n\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n}\n</code></pre>"},{"location":"client_libraries/setup/#configure-danube-client-with-tls","title":"Configure Danube client with TLS","text":"<p>To enable TLS for secure communication between the client and the Danube broker, you need to configure the client with the appropriate certificate.</p> RustGo <pre><code>use danube_client::DanubeClient;\nuse rustls::crypto;\nuse tokio::sync::OnceCell;\n\nstatic CRYPTO_PROVIDER: OnceCell&lt;()&gt; = OnceCell::const_new();\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    CRYPTO_PROVIDER.get_or_init(|| async {\n        let crypto_provider = crypto::ring::default_provider();\n        crypto_provider\n            .install_default()\n            .expect(\"Failed to install default CryptoProvider\");\n    })\n    .await;\n\nlet client = DanubeClient::builder()\n    .service_url(\"https://127.0.0.1:6650\")\n    .with_tls(\"../cert/ca-cert.pem\")?\n    .build()\n    .await?;\n}\n</code></pre> <pre><code>import \"github.com/danube-messaging/danube-go\"\n\nfunc main() {\n\n   //  TLS support not yet implemented\n\n}\n</code></pre>"},{"location":"client_libraries/setup/#configure-danube-client-with-tls-and-jwt-token-for-authentication","title":"Configure Danube client with TLS and JWT token for authentication","text":"<p>In addition to TLS connectivity in the below example we are using the JWT token to autheticate the requests. This token is usually obtained by logging into an application service and generating an API key, or provided by the admin of the service.</p> <p>The API key is used to request a JWT token from the authentication service. The JWT token includes claims that identify and authorize the client. Once a JWT token is obtained, the Danube client will include it in the <code>Authorization</code> header of all the next requests.</p> RustGo <pre><code>use danube_client::DanubeClient;\nuse rustls::crypto;\nuse tokio::sync::OnceCell;\n\n\nstatic CRYPTO_PROVIDER: OnceCell&lt;()&gt; = OnceCell::const_new();\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    CRYPTO_PROVIDER.get_or_init(|| async {\n        let crypto_provider = crypto::ring::default_provider();\n        crypto_provider\n            .install_default()\n            .expect(\"Failed to install default CryptoProvider\");\n    })\n    .await;\n\nlet client = DanubeClient::builder()\n    .service_url(\"https://127.0.0.1:6650\")\n    .with_tls(\"../cert/ca-cert.pem\")?\n    .with_api_key(\"provided_api_key\".to_string())\n    .build()\n    .await?;\n}\n</code></pre> <pre><code>import \"github.com/danube-messaging/danube-go\"\n\nfunc main() {\n\n   //  TLS support not yet implemented\n\n}\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/","title":"danube-admin: Brokers Commands","text":"<p>The <code>danube-admin-cli</code> tool provides commands to manage and view information about brokers in your Danube cluster. Below is the documentation for the commands related to brokers.</p>"},{"location":"danube_clis/danube_admin/brokers/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/brokers/#danube-admin-cli-brokers-list","title":"<code>danube-admin-cli brokers list</code>","text":"<p>List all active brokers in the cluster.</p> <p>Usage:</p> <pre><code>danube-admin-cli brokers list [--output json]\n</code></pre> <p>Description:</p> <p>This command retrieves and displays a list of all active brokers in the cluster. The output is formatted into a table with the following columns:</p> <ul> <li>BROKER ID: The unique identifier for the broker.</li> <li>BROKER ADDRESS: The network address of the broker.</li> <li>BROKER ROLE: The role assigned to the broker (e.g., \"leader\", \"follower\").</li> </ul> <p>Use <code>--output json</code> to print JSON instead of a table.</p> <p>Example Output:</p> <pre><code>+------------+---------------------+-------------+\n| BROKER ID  | BROKER ADDRESS      | BROKER ROLE |\n+------------+---------------------+-------------+\n| 1          | 192.168.1.1:6650    | leader      |\n| 2          | 192.168.1.2:6650    | follower    |\n+------------+---------------------+-------------+\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#danube-admin-cli-brokers-leader-broker","title":"<code>danube-admin-cli brokers leader-broker</code>","text":"<p>Get information about the leader broker in the cluster.</p> <p>Usage:</p> <pre><code>danube-admin-cli brokers leader-broker\n</code></pre> <p>Description:</p> <p>This command fetches and displays the details of the current leader broker in the cluster. The information includes the broker ID, address, and role of the leader.</p> <p>Example Output:</p> <pre><code>Leader Broker: BrokerId: 1, Address: 192.168.1.1:6650, Role: leader\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#danube-admin-cli-brokers-namespaces","title":"<code>danube-admin-cli brokers namespaces</code>","text":"<p>List all namespaces in the cluster.</p> <p>Usage:</p> <pre><code>danube-admin-cli brokers namespaces [--output json]\n</code></pre> <p>Description:</p> <p>This command retrieves and lists all namespaces associated with the cluster. Each namespace is printed on a new line.</p> <p>Use <code>--output json</code> to print JSON instead of plain text.</p> <p>Example Output:</p> <pre><code>Namespace: default\nNamespace: public\nNamespace: my-namespace\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Make sure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"danube_clis/danube_admin/brokers/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List all brokers:</li> </ul> <pre><code>danube-admin-cli brokers list\n</code></pre> <ul> <li>Get the leader broker:</li> </ul> <pre><code>danube-admin-cli brokers leader-broker\n</code></pre> <ul> <li>List all namespaces:</li> </ul> <pre><code>danube-admin-cli brokers namespaces\n</code></pre> <p>For more detailed information or help with the <code>danube-admin-cli</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-admin-cli brokers --help\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/","title":"danube-admin: Namespaces Commands","text":"<p>The <code>danube-admin-cli</code> tool provides commands to manage and view information about namespaces in your Danube cluster. Below is the documentation for the commands related to namespaces.</p>"},{"location":"danube_clis/danube_admin/namespaces/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-cli-namespaces-topics-namespace","title":"<code>danube-admin-cli namespaces topics NAMESPACE</code>","text":"<p>Get the list of topics for a specified namespace.</p> <p>Usage:</p> <pre><code>danube-admin-cli namespaces topics NAMESPACE [--output json]\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all topics associated with a specific namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Use <code>--output json</code> to print JSON instead of plain text.</p> <p>Example Output:</p> <pre><code>Topic: topic1\nTopic: topic2\nTopic: topic3\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-cli-namespaces-policies-namespace","title":"<code>danube-admin-cli namespaces policies NAMESPACE</code>","text":"<p>Get the configuration policies for a specified namespace.</p> <p>Usage:</p> <pre><code>danube-admin-cli namespaces policies NAMESPACE [--output json]\n</code></pre> <p>Description:</p> <p>This command fetches and displays the configuration policies for a specific namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Use <code>--output json</code> to pretty-print JSON when available.</p> <p>Example Output:</p> <pre><code>Policy Name: policy1\nPolicy Description: Description of policy1\nPolicy Name: policy2\nPolicy Description: Description of policy2\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-cli-namespaces-create-namespace","title":"<code>danube-admin-cli namespaces create NAMESPACE</code>","text":"<p>Create a new namespace.</p> <p>Usage:</p> <pre><code>danube-admin-cli namespaces create NAMESPACE\n</code></pre> <p>Description:</p> <p>This command creates a new namespace with the specified name. Replace <code>NAMESPACE</code> with the desired name for the new namespace.</p> <p>Example Output:</p> <pre><code>Namespace Created: true\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-cli-namespaces-delete-namespace","title":"<code>danube-admin-cli namespaces delete NAMESPACE</code>","text":"<p>Delete a specified namespace. The namespace must be empty.</p> <p>Usage:</p> <pre><code>danube-admin-cli namespaces delete NAMESPACE\n</code></pre> <p>Description:</p> <p>This command deletes a namespace. The specified namespace must be empty before it can be deleted. Replace <code>NAMESPACE</code> with the name of the namespace you wish to delete.</p> <p>Example Output:</p> <pre><code>Namespace Deleted: true\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Make sure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"danube_clis/danube_admin/namespaces/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List all topics in a namespace:</li> </ul> <pre><code>danube-admin-cli namespaces topics my-namespace\n</code></pre> <ul> <li>Get the policies for a namespace:</li> </ul> <pre><code>danube-admin-cli namespaces policies my-namespace\n</code></pre> <ul> <li>Create a new namespace:</li> </ul> <pre><code>danube-admin-cli namespaces create my-new-namespace\n</code></pre> <ul> <li>Delete a namespace:</li> </ul> <pre><code>danube-admin-cli namespaces delete my-old-namespace\n</code></pre> <p>For more detailed information or help with the <code>danube-admin-cli</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-admin-cli namespaces --help\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/","title":"danube-admin: Topics Commands","text":"<p>The <code>danube-admin-cli</code> tool provides commands to manage and view information about topics in your Danube cluster. Below is the documentation for the commands related to topics.</p>"},{"location":"danube_clis/danube_admin/topics/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-list-namespace","title":"<code>danube-admin-cli topics list NAMESPACE</code>","text":"<p>Get the list of topics in a specified namespace.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics list NAMESPACE [--output json]\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all topics within a specified namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Use <code>--output json</code> to print JSON instead of plain text.</p> <p>Example Output:</p> <pre><code>Topic: topic1\nTopic: topic2\nTopic: topic3\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-create-topic","title":"<code>danube-admin-cli topics create TOPIC</code>","text":"<p>Create a topic (non\u2011partitioned or partitioned). You can also set schema and dispatch strategy.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics create TOPIC [--namespace NS] [--partitions N] \\\n  [-s, --schema String|Bytes|Int64|Json] [--schema-file PATH | --schema-data JSON] \\\n  [--dispatch-strategy non_reliable|reliable]\n</code></pre> <p>Description:</p> <p>This command creates a new topic. <code>TOPIC</code> accepts either <code>/namespace/topic</code> or <code>topic</code> (when <code>--namespace</code> is provided). Use <code>--partitions</code> to create a partitioned topic. For <code>Json</code> schema, provide the schema via <code>--schema-file</code> or <code>--schema-data</code>.</p> <p>Example Output:</p> <pre><code>Topic Created: true\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-delete-topic","title":"<code>danube-admin-cli topics delete TOPIC</code>","text":"<p>Delete a specified topic.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics delete TOPIC [--namespace NS]\n</code></pre> <p>Description:</p> <p>This command deletes the specified topic. <code>TOPIC</code> accepts <code>/namespace/topic</code> or <code>topic</code> with <code>--namespace</code>.</p> <p>Example Output:</p> <pre><code>Topic Deleted: true\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-subscriptions-topic","title":"<code>danube-admin-cli topics subscriptions TOPIC</code>","text":"<p>Get the list of subscriptions on a specified topic.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics subscriptions TOPIC [--namespace NS] [--output json]\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all subscriptions associated with a specified topic. <code>TOPIC</code> accepts <code>/namespace/topic</code> or <code>topic</code> with <code>--namespace</code>. Use <code>--output json</code> for JSON output.</p> <p>Example Output:</p> <pre><code>Subscriptions: [subscription1, subscription2]\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-describe-topic","title":"<code>danube-admin-cli topics describe TOPIC</code>","text":"<p>Describe a topic: schema and subscriptions.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics describe TOPIC [--namespace NS] [--output json]\n</code></pre> <p>Description:</p> <p>Shows topic name, schema (pretty-printed when JSON), and subscriptions. <code>TOPIC</code> accepts <code>/namespace/topic</code> or <code>topic</code> with <code>--namespace</code>.</p>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-unsubscribe-subscription-subscription-topic","title":"<code>danube-admin-cli topics unsubscribe --subscription SUBSCRIPTION TOPIC</code>","text":"<p>Delete a subscription from a topic.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics unsubscribe --subscription SUBSCRIPTION TOPIC [--namespace NS]\n</code></pre> <p>Description:</p> <p>This command deletes a subscription from a specified topic. <code>TOPIC</code> accepts <code>/namespace/topic</code> or <code>topic</code> with <code>--namespace</code>.</p> <p>Example Output:</p> <pre><code>Unsubscribed: true\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Ensure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"danube_clis/danube_admin/topics/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List topics in a namespace:</li> </ul> <pre><code>danube-admin-cli topics list my-namespace --output json\n</code></pre> <ul> <li>Create a topic:</li> </ul> <pre><code>danube-admin-cli topics create /default/my-topic --dispatch-strategy reliable\n</code></pre> <ul> <li>Delete a topic:</li> </ul> <pre><code>danube-admin-cli topics delete my-topic --namespace default\n</code></pre> <ul> <li>Unsubscribe from a topic:</li> </ul> <pre><code>danube-admin-cli topics unsubscribe --subscription my-subscription my-topic --namespace default\n</code></pre> <ul> <li>List subscriptions for a topic:</li> </ul> <pre><code>danube-admin-cli topics subscriptions my-topic --namespace default --output json\n</code></pre> <p>For more detailed information or help with the <code>danube-admin-cli</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-admin-cli topics --help\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/","title":"Danube-Pubsub CLI - Consume messages","text":"<p>The <code>consume</code> command subscribes to a topic and receives messages with support for different subscription types, schema validation, and message attributes tracking.</p>"},{"location":"danube_clis/danube_cli/consumer/#basic-usage","title":"Basic Usage","text":"<pre><code>danube-cli consume [OPTIONS] --service-addr &lt;SERVICE_ADDR&gt; --subscription &lt;SUBSCRIPTION&gt;\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#required-arguments","title":"Required Arguments","text":"<ul> <li> <p><code>-s, --service-addr &lt;SERVICE_ADDR&gt;</code>   The service URL for the Danube broker (e.g., <code>http://127.0.0.1:6650</code>)</p> </li> <li> <p><code>-m, --subscription &lt;SUBSCRIPTION&gt;</code>   The subscription name to use for consuming messages</p> </li> </ul>"},{"location":"danube_clis/danube_cli/consumer/#optional-arguments","title":"Optional Arguments","text":"<ul> <li> <p><code>-t, --topic &lt;TOPIC&gt;</code>   Topic to consume messages from (default: /default/test_topic)</p> </li> <li> <p><code>-n, --consumer &lt;CONSUMER&gt;</code>   Consumer identifier (default: <code>consumer_pubsub</code>)</p> </li> <li> <p><code>--sub-type &lt;TYPE&gt;</code>   Subscription type: <code>exclusive</code>, <code>shared</code>, <code>fail-over</code> (default: <code>shared</code>)</p> </li> <li> <p><code>-h, --help</code>   Print help information.</p> </li> </ul>"},{"location":"danube_clis/danube_cli/consumer/#message-output-format","title":"Message Output Format","text":""},{"location":"danube_clis/danube_cli/consumer/#standard-messages","title":"Standard Messages","text":"<pre><code>Received message: \"message content\"\nSize: &lt;size&gt; bytes, Total received: &lt;total&gt; bytes\nAttributes: key1=value1, key2=value2\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#reliable-messages","title":"Reliable messages","text":"<pre><code>Received reliable message: \"message content\"\nTopic offset: &lt;offset&gt;, Size: &lt;size&gt; bytes, Total received: &lt;total&gt; bytes\nProducer: &lt;producer_id&gt;, Topic: &lt;topic_name&gt;\nAttributes: key1=value1, key2=value2\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#features","title":"Features","text":"<ul> <li>Schema Validation: Automatically validates messages against topic schema</li> <li>Large Message Handling: Messages over 1KB are displayed as [binary data]</li> <li>Message Tracking: Tracks total bytes received and message segments</li> <li>Attribute Display: Shows message attributes if present</li> <li>Multiple Schema Types: Supports <code>bytes</code>, <code>string</code>, <code>int64</code>, and <code>json</code> schemas</li> </ul>"},{"location":"danube_clis/danube_cli/consumer/#examples","title":"Examples","text":""},{"location":"danube_clis/danube_cli/consumer/#shared-subscription-default","title":"Shared Subscription (Default)","text":"<pre><code>danube-cli consume --service-addr http://localhost:6650 --subscription my_shared_subscription\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#exclusive-subscription","title":"Exclusive Subscription","text":"<pre><code>danube-cli consume -s http://localhost:6650 -m my_exclusive --sub-type exclusive\n</code></pre> <p>To create a new exclusive subscription on the same topic:</p> <pre><code>danube-cli consume -s http://127.0.0.1:6650 -m my_exclusive2 --sub-type exclusive\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#custom-consumer-name","title":"Custom Consumer Name","text":"<pre><code>danube-cli consume -s http://localhost:6650 -n my_consumer -m my_subscription\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#specific-topic","title":"Specific Topic","text":"<pre><code>danube-cli consume -s http://localhost:6650 -t my_topic -m my_subscription\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#notes","title":"Notes","text":"<ul> <li>Messages are automatically acknowledged after processing</li> <li>JSON messages are pretty-printed and validated against schema if available</li> <li>The consumer maintains message ordering for reliable delivery</li> <li>Connection errors and message processing failures are reported to stderr</li> </ul>"},{"location":"danube_clis/danube_cli/producer/","title":"Danube-Pubsub CLI - Produce messages","text":"<p>The <code>produce</code> command sends messages to a specified topic with support for reliable delivery, custom schemas, and message attributes.</p>"},{"location":"danube_clis/danube_cli/producer/#basic-usage","title":"Basic Usage","text":"<pre><code>danube-cli produce [OPTIONS] --service-addr &lt;SERVICE_ADDR&gt; --message &lt;MESSAGE&gt;\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#required-arguments","title":"Required Arguments","text":"<ul> <li> <p><code>-s, --service-addr &lt;SERVICE_ADDR&gt;</code>   The service URL for the Danube broker (e.g., <code>http://127.0.0.1:6650</code>)</p> </li> <li> <p><code>-m, --message &lt;MESSAGE&gt;</code>   The message content to send</p> </li> <li> <p><code>-f, --file &lt;FILE_PATH&gt;</code>   Binary file path to send (takes precedence over --message when specified)</p> </li> </ul>"},{"location":"danube_clis/danube_cli/producer/#basic-options","title":"Basic Options","text":"<ul> <li> <p><code>-n, --producer-name &lt;PRODUCER_NAME&gt;</code>   Producer identifier (default: <code>test_producer</code>)</p> </li> <li> <p><code>-t, --topic &lt;TOPIC&gt;</code>   Destination topic (default: <code>/default/test_topic</code>)</p> </li> <li> <p><code>-p, --partitions &lt;NUMBER&gt;</code>   Number of topic partitions</p> </li> </ul>"},{"location":"danube_clis/danube_cli/producer/#message-configuration","title":"Message Configuration","text":"<ul> <li> <p><code>-y, --schema &lt;SCHEMA&gt;</code>   Message schema type: <code>bytes</code>, <code>string</code>, <code>int64</code>, <code>json</code> (default: <code>string</code>)</p> </li> <li> <p><code>--json-schema &lt;JSON_SCHEMA&gt;</code>   Required JSON schema definition when using <code>json</code> schema type</p> </li> <li> <p><code>-a, --attributes &lt;ATTRIBUTES&gt;</code>   Message attributes in <code>key1:value1,key2:value2</code> format</p> </li> <li> <p><code>-c, --count &lt;COUNT&gt;</code>   Number of messages to send (default: <code>1</code>)</p> </li> <li> <p><code>-i, --interval &lt;INTERVAL&gt;</code>   Delay between messages in milliseconds (default: 500, minimum: 100)</p> </li> </ul>"},{"location":"danube_clis/danube_cli/producer/#reliable-delivery-options","title":"Reliable Delivery Options","text":"<ul> <li> <p><code>--reliable</code>   Enable reliable message delivery with storage persistence</p> </li> <li> <p><code>-h, --help</code>   Description: Print help information</p> </li> </ul>"},{"location":"danube_clis/danube_cli/producer/#example","title":"Example","text":""},{"location":"danube_clis/danube_cli/producer/#basic-message-production","title":"Basic Message Production","text":"<pre><code>danube-cli produce --service-addr http://localhost:6650 --count 100 --message \"Hello Danube\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#json-messages-with-schema","title":"JSON Messages with Schema","text":"<pre><code>danube-cli produce -s http://localhost:6650 -c 100 \\\n  -y json \\\n  --json-schema '{\"type\": \"object\", \"properties\": {\"field1\": {\"type\": \"string\"}}}' \\\n  -m '{\"field1\":\"Hello Danube\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#reliable-message-delivery","title":"Reliable Message Delivery","text":"<pre><code>danube-cli produce -s http://localhost:6650 -m \"Hello Danube\" -c 100 --reliable\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#binary-file-send-with-reliable-delivery","title":"Binary File send with Reliable Delivery","text":"<pre><code>danube-cli produce -s http://localhost:6650 -m \"none\" -f ./data.blob -c 100 --reliable\n</code></pre>"},{"location":"development/dev_environment/","title":"Development Environment Setup for Danube Broker","text":"<p>This document guides you through setting up the development environment, running danube broker instances, and be able to effectively contribute to the code.</p>"},{"location":"development/dev_environment/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following installed:</p> <ul> <li> <p>Rust: Ensure you have Rust installed. You can download and install it from the Rust website.</p> </li> <li> <p>Docker: Install Docker if you haven\u2019t already. Follow the installation instructions on the Docker website.</p> </li> </ul>"},{"location":"development/dev_environment/#contributing-to-the-repository","title":"Contributing to the Repository","text":"<ol> <li> <p>Fork the Repository:</p> </li> <li> <p>Go to the Danube Broker GitHub repository.</p> </li> <li> <p>Click the \"Fork\" button on the top right corner of the page to create your own copy of the repository.</p> </li> <li> <p>Clone Your Fork:</p> </li> </ol> <p>Once you have forked the repository, clone your forked repository:</p> <pre><code>git clone https://github.com/&lt;your-username&gt;/danube.git\ncd danube\n</code></pre> <ol> <li>Add the Original Repository as a Remote (optional but recommended for keeping up-to-date):</li> </ol> <pre><code>git remote add upstream https://github.com/danube-messaging/danube.git\n</code></pre>"},{"location":"development/dev_environment/#building-the-project","title":"Building the Project","text":"<ol> <li>Build the Project:</li> </ol> <p>To build the Danube Broker:</p> <pre><code>cargo build \nor  \ncargo build --release\n</code></pre>"},{"location":"development/dev_environment/#running-etcd","title":"Running ETCD","text":"<ol> <li>Start ETCD:</li> </ol> <p>Use the Makefile to start an ETCD instance. This will run ETCD in a Docker container.</p> <pre><code>make etcd\n</code></pre> <ol> <li>Clean Up ETCD:</li> </ol> <p>To stop and remove the ETCD instance and its data:</p> <pre><code>make etcd-clean\n</code></pre>"},{"location":"development/dev_environment/#running-a-single-broker-instance","title":"Running a Single Broker Instance","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. If not, use the <code>make etcd</code> command to start it.</p> <ol> <li>Run the Broker:</li> </ol> <p>Use the following command to start a single broker instance:</p> <pre><code>RUST_LOG=danube_broker=info target/debug/danube-broker --config-file config/danube_broker.yml\n</code></pre>"},{"location":"development/dev_environment/#running-multiple-broker-instances","title":"Running Multiple Broker Instances","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. Use:</p> <pre><code>make etcd\n</code></pre> <ol> <li>Run Multiple Brokers:</li> </ol> <p>Use the following Makefile command to start multiple broker instances:</p> <pre><code>make brokers\n</code></pre> <p>This will start brokers on ports 6650, 6651, and 6652. Logs for each broker will be saved in <code>temp/</code> directory.</p> <ol> <li>Clean Up Broker Instances:</li> </ol> <p>To stop all running broker instances:</p> <pre><code>make brokers-clean\n</code></pre>"},{"location":"development/dev_environment/#reading-logs","title":"Reading Logs","text":"<p>Logs for each broker instance are stored in the <code>temp/</code> directory. You can view them using:</p> <pre><code>cat temp/broker_&lt;port&gt;.log\n</code></pre> <p>Replace <code>&lt;port&gt;</code> with the actual port number (6650, 6651, or 6652).</p>"},{"location":"development/dev_environment/#inspecting-etcd-metadata","title":"Inspecting ETCD Metadata","text":"<ol> <li>Set Up <code>etcdctl</code>:</li> </ol> <p>Export the following environment variables:</p> <pre><code>export ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=http://localhost:2379\n</code></pre> <ol> <li>Inspect Metadata:</li> </ol> <p>Use <code>etcdctl</code> commands to inspect metadata. For example, to list all keys:</p> <pre><code>etcdctl get \"\" --prefix\n</code></pre> <p>To get a specific key:</p> <pre><code>etcdctl get &lt;key&gt;\n</code></pre>"},{"location":"development/dev_environment/#makefile-targets-summary","title":"Makefile Targets Summary","text":"<ul> <li><code>make etcd</code>: Starts an ETCD instance in Docker.</li> <li><code>make etcd-clean</code>: Stops and removes the ETCD instance and its data.</li> <li><code>make brokers</code>: Builds and starts broker instances on predefined ports.</li> <li><code>make brokers-clean</code>: Stops and removes all running broker instances.</li> </ul>"},{"location":"development/dev_environment/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>ETCD Not Starting: Check Docker logs and ensure no other service is using port 2379.</li> <li>Broker Not Starting: Ensure ETCD is running and accessible at the specified address and port.</li> </ul>"},{"location":"development/internal_resources/","title":"Resources mapping","text":"<p>This document describes how the resources are organized in the Metadata store</p>"},{"location":"development/internal_resources/#metadatastore-and-localcache","title":"MetadataStore and LocalCache","text":"<p>Basically the entire configuration and the metadata for all the cluster's objects (topics, namespaces, etc) are stored in MetadataStorage (ETCD) and in the LocalCache to ensure fast retrieval for the local broker and to reduce the number of request to the metadata database.</p> <p>The pattern:</p> <ul> <li>Put / Delete requests should use MetadataStore (ETCD), to ensure consistency across cluster</li> <li>Get requests should be served from the Local Cache</li> </ul> <p>The LocalCache continuously update from 2 sources for increase consistency:</p> <ul> <li>the Watch operation on ETCD</li> <li>the Syncronizer topic, where all Put / Delete requests are published and read by the brokers.</li> </ul>"},{"location":"development/internal_resources/#resources-types","title":"Resources Types","text":""},{"location":"development/internal_resources/#cluster-resources","title":"Cluster Resources","text":"<p>Holds information about the cluster and the cluster's brokers. Mainly read and write by Danube Service.</p> <ul> <li>/cluster/cluster-name</li> <li>holds a String with the name of the cluster</li> <li>/cluster/register/{broker-id}</li> <li>the broker register once it join the cluster, contain the broker metadata (broker id &amp; socket addr)  </li> <li>/cluster/brokers/{broker-id}/{namespace}/{topic}</li> <li>topics served by the broker, with value ()</li> <li>Load Manager updates the path, with topic assignments to brokers</li> <li>Brokers should watch it's own path like (/cluster/brokers/1122334455) - and perform the neccesary actions on adding or removing a topic</li> <li>/cluster/unassigned/{namespace}/{topic}</li> <li>New unassigned topics created by Broker</li> <li>Load Manager should watch this path, add assign the topic to a broker</li> <li>/cluster/load/{broker-id}</li> <li>broker periodically reports its load metrics on this path</li> <li>Load Manager watch this path to calculate broker load rankings for the cluster</li> <li>/cluster/load_balance</li> <li>the load_balance updated decision, posted by the Load Manager, contain a HashMap with keys the broker_id and value the list of topic_name</li> <li>/cluster/leader</li> <li>the value posted by Leader Election service, it holds broker_id of the current Leader of the CLuster</li> </ul> <p>Example:</p> <ul> <li>/cluster/brokers/1122334455/markets/trade-events - value is ()</li> <li>/cluster/brokers/1122334455/markets/trade-events - value is ()</li> </ul>"},{"location":"development/internal_resources/#namespace-resources","title":"Namespace Resources","text":"<p>Holds information about the namespace policy and the namespace's topics</p> <ul> <li>/namespaces/{namespace}/policy</li> <li>/namespaces/{namespace}/topics/{namespace}/{topic}</li> </ul> <p>Example:</p> <ul> <li>/namespaces/markets/policy - the value stores a Json like { \"retentionTimeInMinutes\": 1440 }</li> <li>/namespaces/markets/topics/markets/trade-events - topics part of the namespace, value is ()</li> </ul>"},{"location":"development/internal_resources/#topic-resources","title":"Topic Resources","text":"<p>Holds information about the topic policy and the associated producers / subscriptions, including partitioned topic.</p> <ul> <li>/topics/{namespace}/{topic}/policy</li> <li>holds the topic policy, the value stores a Json</li> <li>/topics/{namespace}/{topic}/schema</li> <li>holds the topic schema, the value stores the schema</li> <li>/topics/{namespace}/{topic}/producers/{producer_id}</li> <li>holds the producer config</li> <li>/topics/{namespace}/{topic}/subscriptions/{subscription_name}</li> <li>holds the subscription config</li> </ul> <p>Example:</p> <ul> <li>/topics/markets/trade-events/producers/1122334455 - with value Producer Metadata</li> <li>/topics/markets/trade-events/subscriptions/my_subscription - with value Subscription Metadata</li> <li>/topics/markets/trade-events-part-1/policy - where /markets/trade-events-part-1 is the partitioned topic that stores partition policy</li> </ul>"},{"location":"development/internal_resources/#subscriptions-resources","title":"Subscriptions Resources","text":"<p>Holds information about the topic subscriptions, including associated consumers</p> <ul> <li>/subscriptions/{subscription_name}/{consumer_id}</li> <li>holds the consumer metadata</li> </ul> <p>Example:</p> <ul> <li>/subscriptions/my_subscription/23232323</li> </ul>"},{"location":"getting_started/Danube_docker_compose/","title":"Run Danube with Docker Compose","text":"<p>This guide provides instructions on how to run Danube Messaging using Docker and Docker Compose. It sets up ETCD for metadata storage and MinIO for topic persistence storage.</p>"},{"location":"getting_started/Danube_docker_compose/#architecture-overview","title":"Architecture Overview","text":"<p>The setup includes:</p> <ul> <li>2 Danube Brokers: High-availability message brokers with load balancing</li> <li>ETCD: Distributed metadata store for cluster coordination</li> <li>MinIO: S3-compatible object storage for persistent message storage</li> <li>MinIO Client (MC): Automatic bucket creation and configuration</li> </ul> <p></p>"},{"location":"getting_started/Danube_docker_compose/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine 20.10+</li> <li>Docker Compose 2.0+</li> <li>At least 4GB RAM available for containers</li> <li>Ports 2379, 2380, 6650-6651, 9000-9001, 9040-9041, 50051-50052 available</li> </ul>"},{"location":"getting_started/Danube_docker_compose/#quick-start","title":"Quick Start","text":""},{"location":"getting_started/Danube_docker_compose/#step-1-setup-choose-one-option","title":"Step 1: Setup (Choose One Option)","text":"<p>Option 1: Download Docker Compose Files (Recommended for running the broker)</p> <p>Create a directory and download the required files:</p> <pre><code>mkdir danube-docker &amp;&amp; cd danube-docker\n</code></pre> <p>Download the docker-compose file:</p> <pre><code>curl -O https://raw.githubusercontent.com/danube-messaging/danube/main/docker/docker-compose.yml\n</code></pre> <p>Download the broker configuration file:</p> <pre><code>curl -O https://raw.githubusercontent.com/danube-messaging/danube/main/docker/danube_broker.yml\n</code></pre> <p>Option 2: Clone Repository (Recommended for development and building from source)</p> <pre><code>git clone https://github.com/danube-messaging/danube.git\ncd danube/docker\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#step-2-start-the-cluster","title":"Step 2: Start the Cluster","text":"<p>Start the entire cluster:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#step-3-verify-all-services-are-healthy","title":"Step 3: Verify all services are healthy","text":"<p>Verify all services are running:</p> <pre><code>docker-compose ps\n</code></pre> <p>Expected output:</p> <pre><code>\u2717 docker-compose ps\nNAME        IMAGE        COMMAND       SERVICE         CREATED        STATUS       PORTS\n\ndanube-broker1   docker-broker1                             \"/usr/local/bin/danu\u2026\"   broker1      About a minute ago   Up 6 seconds (health: starting)   0.0.0.0:6650-&gt;6650/tcp, [::]:6650-&gt;6650/tcp, 0.0.0.0:9040-&gt;9040/tcp, [::]:9040-&gt;9040/tcp, 0.0.0.0:50051-&gt;50051/tcp, [::]:50051-&gt;50051/tcp\n\ndanube-broker2   docker-broker2                             \"/usr/local/bin/danu\u2026\"   broker2      About a minute ago   Up 6 seconds (health: starting)   0.0.0.0:6651-&gt;6650/tcp, [::]:6651-&gt;6650/tcp, 0.0.0.0:9041-&gt;9040/tcp, [::]:9041-&gt;9040/tcp, 0.0.0.0:50052-&gt;50051/tcp, [::]:50052-&gt;50051/tcp\n\ndanube-cli       docker-danube-cli                          \"sleep infinity\"         danube-cli   About a minute ago   Up 6 seconds                      \n\ndanube-etcd      quay.io/coreos/etcd:v3.5.9                 \"/usr/local/bin/etcd\"    etcd         About a minute ago   Up 12 seconds (healthy)           0.0.0.0:2379-2380-&gt;2379-2380/tcp, [::]:2379-2380-&gt;2379-2380/tcp\n\ndanube-mc        minio/mc:RELEASE.2024-09-16T17-43-14Z      \"/bin/sh -c ' echo '\u2026\"   mc           About a minute ago   Up About a minute                 \n\ndanube-minio     minio/minio:RELEASE.2025-07-23T15-54-02Z   \"/usr/bin/docker-ent\u2026\"   minio        About a minute ago   Up About a minute (healthy)       0.0.0.0:9000-9001-&gt;9000-9001/tcp, [::]:9000-9001-&gt;9000-9001/tcp\n</code></pre> <p>Check logs (optional):</p> <pre><code># View all logs\ndocker-compose logs -f\n\n# View specific service logs\ndocker-compose logs -f broker1\ndocker-compose logs -f broker2\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#service-endpoints","title":"Service Endpoints","text":"Service Endpoint Purpose Danube Broker 1 <code>localhost:6650</code> gRPC messaging Danube Broker 2 <code>localhost:6651</code> gRPC messaging Admin API 1 <code>localhost:50051</code> Broker administration Admin API 2 <code>localhost:50052</code> Broker administration Prometheus 1 <code>localhost:9040</code> Metrics and monitoring Prometheus 2 <code>localhost:9041</code> Metrics and monitoring MinIO API <code>localhost:9000</code> S3-compatible storage MinIO Console <code>localhost:9001</code> Web UI (minioadmin/minioadmin123) ETCD <code>localhost:2379</code> Metadata store"},{"location":"getting_started/Danube_docker_compose/#testing-with-danube-cli","title":"Testing with Danube CLI","text":""},{"location":"getting_started/Danube_docker_compose/#using-the-cli-container","title":"Using the CLI Container","text":"<p>The Docker Compose setup includes a <code>danube-cli</code> container with both <code>danube-cli</code> and <code>danube-admin-cli</code> tools pre-installed. This eliminates the need to build or install Rust locally.</p> <p>No local installation required - use the containerized CLI tools directly.</p>"},{"location":"getting_started/Danube_docker_compose/#reliable-messaging-with-s3-storage","title":"Reliable Messaging with S3 Storage","text":"<p>Test the cloud-ready persistent storage capabilities:</p> <p>Produce with reliable delivery and S3 persistence:</p> <pre><code>docker exec -it danube-cli danube-cli produce \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/persistent-topic\" \\\n  --count 1000 \\\n  --message \"Persistent message\" \\\n  --reliable\n</code></pre> <p>Consume persistent messages:</p> <pre><code>docker exec -it danube-cli danube-cli consume \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/persistent-topic\" \\\n  --subscription \"persistent-sub\" \\\n  --sub-type exclusive\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#non-reliable-message-flow-testing","title":"Non-Reliable Message Flow Testing","text":""},{"location":"getting_started/Danube_docker_compose/#basic-string-messages","title":"Basic string messages","text":"<p>Produce basic string messages:</p> <pre><code>docker exec -it danube-cli danube-cli produce \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/test-topic\" \\\n  --count 100 \\\n  --message \"Hello from Danube Docker!\"\n</code></pre> <p>Consume from shared subscription:</p> <pre><code>docker exec -it danube-cli danube-cli consume \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/test-topic\" \\\n  --subscription \"shared-sub\" \\\n  --consumer \"docker-consumer\"\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#json-schema-messages","title":"JSON schema messages","text":"<p>Produce JSON messages with schema:</p> <pre><code>docker exec -it danube-cli danube-cli produce \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/json-topic\" \\\n  --count 100 \\\n  --schema json \\\n  --json-schema '{\"type\":\"object\",\"properties\":{\"message\":{\"type\":\"string\"},\"timestamp\":{\"type\":\"number\"}}}' \\\n  --message '{\"message\":\"Hello JSON\",\"timestamp\":1640995200}'\n</code></pre> <p>Consume JSON messages:</p> <pre><code>docker exec -it danube-cli danube-cli consume \\\n  --service-addr http://broker2:6650 \\\n  --topic \"/default/json-topic\" \\\n  --subscription \"json-sub\" \\\n  --consumer \"json-consumer\"\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#admin-cli-operations","title":"Admin CLI Operations","text":"<p>Use danube-admin-cli for cluster management:</p> <pre><code># List active brokers\ndocker exec -it danube-cli danube-admin-cli brokers list\n\n# List namespaces in cluster\ndocker exec -it danube-cli danube-admin-cli brokers namespaces\n\n# List topics in a namespace\ndocker exec -it danube-cli danube-admin-cli topics list default\n\n# List subscriptions on a topic\ndocker exec -it danube-cli danube-admin-cli topics subscriptions /default/test-topic\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"getting_started/Danube_docker_compose/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>Access broker metrics for monitoring:</p> <pre><code># Broker 1 metrics\ncurl http://localhost:9040/metrics\n\n# Broker 2 metrics  \ncurl http://localhost:9041/metrics\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#minio-console","title":"MinIO Console","text":"<ol> <li>Open http://localhost:9001 in your browser</li> <li>Login with credentials: <code>minioadmin</code> / <code>minioadmin123</code></li> <li>Navigate to \"Buckets\" to see:</li> <li><code>danube-messages</code>: Persistent message storage</li> <li><code>danube-wal</code>: Write-ahead log storage</li> </ol>"},{"location":"getting_started/Danube_docker_compose/#etcd-inspection","title":"ETCD Inspection","text":"<pre><code># List all keys in ETCD\ndocker exec danube-etcd etcdctl --endpoints=http://127.0.0.1:2379 get --prefix \"\"\n\n# Watch for changes\ndocker exec danube-etcd etcdctl --endpoints=http://127.0.0.1:2379 watch --prefix \"\"\n\n# Check broker registrations\ndocker exec danube-etcd etcdctl --endpoints=http://127.0.0.1:2379 get --prefix \"/cluster/register\"\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#configuration","title":"Configuration","text":""},{"location":"getting_started/Danube_docker_compose/#broker-configuration","title":"Broker Configuration","text":"<p>The <code>danube_broker.yml</code> file is optimized for:</p> <ul> <li>S3 Storage: MinIO integration with automatic credential management</li> <li>High Performance: Optimized WAL rotation and batch sizes</li> <li>Development: Relaxed security and unlimited resource policies</li> <li>Monitoring: Prometheus metrics enabled on all brokers</li> </ul>"},{"location":"getting_started/Danube_docker_compose/#environment-variables","title":"Environment Variables","text":"<p>Key environment variables used:</p> <ul> <li><code>AWS_ACCESS_KEY_ID=minioadmin</code></li> <li><code>AWS_SECRET_ACCESS_KEY=minioadmin123</code></li> <li><code>AWS_REGION=us-east-1</code></li> <li><code>RUST_LOG=danube_broker=info,danube_core=info</code></li> </ul>"},{"location":"getting_started/Danube_docker_compose/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting_started/Danube_docker_compose/#common-issues","title":"Common Issues","text":"<ol> <li>Port conflicts: Ensure all required ports are available</li> <li>Memory issues: Increase Docker memory allocation if containers fail to start</li> <li>Storage issues: Check MinIO bucket creation in logs: <code>docker-compose logs mc</code></li> </ol>"},{"location":"getting_started/Danube_docker_compose/#reset-environment","title":"Reset Environment","text":"<pre><code># Stop and remove all containers, networks, and volumes\ndocker-compose down -v\n\n# Remove all Danube-related Docker resources\ndocker volume prune -f\ndocker network prune -f\n\n# Restart fresh\ndocker-compose up -d\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#production-considerations","title":"Production Considerations","text":"<p>This setup demonstrates Danube's cloud-ready capabilities. For production deployment:</p> <ol> <li>Replace MinIO with AWS S3, Google Cloud Storage, or Azure Blob Storage</li> <li>Enable TLS/SSL authentication in broker configuration</li> <li>Configure resource limits and health checks appropriately</li> <li>Set up monitoring with Prometheus and Grafana</li> <li>Implement backup strategies for ETCD and persistent storage</li> <li>Use container orchestration like Kubernetes for scaling</li> </ol>"},{"location":"getting_started/Danube_docker_compose/#aws-s3-migration","title":"AWS S3 Migration","text":"<p>To migrate from MinIO to AWS S3, update <code>danube_broker.yml</code>:</p> <pre><code>wal_cloud:\n  cloud:\n    backend: \"s3\"\n    root: \"s3://your-production-bucket/danube-cluster\"\n    region: \"us-west-2\"\n    # Remove endpoint for AWS S3\n    # endpoint: \"http://minio:9000\"  \n    # Use IAM roles or environment variables for credentials\n</code></pre> <p>This Docker Compose setup showcases Danube's architecture with cloud-native storage.</p>"},{"location":"getting_started/Danube_kubernetes/","title":"Run Danube messaging on Kubernetes","text":"<p>This documentation covers the instalation of the Danube cluster on the kubernetes. The Helm chart deploys the Danube Cluster with ETCD as metadata storage in the same namespace.</p> <p>The documentation assumes that you have a Kubernetes cluster running and that you have installed the Helm package manager. For local testing you can use kind.</p>"},{"location":"getting_started/Danube_kubernetes/#install-the-ngnix-ingress-controller","title":"Install the Ngnix Ingress controller","text":"<p>Using the Official NGINX Ingress Helm Chart. This is required in order to route traffic to each broker service in the cluster. The Broker configuration is provisioned in the danube_helm, you can tweak the values.yaml per your needs.</p> <p>The Danube messaging has no dependency on ngnix, can work with any ingress controller of your choice.</p> <p>Install the NGINX Ingress Controller using Helm:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#run-the-nginx-ingress-controller-on-your-local-kubernetes-cluster","title":"Run the NGINX Ingress Controller on your local kubernetes cluster","text":"<p>For local testing the NGINX Ingress Controller can be exposed using a NodePort service so that the traffic from the local machine (outside the cluster) can reach the Ingress controller.</p> <pre><code>helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.service.type=NodePort\n</code></pre> <p>You can find out which port is assigned by running</p> <pre><code>kubectl get svc\n\nNAME                                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nkubernetes                                         ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                      4m17s\nnginx-ingress-ingress-nginx-controller             NodePort    10.96.245.118   &lt;none&gt;        80:30115/TCP,443:30294/TCP   2m58s\nnginx-ingress-ingress-nginx-controller-admission   ClusterIP   10.96.169.82    &lt;none&gt;        443/TCP                      2m58s\n</code></pre> <p>If ngnix is running as NodePort (usually for testing), you need local port in this case 30115, in order to provide to danube_helm installation.</p>"},{"location":"getting_started/Danube_kubernetes/#run-the-nginx-ingress-controller-in-a-remote-cluster-cloud","title":"Run the NGINX Ingress Controller in a remote cluster (cloud)","text":"<pre><code>helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.publishService.enabled=true\n</code></pre> <ul> <li>The publishService feature enables the Ingress controller to publish information about itself (such as its external IP or hostname) in a Kubernetes Service resource.</li> <li>This is particularly useful when you are running the Ingress controller in a cloud environment (like AWS, GCP, or Azure) and need it to publish its external IP address to handle incoming traffic</li> </ul>"},{"location":"getting_started/Danube_kubernetes/#install-danube-messaging-brokers","title":"Install Danube Messaging Brokers","text":"<p>First, add the repository to your Helm client:</p> <pre><code>helm repo add danube https://danube-messaging.github.io/danube_helm\nhelm repo update\n</code></pre> <p>You can install the chart with the release name <code>my-danube-cluster</code> using the below command. This will deploy the Danube Broker and an ETCD instance with the default configuration.</p>"},{"location":"getting_started/Danube_kubernetes/#running-on-the-local-kubernetes-cluster","title":"Running on the local kubernetes cluster","text":"<pre><code>helm install my-danube-cluster danube/danube-helm-chart --set broker.service.advertisedPort=30115\n</code></pre> <p>The advertisedPort is used to allow the client to reach the brokers, through the ingress NodePort.</p>"},{"location":"getting_started/Danube_kubernetes/#running-on-the-remote-cluster-cloud","title":"Running on the remote cluster (cloud)","text":"<p>The Danube cluster configuration from the values.yaml file has to be adjusted for your needs.</p> <p>You can override the default values by providing a custom <code>values.yaml</code> file:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart -f custom-values.yaml\n</code></pre> <p>Alternatively, you can specify individual values using the <code>--set</code> flag:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart --set broker.service.type=\"ClusterIP\"\n</code></pre> <p>You can further customize the installation, check the readme file. The default configuration is running 3 Danube Brokers in cluster.</p>"},{"location":"getting_started/Danube_kubernetes/#check-the-install","title":"Check the install","text":"<p>Make sure that the brokers, etcd and the ngnix ingress are running properly in the cluster.</p>"},{"location":"getting_started/Danube_kubernetes/#example-running-on-local-kubernetes-cluster","title":"Example running on local kubernetes cluster","text":"<pre><code>kubectl get all\n\nNAME                                                          READY   STATUS    RESTARTS   AGE\npod/my-danube-cluster-danube-broker1-766665d6f4-qdbf6         1/1     Running   0          12s\npod/my-danube-cluster-danube-broker2-5774ff4dd6-dvx66         1/1     Running   0          12s\npod/my-danube-cluster-danube-broker3-6db6b5fccd-dkr2k         1/1     Running   0          12s\npod/my-danube-cluster-etcd-867f5b85f8-g4m9m                   1/1     Running   0          12s\npod/nginx-ingress-ingress-nginx-controller-7bc7c7776d-wqc5g   1/1     Running   0          47m\n\nNAME                                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                       AGE\nservice/kubernetes                                         ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                       48m\nservice/my-danube-cluster-danube-broker1                   ClusterIP   10.96.40.244    &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-danube-broker2                   ClusterIP   10.96.204.21    &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-danube-broker3                   ClusterIP   10.96.46.5      &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-etcd                             ClusterIP   10.96.232.70    &lt;none&gt;        2379/TCP                      12s\nservice/nginx-ingress-ingress-nginx-controller             NodePort    10.96.245.118   &lt;none&gt;        80:30115/TCP,443:30294/TCP    47m\nservice/nginx-ingress-ingress-nginx-controller-admission   ClusterIP   10.96.169.82    &lt;none&gt;        443/TCP                       47m\n\nNAME                                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/my-danube-cluster-danube-broker1         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-danube-broker2         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-danube-broker3         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-etcd                   1/1     1            1           12s\ndeployment.apps/nginx-ingress-ingress-nginx-controller   1/1     1            1           47m\n\nNAME                                                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/my-danube-cluster-danube-broker1-766665d6f4         1         1         1       12s\nreplicaset.apps/my-danube-cluster-danube-broker2-5774ff4dd6         1         1         1       12s\nreplicaset.apps/my-danube-cluster-danube-broker3-6db6b5fccd         1         1         1       12s\nreplicaset.apps/my-danube-cluster-etcd-867f5b85f8                   1         1         1       12s\nreplicaset.apps/nginx-ingress-ingress-nginx-controller-7bc7c7776d   1         1         1       47m\n</code></pre> <p>Validate that the brokers have started correctly:</p> <pre><code>kubectl logs pod/my-danube-cluster-danube-broker1-766665d6f4-qdbf6\n\ninitializing metrics exporter\n2024-08-28T04:30:22.969462Z  INFO danube_broker: Use ETCD storage as metadata persistent store\n2024-08-28T04:30:22.969598Z  INFO danube_broker: Start the Danube Service\n2024-08-28T04:30:22.969612Z  INFO danube_broker::danube_service: Setting up the cluster MY_CLUSTER\n2024-08-28T04:30:22.971978Z  INFO danube_broker::danube_service::local_cache: Initial cache populated\n2024-08-28T04:30:22.972013Z  INFO danube_broker::danube_service: Started the Local Cache service.\n2024-08-28T04:30:22.990763Z  INFO danube_broker::danube_service::broker_register: Broker 14150019297734190044 registered in the cluster\n2024-08-28T04:30:22.991620Z  INFO danube_broker::danube_service: Namespace default already exists.\n2024-08-28T04:30:22.991926Z  INFO danube_broker::danube_service: Namespace system already exists.\n2024-08-28T04:30:22.992480Z  INFO danube_broker::danube_service: Namespace default already exists.\n2024-08-28T04:30:22.992490Z  INFO danube_broker::danube_service: cluster metadata setup completed\n2024-08-28T04:30:22.992551Z  INFO danube_broker::danube_service:  Started the Broker GRPC server\n2024-08-28T04:30:22.992563Z  INFO danube_broker::broker_server: Server is listening on address: 0.0.0.0:6650\n2024-08-28T04:30:22.992605Z  INFO danube_broker::danube_service: Started the Leader Election service\n2024-08-28T04:30:22.993050Z  INFO danube_broker::danube_service: Started the Load Manager service.\n2024-08-28T04:30:22.993143Z  INFO danube_broker::danube_service:  Started the Danube Admin GRPC server\n2024-08-28T04:30:22.993274Z  INFO danube_broker::admin: Admin is listening on address: 0.0.0.0:50051\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#setup-in-order-to-communicate-with-cluster-danube-brokers","title":"Setup in order to communicate with cluster danube brokers","text":"<p>If you would like to communicate to the messaging sytem by using the danube-cli tool, or your own danube clients running locally, you can do the following:</p> <pre><code>kubectl get nodes -o wide\nNAME                 STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION       CONTAINER-RUNTIME\nkind-control-plane   Ready    control-plane   53m   v1.30.0   172.20.0.2    &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   5.15.0-118-generic   containerd://1.7.15\n</code></pre> <p>Use the INTERNAL-IP to route the traffic to broker hosts. Add the following in the hosts file, but make sure you match the number and the name of the brokers from the helm values.yaml file.</p> <pre><code>cat /etc/hosts\n172.20.0.2 broker1.example.com broker2.example.com broker3.example.com\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#inspect-the-etcd-instance","title":"Inspect the etcd instance","text":"<p>If you want to connect from your local machine, use kubectl port-forward to forward the etcd port to your local machine:</p> <p>Port Forward etcd Service:</p> <pre><code>kubectl port-forward service/my-danube-cluster-etcd 2379:2379\n</code></pre> <p>Once port forwarding is set up, you can run etcdctl commands from your local machine:</p> <pre><code>etcdctl --endpoints=http://localhost:2379 watch --prefix /\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#cleanup","title":"Cleanup","text":"<p>To uninstall the <code>my-danube-cluster</code> release:</p> <pre><code>helm uninstall my-danube-cluster\n</code></pre> <p>This command removes all the Kubernetes components associated with the chart and deletes the release.</p>"},{"location":"getting_started/Danube_local/","title":"Run Danube Broker on your local machine","text":""},{"location":"getting_started/Danube_local/#start-metadata-storage-etcd","title":"Start Metadata Storage (ETCD)","text":"<p>Danube uses ETCD for metadata storage to provide high availability and scalability. Run ETCD using Docker:</p> <pre><code>docker run -d --name etcd-danube -p 2379:2379 quay.io/coreos/etcd:latest etcd --advertise-client-urls http://0.0.0.0:2379 --listen-client-urls http://0.0.0.0:2379\n</code></pre> <p>Verify ETCD is running:</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS          PORTS                                                 NAMES\n27792bce6077   quay.io/coreos/etcd:latest   \"etcd --advertise-cl\u2026\"   35 seconds ago   Up 34 seconds   0.0.0.0:2379-&gt;2379/tcp, :::2379-&gt;2379/tcp, 2380/tcp   etcd-danube\n</code></pre>"},{"location":"getting_started/Danube_local/#configure-and-run-danube-broker","title":"Configure and Run Danube Broker","text":""},{"location":"getting_started/Danube_local/#create-and-configure-broker-config","title":"Create and configure broker config","text":"<p>Create a local config file, use the sample config file as a reference.</p> <pre><code>touch danube_broker.yml\n</code></pre>"},{"location":"getting_started/Danube_local/#download-and-run-the-danube-broker","title":"Download and run the Danube Broker","text":"<p>Download the latest binary from the releases page.</p> <p>If you would like to run Danube Brokers in cluster, you need to upload the binary to each machine and use the same cluster configuration name.</p> <p>Run the Danube Broker:</p> <pre><code>touch broker.log\n</code></pre> <pre><code>RUST_LOG=info ./danube-broker-linux --config-file danube_broker.yml --broker-addr \"0.0.0.0:6650\" --admin-addr \"0.0.0.0:50051\" &gt; broker.log 2&gt;&amp;1 &amp;\n</code></pre> <p>Check the logs:</p> <pre><code>tail -n 100 -f broker.log\n</code></pre> <pre><code>2025-01-12T06:15:53.705416Z  INFO danube_broker: Use ETCD storage as metadata persistent store\n2025-01-12T06:15:53.705665Z  INFO danube_broker: Start the Danube Service\n2025-01-12T06:15:53.705679Z  INFO danube_broker::danube_service: Setting up the cluster MY_CLUSTER\n2025-01-12T06:15:53.707988Z  INFO danube_broker::danube_service::local_cache: Initial cache populated\n2025-01-12T06:15:53.709521Z  INFO danube_broker::danube_service: Started the Local Cache service.\n2025-01-12T06:15:53.713329Z  INFO danube_broker::danube_service::broker_register: Broker 15139934490483381581 registered in the cluster\n2025-01-12T06:15:53.714977Z  INFO danube_broker::danube_service: Namespace default already exists.\n2025-01-12T06:15:53.716405Z  INFO danube_broker::danube_service: Namespace system already exists.\n2025-01-12T06:15:53.717979Z  INFO danube_broker::danube_service: Namespace default already exists.\n2025-01-12T06:15:53.718012Z  INFO danube_broker::danube_service: cluster metadata setup completed\n2025-01-12T06:15:53.718092Z  INFO danube_broker::danube_service:  Started the Broker GRPC server\n2025-01-12T06:15:53.718116Z  INFO danube_broker::broker_server: Server is listening on address: 0.0.0.0:6650\n2025-01-12T06:15:53.718191Z  INFO danube_broker::danube_service: Started the Leader Election service\n2025-01-12T06:15:53.722454Z  INFO danube_broker::danube_service: Started the Load Manager service.\n2025-01-12T06:15:53.724727Z  INFO danube_broker::danube_service:  Started the Danube Admin GRPC server\n2025-01-12T06:15:53.724727Z  INFO danube_broker::admin: Admin is listening on address: 0.0.0.0:50051\n</code></pre>"},{"location":"getting_started/Danube_local/#use-danube-cli-to-publish-and-consume-messages","title":"Use Danube CLI to Publish and Consume Messages","text":"<p>Download the latest Danube CLI binary from the releases page and run it:</p> <pre><code>./danube-cli-linux produce -s http://127.0.0.1:6650 -t /default/demo_topic -c 1000 -m \"Hello, Danube!\"\n</code></pre> <pre><code>Message sent successfully with ID: 9\nMessage sent successfully with ID: 10\nMessage sent successfully with ID: 11\nMessage sent successfully with ID: 12\n</code></pre> <p>Open a new terminal and run the below command to consume the messages:</p> <pre><code>./danube-cli-linux consume -s http://127.0.0.1:6650 -t /default/demo_topic -m my_subscription\n</code></pre> <pre><code>Received bytes message: 9, with payload: Hello, Danube!\nReceived bytes message: 10, with payload: Hello, Danube!\nReceived bytes message: 11, with payload: Hello, Danube!\nReceived bytes message: 12, with payload: Hello, Danube!\n</code></pre>"},{"location":"getting_started/Danube_local/#validate","title":"Validate","text":"<p>Ensure ETCD is running and accessible. You can check its status by accessing <code>http://&lt;ETCD_SERVER_IP&gt;:2379</code> from a browser or using <code>curl</code>:</p> <pre><code>curl http://&lt;ETCD_SERVER_IP&gt;:2379/v3/version\n</code></pre> <p>Ensure each broker instance is running and listening on the specified port. You can check this with <code>netstat</code> or <code>ss</code>:</p> <pre><code>netstat -tuln | grep 6650\n</code></pre> <p>For debugging, check the logs of each Danube broker instance.</p>"},{"location":"getting_started/Danube_local/#cleanup","title":"Cleanup","text":"<p>Stop the Danube Broker:</p> <pre><code>pkill danube-broker\n</code></pre> <p>Stop and remove ETCD container</p> <pre><code>docker stop etcd-danube\n</code></pre> <pre><code>docker rm -f etcd-danube\n</code></pre> <p>Verify cleanup</p> <pre><code>ps aux | grep danube-broker\ndocker ps | grep etcd-danube\n</code></pre>"}]}