{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Danube Pub/Sub messaging docs","text":"<p>Danube is an open-source, distributed messaging broker platform, developed in Rust.</p> <p>Danube aims to be a lightweight yet powerful, secure and scalable messaging platform, suitable for event-driven applications. Allows single or multiple producers to publish messages on the topics and multiple consumers, using the subscription models, to consume the messages from the topics.</p> <p>Inspired by the Apache Pulsar messaging and streaming platform, Danube incorporates some similar concepts but is designed to carve its own path within the distributed messaging ecosystem. For additional design considerations, please refer to the Danube Architecture section.</p>"},{"location":"#core-capabilities-of-the-danube-messaging-platform","title":"Core Capabilities of the Danube Messaging Platform","text":"<ul> <li>Topics: A unit of storage that organizes messages into a stream.</li> <li>Non-partitioned topics: Served by a single broker.</li> <li>Partitioned topics: Divided into partitions, served by different brokers within the cluster, enhancing scalability and fault tolerance.</li> <li>Message Dispatch:</li> <li>Non-reliable Message Dispatch: Messages reside in memory and are promptly distributed to consumers, ideal for scenarios where speed is crucial. The acknowledgement mechanism is ignored.</li> <li>Reliable Message Dispatch: The acknowledgement mechanism and persistent storage is used to ensure guaranteed message delivery.</li> <li>Subscription Types::</li> <li>Supports various subscription types (Exclusive, Shared, Failover) enabling different messaging patterns such as message queueing and pub-sub.</li> <li>Flexible Message Schemas</li> <li>Supports multiple message schemas (Bytes, String, Int64, JSON) providing flexibility in message format and structure.</li> </ul>"},{"location":"#crates-within-the-danube-workspace","title":"Crates within the Danube workspace","text":"<p>Danube Broker core crates:</p> <ul> <li>danube-broker - The main crate, danube pubsub platform</li> <li>danube-core - Danube messaging core types and traits</li> <li>danube-metadata-store - Responsibile of Metadata storage and cluster coordination.</li> <li>danube-reliable-dispatch - Responsible of reliable dispatching and topic storage.</li> <li>danube-persisitent-storage - Responsible of persistent storage mechanism.</li> </ul> <p>Danube CLIs and client library:</p> <ul> <li>danube-client - An async Rust client library for interacting with Danube messaging system</li> <li>danube-cli - Client CLI to handle message publishing and consumption</li> <li>danube-admin-cli - Admin CLI designed for interacting with and managing the Danube cluster</li> </ul>"},{"location":"#danube-client-libraries","title":"Danube client libraries","text":"<ul> <li>danube-client - Danube messaging async Rust client library</li> <li>danube-go - Danube messaging Go client library</li> </ul> <p>Contributions in other languages, such as Python, Java, etc., are greatly appreciated.</p>"},{"location":"architecture/PubSub_messaging_vs_Streaming/","title":"Pub/Sub vs Streaming design considerations","text":"<p>The below documentation provides a comparison of Pub/Sub messaging and Streaming architectures, highlighting their key differences and use cases, it is intended to be used as a reference for the Danube Broker.</p>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#pubsub-messaging","title":"Pub/Sub messaging","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#purpose-and-use-cases","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system.</li> <li>Use Cases: Event-driven architectures, real-time notifications, decoupled microservices, and distributed systems. Suitable for scenarios where low latency is critical and some message loss is acceptable, such as real-time monitoring, telemetry data, and ephemeral chat messages.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#architecture-and-design","title":"Architecture and Design","text":"<ul> <li>Components: Consists of Producers, Consumers (subscriptions),  and the message broker.</li> <li>Message Flow: Producers send messages to a broker, which then distributes them to subscribers based on subscription criteria.</li> <li>Scaling: Scales by adding more brokers or distributing load (topics / partitions) across multiple brokers.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#data-handling-and-processing-models","title":"Data Handling and Processing Models","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#pubsub-messaging-producers","title":"Pub/Sub messaging Producers","text":"<ul> <li>Low Latency: Messages sent to topics are not stored on disk, which reduces the latency associated with producing messages.</li> <li>Order Guarantees: Provide ordering within the topic or partition.</li> <li>Message Delivery: There are no guarantees that messages will be delivered. If the broker crashes or if there are network issues, messages might be lost.</li> <li>Transient Acknowledgements: Acknowledgements to the producer are quicker since they are based on in-memory operations rather than disk writes.</li> <li>Publishing Without Consumers: Producers are allowed to publish messages to topics even if there are no active consumers. However, if no consumers are connected, these messages will effectively be dropped because the topics do not store messages.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#pubsub-messaging-consumers","title":"Pub/Sub messaging Consumers","text":"<ul> <li>Real-time Consumption: Consumers of the topics typically process messages in real-time. If a consumer is not available, the message might be lost.</li> <li>No Replay: Since messages are not stored, consumers cannot replay messages. They must process them as they arrive.</li> <li>Reduced Overhead: The topics can handle higher throughput with lower overhead, suitable for use cases where occasional message loss is acceptable.</li> <li>Message Delivery: Messages are delivered to consumers only if they are currently connected and subscribed to the topic. If there are no consumers, the messages are not retained and are discarded by the broker.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#order-of-operations-of-pubsub-messaging","title":"Order of Operations of Pub/Sub messaging","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Broker Receives Message: The broker processes the message.</li> <li>Consumer Availability Check: If consumers are available, the message is delivered to them in real-time.</li> <li>No Consumers: If no consumers are connected, the message is discarded.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#streaming-design-considerations","title":"Streaming design considerations","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#purpose-and-use-cases_1","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for processing and analyzing large volumes of data in real-time as it is generated.</li> <li>Use Cases: Real-time analytics, data pipelines, event sourcing, continuous data processing, and stream processing applications. Ideal for use cases requiring high reliability and message durability, such as financial transactions, order processing, and logging critical events.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#architecture-and-design_1","title":"Architecture and Design","text":"<ul> <li>Components: Consists of producers, consumers, stream processors, and a distributed log.</li> <li>Data Flow: Producers write data to a distributed log, which consumers and stream processors read from in a continuous fashion.</li> <li>Scaling: Designed to handle high throughput and scale horizontally by partitioning data across multiple nodes in a cluster.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#data-handling-and-processing-models_1","title":"Data Handling and Processing Models","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#streaming-producers","title":"Streaming Producers","text":"<ul> <li>Durability: Messages sent to topics are stored to persistent storage and replicated according to the topic's configuration. This ensures that messages are not lost even if brokers crash. This allows playback of streams for for historical data analysis and reprocessing.</li> <li>Acknowledgements: Producers receive acknowledgments once the message is safely stored and replicated. This adds a small amount of latency compared to pub/sub messaging.</li> <li>Order Guarantees: Provide ordering within the topic or partition.</li> <li>Delivery Guarantees: Producers can rely on stronger delivery guarantees (e.g., at least once or effectively once).</li> <li>Publishing Without Consumers: Producers can publish messages to a topic even if there are no active consumers. These messages will be stored by the broker.</li> <li>Processing: Supports complex processing such as windowed operations, aggregations, joins, and stateful transformations.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#streaming-consumers","title":"Streaming Consumers","text":"<ul> <li>Message Retention: Consumers can consume messages at any time as long as the retention policy allows. This is useful for replaying messages, ensuring that no messages are missed.</li> <li>Consumption Acknowledgements: Consumers acknowledge each message, allowing the broker to track which messages have been consumed and manage retention accordingly.</li> <li>Fault Tolerance: If a consumer crashes, it can resume consumption from where it left off, as the messages are stored persistently on the broker.</li> <li>Message Retention: Messages are stored according to the configured retention policies. This ensures that even if no consumers are currently connected, the messages will be available for consumption later.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#order-of-operations-of-streaming","title":"Order of Operations of Streaming","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Broker Receives and Stores Message: The broker stores the message on the persistent storage and replicates it according to the configuration.</li> <li>Message Acknowledgment: The broker acknowledges the producer that the message is safely stored.</li> <li>Consumer Availability Check: If consumers are available, the message is delivered to them.</li> <li>No Consumers: If no consumers are connected, the message remains stored and is available for future consumption.</li> </ul>"},{"location":"architecture/Queuing_PubSub_messaging/","title":"Pub-Sub messaging","text":"<p>Danube is built on the publish-subscribe pattern. In this pattern, producers publish messages to topics; consumers subscribe to those topics, process incoming messages, and send acknowledgments to the broker when processing is finished.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#queuing-vs-pubsub-fan-out","title":"Queuing vs Pub/Sub (fan-out)","text":"<p>Below is some general documentation about Queuing and Pub-Sub, not related to Danube implementation, but just to ensure we are aware of the standards.</p> <p>Messaging queuing and pub-sub are both messaging patterns used for asynchronous communication between applications, but they differ in their approach:</p>"},{"location":"architecture/Queuing_PubSub_messaging/#queuing-messaging","title":"Queuing Messaging","text":"<ul> <li>Model: Point-to-Point (One-to-One). A message producer sends a message to a specific queue, and only one consumer can receive and process that message.</li> <li>Order: Messages are typically processed in the order they are received (FIFO - First-In-First-Out). This ensures tasks are completed sequentially.</li> <li>Delivery: Messages are guaranteed to be delivered at least once. This reliability is crucial for critical tasks.</li> </ul> <p>Examples: Use cases include processing orders, sending emails, or handling failed transactions.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#pubsub-messaging-fan-out","title":"Pub/Sub Messaging (fan-out)","text":"<ul> <li>Model: Publish-Subscribe (One-to-Many). A producer publishes messages to a topic, and any interested subscribers can receive the message. Multiple subscribers can receive the same message.</li> <li>Order: Message order is not guaranteed. Subscribers receive messages as they are published. This is suitable for real-time updates or notifications.</li> <li>Delivery: Delivery is often \"fire-and-forget,\" meaning there's no guarantee a subscriber receives the message. This is acceptable for non-critical data.</li> </ul> <p>Examples: Use cases include broadcasting stock price updates, sending chat messages, or triggering real-time analytics.</p> <p>In Summary:</p> <ul> <li>Messaging queues are for reliable, ordered delivery to a single consumer, ideal for task processing.</li> <li>Pub-sub is for broadcasting messages to many interested parties, good for real-time updates.</li> </ul>"},{"location":"architecture/Queuing_PubSub_messaging/#danube-subscriptions","title":"Danube Subscriptions","text":"<p>A Danube subscription is a named configuration rule that determines how messages are delivered to consumers. It is a lease on a topic established by a group of consumers:</p> <ul> <li>Exclusive (can be used for pub-sub) - The exclusive type is a subscription type that only allows a single consumer to attach to the subscription. If multiple consumers subscribe to a topic using the same subscription, an error occurs.</li> <li>Shared (for queuing) - The shared subscription type Danube allows multiple consumers to attach to the same subscription. Messages are delivered in a round-robin distribution across consumers, and any given message is delivered to only one consumer.</li> </ul>"},{"location":"architecture/Queuing_PubSub_messaging/#queuing-or-pubsub-fan-out-using-danube","title":"Queuing or Pub/Sub (fan-out) using Danube","text":"<ul> <li>If you want to achieve message queuing among consumers, share the same subscription name among multiple consumers</li> <li>If you want to achieve traditional fan-out pub-sub messaging among consumers, specify a unique subscription name for each consumer with an exclusive subscription type.</li> </ul>"},{"location":"architecture/architecture/","title":"Danube Messaging Architecture","text":"<p>The Danube messaging system is a distributed messaging system, based on a publish-subscribe model, aiming to provide high throughput and low latency.</p> <p>The Danube Messaging system's architecture is designed for flexibility and scalability, making it suitable for event-driven and cloud native applications. Its decoupled and plugin architecture allows for independent scaling and easy integration of various storage backends. Using the dispatch strategies and the subscribtion models the system can accomodate different messaging patterns.</p> <p></p>"},{"location":"architecture/architecture/#brokers","title":"Brokers","text":"<p>Brokers are the core of the Danube Messaging system, responsible for routing and distributing messages, managing client connections and subscriptions, and implementing both reliable and non-reliable dispatch strategies. They act as the main entry point for publishers and subscribers, ensuring efficient and effective message flow.</p> <p>The Producers and Consumers connect to the Brokers to publish and consume messages, and use the subscription and dispatch mechanisms to accommodate various messaging patterns and reliability requirements.</p>"},{"location":"architecture/architecture/#metadata-storage","title":"Metadata Storage","text":"<p>The ETCD cluster serves as the metadata storage for the system by maintaining configuration data, topic information, and broker coordination and load-balancing, ensuring the entire system operates with high availability and consistent state management across all nodes.</p>"},{"location":"architecture/architecture/#storage-layer","title":"Storage Layer","text":"<p>The Storage Layer is where the messages are stored. It's a plugable architecture, you can choose to use immediate dispatch so the messages are not persisted and sent to the consumers immediatelly, or you can use reliable dispatch where the messages are persisted and sent to the consumers when they are available.</p> <p>The Storage Layer introduces flexibility through its pluggable architecture and storing mechanisms including Local Disk for simple persistence, or enhanced scalability and durability via GRPC storage interface that can connect to any supported object/block storage service.</p>"},{"location":"architecture/architecture/#non-reliable-dispatch","title":"Non-Reliable Dispatch","text":"<p>Non-Reliable Dispatch operates with zero storage overhead, as messages flow directly from publishers to subscribers without intermediate persistence. This mode delivers maximum performance and lowest latency, making it ideal for scenarios where occasional message loss is acceptable, such as real-time metrics or live streaming data.</p>"},{"location":"architecture/architecture/#reliable-dispatch","title":"Reliable Dispatch","text":"<p>Reliable Dispatch offers guaranteed message delivery using the storage options:</p> <ul> <li>Local Disk storage ensures message persistence on the broker's filesystem for local persistence.</li> <li>GRPC storage interface enables external storage persistence, connecting to an external GRPC server that can be implemented for AWS S3, GCP Storage, or other distributed object / block storage systems like MINIO, ROOK, offering unlimited scalability and enterprise-grade reliability.</li> <li>InMemory storage provides quick access and is only recommended for development or testing environments;</li> </ul> <p>The ability to choose between these dispatch modes gives users the flexibility to optimize their messaging infrastructure based on their specific requirements for performance, reliability, and resource utilization.</p>"},{"location":"architecture/architecture/#design-considerations","title":"Design Considerations","text":""},{"location":"architecture/architecture/#decoupled-architecture","title":"Decoupled Architecture","text":"<p>The Danube Messaging system features a decoupled architecture where components are loosely coupled, allowing for independent scaling, easy maintenance and upgrades, and failure isolation.</p>"},{"location":"architecture/architecture/#plugin-architecture","title":"Plugin Architecture","text":"<p>With a plugin architecture, the system supports flexible storage backend options, making it easy to extend and customize according to different use cases. This adaptability ensures that the system can meet diverse application requirements and is cloud-native ready.</p>"},{"location":"architecture/architecture/#event-driven-focus","title":"Event-Driven Focus","text":"<p>Optimized for event-driven systems, the Danube Messaging system supports various message delivery patterns and scalable message processing. Its design is well-suited for microservices, providing efficient and scalable handling of event-driven workloads.</p>"},{"location":"architecture/architecture/#danube-platform-capabilities-matrix","title":"Danube Platform capabilities matrix","text":"Dispatch Topics Subscription Message Persistence Ordering Guarantee Delivery Guarantee Non-Reliable Non-partitioned Topic Exclusive No Yes At-Most-Once Shared No No At-Most-Once Partitioned Topic Exclusive No Per partition At-Most-Once Shared No No At-Most-Once ---------------- ------------------- -------------- ---------------------- -------------------- -------------------- Reliable Non-partitioned Topic Exclusive Yes Yes At-Least-Once Shared Yes No At-Least-Once Partitioned Topic Exclusive Yes Per partition At-Least-Once Shared Yes No At-Least-Once"},{"location":"architecture/dispatch_strategy/","title":"Dispatch Strategy","text":"<p>The dispatch strategies in Danube represent two distinct approaches to message delivery, each serving different use cases:</p>"},{"location":"architecture/dispatch_strategy/#non-reliable-dispatch-strategy","title":"Non-Reliable Dispatch Strategy","text":"<p>This strategy prioritizes speed and minimal resource usage by delivering messages directly from producers to subscribers without storing them. Messages flow through the broker in a \"fire and forget\" manner, achieving the lowest possible latency. It's particularly effective for use cases like real-time metrics, live data streams, or scenarios where occasional message loss is acceptable in favor of maximum throughput and performance.</p>"},{"location":"architecture/dispatch_strategy/#reliable-dispatch-strategy","title":"Reliable Dispatch Strategy","text":"<p>This strategy ensures guaranteed message delivery by implementing a store-and-forward mechanism. When a message arrives, it's first stored in the chosen storage backend before being dispatched to subscribers. The storage options include:</p> <ul> <li>Local Disk: Messages are persisted to the broker's filesystem, providing durability with lightweight operation</li> <li>GRPC Storage Interface: Connects to external storage systems using danube-storage components. It can be implemented for cloud storage as AWS S3, GCP Storage or Azure Blob Storage, also for distributed object / block storage systems like MINIO,ROOK, LONGHORN etc.</li> <li>InMemory: Messages are kept in RAM for quick access, only for development &amp; teting environments</li> </ul> <p>The Reliable Dispatch maintains message state and tracks delivery acknowledgments, ensuring messages are not lost even if subscribers are temporarily unavailable.</p> <p>These dispatch strategies embody Danube's design principle of flexibility, letting users choose the right balance between performance and reliability based on their specific requirements. The ability to switch between these strategies or use them simultaneously for different topics makes Danube adaptable to various messaging patterns in distributed systems.</p>"},{"location":"architecture/internal_danube_services/","title":"Danube Cluster Services Role","text":"<p>This document enumerates the principal internal components of the Danube Broker.</p>"},{"location":"architecture/internal_danube_services/#danube-service-components","title":"Danube Service Components","text":"<p>The Broker owns the topics and manages their lifecycle. It also facilitates the creation of producers, subscriptions, and consumers, ensuring that producers can publish messages to topics and consumers can consume messages from topics.</p>"},{"location":"architecture/internal_danube_services/#leader-election-service","title":"Leader Election Service","text":"<p>The Leader Election Service selects one broker from the cluster to act as the Leader. The Broker Leader is responsible for making decisions. This service is used by the Load Manager, ensuring only one broker in the cluster posts the cluster aggregated Load Report.</p> <p>Leader Election Flow:</p> <ul> <li>The first broker registered in the cluster becomes the Leader by registering itself on \"/cluster/leader\".</li> <li>The field is registered with a lease, so the leader broker must periodically renew its lease to maintain leadership.</li> <li>Subsequent brokers attempt to become leaders but become Followers if the path is already in use.</li> <li>All brokers periodically check the leader path. If there is no change, the state is maintained; otherwise, brokers attempt to become the leader.</li> </ul>"},{"location":"architecture/internal_danube_services/#load-manager-service","title":"Load Manager Service","text":"<p>The Load Manager monitors and distributes load across brokers by managing topic and partition assignments. It implements rebalancing logic to redistribute topics/partitions when brokers join or leave the cluster and is responsible for failover mechanisms to handle broker failures.</p> <p>Load Manager Flow:</p> <ul> <li>All brokers periodically post their Load Reports on the path \"/cluster/brokers/load/{broker-id}\".</li> <li>The leader broker watches for load reports from all brokers in the cluster.</li> <li>It calculates rankings using the selected Load Balance algorithm.</li> <li>It posts its calculations for the cluster on the \"/cluster/load_balance\" path.</li> </ul> <p>Creation of a New Topic:</p> <ul> <li>A broker registers the Topic on the \"/cluster/unassigned\" path.</li> <li>The Load Manager of the leader Broker watches this path and assigns the broker with the least load to host the new topic by posting the topic to the \"/cluster/brokers/{broker-id}/{topic_name}\" path.</li> <li>Each broker watches its own path: \"/cluster/brokers/{broker-id}\". For any event on that path, such as the addition or deletion of topics, it acts accordingly by creating a new topic locally or deleting the topic it owned and all related resources.</li> <li>On topic creation, the broker checks if the topic already exists locally. If not, it retrieves all data about the topic, including subscriptions and producers, from the Local Metadata Cache.</li> <li>On topic removal, the broker handles the disconnections of producers and consumers and removes the locally allocated resources.</li> </ul> <p>For further consideration: We may want the broker to ask the Load Manager to get the next broker and initiate topic creation. Either it just posts the topic on the \"/cluster/unassigned\" path, or if it is the selected broker, it also creates the topic locally.</p>"},{"location":"architecture/internal_danube_services/#local-metadata-cache","title":"Local Metadata Cache","text":"<p>This cache stores various types of metadata required by Danube brokers, such as topic and namespace data, which are frequently accessed during message production and consumption. This reduces the need for frequent queries to the central metadata store, ETCD.</p> <p>The docs/internal_resources.md document describes how the resources are organized in the Metadata Store.</p> <p>Updates/events are received via ETCD Watch events and/or the metadata event synchronizer.</p>"},{"location":"architecture/internal_danube_services/#syncronizer","title":"Syncronizer","text":"<p>Not yet implemented.</p> <p>The synchronizer ensures that metadata and configuration settings across different brokers remain consistent. It propagates changes to metadata and configuration settings using client Producers and Consumers.</p> <p>This is in addition to Metadata Storage watch events, allowing brokers to process metadata updates even if there was a communication glitch or the broker was unavailable for a short period, potentially missing the Store Watch events. The synchronizer allows for dynamic updates to configuration settings without requiring a broker service restart.</p>"},{"location":"architecture/messages/","title":"Message Structure in Danube","text":"<p>A message in Danube represents the fundamental unit of data transmission between producers and consumers. Each message contains both the payload and associated metadata for proper routing and processing.</p>"},{"location":"architecture/messages/#streammessage-structure","title":"StreamMessage Structure","text":""},{"location":"architecture/messages/#core-fields","title":"Core Fields","text":"<ul> <li>request_id (u64): Unique identifier for tracking the message request</li> <li>msg_id (MessageID): Complex identifier containing routing and location information</li> <li>payload (Vec): The actual message content</li> <li>publish_time (u64): Timestamp when the message was published</li> <li>producer_name (String): Name of the producer that sent the message</li> <li>subscription_name (String): Name of the subscription for consumer acknowledgment routing</li> <li>attributes (HashMap): User-defined key-value pairs for custom metadata"},{"location":"architecture/messages/#messageid-fields","title":"MessageID Fields","text":"<ul> <li>producer_id (u64): Unique identifier for the producer within a topic</li> <li>topic_name (String): Name of the topic the message belongs to</li> <li>broker_addr (String): Address of the broker handling the message</li> <li>segment_id (u64): Unique identifier for the topic segment</li> <li>segment_offset (u64): Message position within the segment</li> </ul>"},{"location":"architecture/messages/#usage","title":"Usage","text":""},{"location":"architecture/messages/#producer-perspective","title":"Producer Perspective","text":"<p>Producers create messages by setting the payload and optional attributes. The system automatically generates and manages other fields like request_id, msg_id, and publish_time.</p>"},{"location":"architecture/messages/#consumer-perspective","title":"Consumer Perspective","text":"<p>Consumers receive the complete StreamMessage structure, providing access to both the message payload and all associated metadata for processing and acknowledgment handling.</p>"},{"location":"architecture/messages/#message-routing","title":"Message Routing","text":"<p>The MessageID structure enables efficient message routing and acknowledgment handling across the Danube messaging system, ensuring messages reach their intended destinations and can be properly tracked.</p>"},{"location":"architecture/subscriptions/","title":"Subscription","text":"<p>A Danube subscription is a named configuration rule that determines how messages are delivered to consumers. It is a lease on a topic established by a group of consumers.</p> <p>The Danube permits multiple producers and subcribers to the same topic. The Subscription Types can be combined to obtain message queueing or fan-out pub-sub messaging patterns.</p> <p></p>"},{"location":"architecture/subscriptions/#exclusive","title":"Exclusive","text":"<p>The Exclusive type is a subscription type that only allows a single consumer to attach to the subscription. If multiple consumers subscribe to a topic using the same subscription, an error occurs. This consumer has exclusive access to all messages published to the topic or partition.</p>"},{"location":"architecture/subscriptions/#exclusive-subscription-on-non-partitioned-topic","title":"Exclusive subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumer</code>: Only one consumer can be attached to the topic with an Exclusive subscription.</li> <li><code>Message Handling</code>: The single consumer handles all messages from the topic, receiving every message published to that topic.</li> </ul>"},{"location":"architecture/subscriptions/#exclusive-subscription-on-partitioned-topic-multiple-partitions","title":"Exclusive subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumer</code>: One consumer is allowed to connect to the subscription across all partitions of the partitioned topic.</li> <li><code>Message Handling</code> : This single consumer processes messages from all partitions of the partitioned topic. If a topic is partitioned into multiple partitions, the exclusive consumer handles messages from every partition.</li> </ul>"},{"location":"architecture/subscriptions/#shared","title":"Shared","text":"<p>The Shared subscription type Danube allows multiple consumers to attach to the same subscription. Messages are delivered in a round-robin distribution across consumers, and any given message is delivered to only one consumer.</p>"},{"location":"architecture/subscriptions/#shared-subscription-on-non-partitioned-topic","title":"Shared subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumers</code>: Multiple consumers can subscribe to the same topic.</li> <li><code>Message Handling</code>: Messages are distributed among all consumers in a round-robin fashion.</li> </ul>"},{"location":"architecture/subscriptions/#shared-subscription-on-partitioned-topic-multiple-partitions","title":"Shared subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumers</code>: Multiple consumers can subscribe to the partitioned topic.</li> <li><code>Message Handling</code>: Messages are distributed across all partitions, and then among consumers in a round-robin fashion. Each message from any partition is delivered to only one consumer.</li> </ul>"},{"location":"architecture/subscriptions/#failover","title":"FailOver","text":"<p>The FailOver subscription type allows only one consumer (the active consumer) to receive messages at any given time. An active consumer is picked for a non-partitioned topic or for each partition of a partitioned topic and receives messages.</p>"},{"location":"architecture/subscriptions/#failover-subscription-on-non-partitioned-topic","title":"FailOver subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumers</code>: Only one active consumer processes messages at a time.</li> <li><code>Message Handling</code>: If the active consumer fails, a standby consumer will take over.</li> </ul>"},{"location":"architecture/subscriptions/#failover-subscription-on-partitioned-topic-multiple-partitions","title":"FailOver subscription on  Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumers</code>: One active consumer processes messages for each partition.</li> <li><code>Message Handling</code>: If the active consumer for a partition fails, a standby consumer takes over.</li> </ul>"},{"location":"architecture/topics/","title":"Topic","text":"<p>A topic is a unit of storage that organizes messages into a stream. As in other messaging systems, topics are named channels for transmitting messages from producers to consumers. Topic names are URLs that have a well-defined structure:</p>"},{"location":"architecture/topics/#namespacetopic_name","title":"/{namespace}/{topic_name}","text":"<p>Example: /default/markets (where default is the namespace and markets the topic)</p>"},{"location":"architecture/topics/#partitioned-topics","title":"Partitioned Topics","text":"<p>Danube support both partitioned and non-partitioned topics. The non-partitioned topics are served by a single broker, while the partitioned topic has partitiones that are served by multiple brokers within the cluster, thus allowing for higher throughput.</p> <p>A partitioned topic is implemented as N internal topics, where N is the number of partitions. When publishing messages to a partitioned topic, each message is routed to one of several brokers. The distribution of partitions across brokers is handled automatically.</p> <p></p> <p>Messages for the topic are broadcast to two consumers. The routing mode determines each message should be published to which partition, while the subscription type determines which messages go to which consumers.</p>"},{"location":"architecture/topics/#benefits-of-the-partitioned-topics","title":"Benefits of the Partitioned topics","text":"<ul> <li><code>Scalability</code>: Partitioned topics enable horizontal scaling by distributing the load across multiple partitions. This is essential for high-throughput systems that need to handle large volumes of data efficiently.</li> <li><code>Parallel Processing</code>: It allows multiple consumers to process different partitions of the same topic concurrently, improving throughput and processing efficiency.</li> <li><code>Data Locality</code>: Partitioning can help in maintaining data locality and reducing processing latency, as consumers handle a specific subset of the data (key-shared distribution not yet supported).</li> </ul>"},{"location":"architecture/topics/#creation-of-partitioned-topics","title":"Creation of Partitioned Topics","text":"<p>Partitioned topics are created with a predefined number of partitions. When you create a partitioned topic, you specify the number of partitions it should have. This number remains fixed for the lifetime of the topic, although you can configure this number at topic creation time.</p>"},{"location":"architecture/topics/#producers","title":"Producers","text":"<p>The producers routing mechanism determine which messages go to which partition.</p>"},{"location":"architecture/topics/#routing-modes","title":"Routing modes","text":"<p>When publishing to partitioned topics, you must specify a routing mode. The routing mode determines each message should be published to which partition or which internal topic.</p> <ul> <li>RoundRobinPartition - The producer will publish messages across all partitions in round-robin fashion to achieve maximum throughput. If a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> <li>SinglePartition - If no key is provided, the producer will randomly pick one single partition and publish all the messages into that partition. While if a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> </ul>"},{"location":"architecture/topics/#consumers-subscriptions","title":"Consumers (subscriptions)","text":"<p>The subscription type determines which messages go to which consumers.</p> <p>Check the Subscription documentation for details on how messages are distributed to consumers based on the subscription type.</p>"},{"location":"client_libraries/clients/","title":"Danube client library","text":"<p>Currently, the supported clients are the Rust Client and Go Client clients. However, the community is encouraged to contribute by developing clients in other programming languages.</p>"},{"location":"client_libraries/clients/#rust-client","title":"Rust client","text":"<p>The Rust danube-client is an asynchronous Rust client library. To start using the <code>danube-client</code> library in your Rust project, you need to add it as a dependency. You can do this by running the following command:</p> <pre><code>cargo add danube-client\n</code></pre> <p>This command will add danube-client to your <code>Cargo.toml</code> file. Once added, you can import and use the library in your Rust code to interact with the Danube Pub/Sub messaging platform.</p>"},{"location":"client_libraries/clients/#go-client","title":"Go client","text":"<p>To start using the danube-go library in your Go project, you need to add it as a dependency. You can do this by running the following command:</p> <pre><code>go get github.com/danube-messaging/danube-go\n</code></pre> <p>This command will fetch the <code>danube-go</code> library and add it to your <code>go.mod</code> file. Once added, you can import and use the library in your Go code to interact with the Danube Pub/Sub messaging platform.</p>"},{"location":"client_libraries/clients/#community-danube-clients","title":"Community Danube clients","text":"<p>TBD</p>"},{"location":"client_libraries/consumer/","title":"Consumer","text":"<p>A consumer is a process that attaches to a topic via a subscription and then receives messages.</p> <p>Subscription Types - describe the way the consumers receive the messages from topics</p> <ul> <li>Exclusive -  Only one consumer can subscribe, guaranteeing message order.</li> <li>Shared -  Multiple consumers can subscribe, messages are delivered round-robin, offering good scalability but no order guarantee.</li> <li>Failover - Similar to shared subscriptions, but multiple consumers can subscribe, and one actively receives messages.</li> </ul>"},{"location":"client_libraries/consumer/#example","title":"Example","text":"RustGo <pre><code>let topic = \"/default/test_topic\";\n\n  // Create the Exclusive consumer\nlet mut consumer = danube_client\n    .new_consumer()\n    .with_topic(topic.to_string())\n    .with_consumer_name(consumer_name.to_string())\n    .with_subscription(format!(\"test_subscription_{}\", consumer_name))\n    .with_subscription_type(SubType::Exclusive)\n    .build();\n\n// Subscribe to the topic\nconsumer.subscribe().await?;\n\n// Start receiving messages\nlet mut message_stream = consumer.receive().await?;\n\n if let Some(stream_message) = message_stream.recv().await {\n\n    //process the message and ack for receive\n    consumer.ack(&amp;stream_message).await?\n\n }\n</code></pre> <pre><code>ctx := context.Background()\ntopic := \"/default/topic_test\"\nconsumerName := \"consumer_test\"\nsubscriptionName := \"subscription_test\"\nsubType := danube.Exclusive\n\nconsumer, err := client.NewConsumer(ctx).\n    WithConsumerName(consumerName).\n    WithTopic(topic).\n    WithSubscription(subscriptionName).\n    WithSubscriptionType(subType).\n    Build()\nif err != nil {\n    log.Fatalf(\"Failed to initialize the consumer: %v\", err)\n}\n\nif err := consumer.Subscribe(ctx); err != nil {\n    log.Fatalf(\"Failed to subscribe: %v\", err)\n}\nlog.Printf(\"The Consumer %s was created\", consumerName)\n\n\n\n// Receiving messages\nstream, err := consumer.Receive(ctx)\nif err != nil {\n    log.Fatalf(\"Failed to receive messages: %v\", err)\n}\n\nfor msg := range stream {\n    fmt.Printf(\"Received message: %+v\\n\", string(msg.GetPayload()))\n\n    if _, err := consumer.Ack(ctx, msg); err != nil {\n        log.Fatalf(\"Failed to acknowledge message: %v\", err)\n    }\n}\n</code></pre>"},{"location":"client_libraries/consumer/#complete-example","title":"Complete example","text":"<p>For complete code examples of using producers and consumers, check the links:</p> <ul> <li>Rust Examples</li> <li>Go Examples</li> </ul>"},{"location":"client_libraries/producer/","title":"Producer","text":"<p>A producer is a process that attaches to a topic and publishes messages to a Danube broker. The Danube broker processes the messages.</p> <p>Access Mode is a mechanism to determin the permissions of producers on topics.</p> <ul> <li>Shared - Multiple producers can publish on a topic.</li> <li>Exclusive - If there is already a producer connected, other producers trying to publish on this topic get errors immediately.</li> </ul> <p>Before an application creates a producer/consumer, the  client library needs to initiate a setup phase including two steps:</p> <ul> <li>The client attempts to determine the owner of the topic by sending a Lookup request to Broker.  </li> <li>Once the client library has the broker address, it creates a RPC connection (or reuses an existing connection from the pool) and (in later stage authenticates it ).</li> <li>Within this connection, the clients (producer, consumer) and brokers exchange RPC commands. At this point, the client sends a command to create producer/consumer to the broker, which will comply after doing some validation checks.</li> </ul>"},{"location":"client_libraries/producer/#create-producer","title":"Create Producer","text":"RustGo <pre><code>let topic = \"/default/topic_test\";\nlet producer_name = \"producer_test\";\n\n// Create the producer\nlet mut producer = danube_client\n    .new_producer()\n    .with_topic(topic)\n    .with_name(producer_name)\n    .with_schema(\"my_schema\".into(), SchemaType::String)\n    .build();\n\nproducer.create().await?;\n\nproducer\n    .send(\"Hello Danube\".as_bytes().into(), None)\n    .await?;\n</code></pre> <pre><code>topic := \"/default/topic_test\"\nproducerName := \"producer_test\"\n\nproducer, err := client.NewProducer(ctx).\n    WithName(producerName).\n    WithTopic(topic).\n    WithSchema(\"test_schema\", danube.SchemaType_STRING).\n    Build()\nif err != nil {\n    log.Fatalf(\"unable to initialize the producer: %v\", err)\n}\n\nif err := producer.Create(ctx); err != nil {\n    log.Fatalf(\"Failed to create producer: %v\", err)\n}\n\npayload := fmt.Sprintf(\"Hello Danube %d\", i)\n// Convert string to bytes\nbytes_payload := []byte(payload)\n\nmessageID, err := producer.Send(ctx, bytes_payload , nil)\nif err != nil {\n    log.Fatalf(\"Failed to send message: %v\", err)\n}\n</code></pre>"},{"location":"client_libraries/producer/#create-producer-with-partitioned-topic","title":"Create Producer with partitioned topic","text":"<p>Here we create a producer with a partitioned topic. A partitioned topic is implemented as N internal topics, where N is the number of partitions. When publishing messages to a partitioned topic, each message is routed to one of several brokers. The distribution of partitions across brokers is handled automatically.</p> RustGo <pre><code>let topic = \"/default/topic_test\";\nlet producer_name = \"producer_test\";\nlet partitions = 3\n\n// Create the producer\nlet mut producer = danube_client\n    .new_producer()\n    .with_topic(topic)\n    .with_name(producer_name)\n    .with_schema(\"my_schema\".into(), SchemaType::String)\n    .with_partitions(partitions)\n    .build();\n\nproducer.create().await?;\n\nproducer\n    .send(\"Hello Danube\".as_bytes().into(), None)\n    .await?;\n</code></pre> <pre><code>topic := \"/default/topic_test\"\nproducerName := \"producer_test\"\n\nproducer, err := client.NewProducer(ctx).\n    WithName(producerName).\n    WithTopic(topic).\n    WithSchema(\"test_schema\", danube.SchemaType_STRING).\n    WithPartitions(3).\n    Build()\nif err != nil {\n    log.Fatalf(\"unable to initialize the producer: %v\", err)\n}\n\nif err := producer.Create(ctx); err != nil {\n    log.Fatalf(\"Failed to create producer: %v\", err)\n}\n\npayload := fmt.Sprintf(\"Hello Danube %d\", i)\n// Convert string to bytes\nbytes_payload := []byte(payload)\n\nmessageID, err := producer.Send(ctx, bytes_payload , nil)\nif err != nil {\n    log.Fatalf(\"Failed to send message: %v\", err)\n}\n</code></pre>"},{"location":"client_libraries/producer/#create-producer-with-reliable-dispatch-topic","title":"Create Producer with reliable dispatch topic","text":"<p>Here we create Producer with reliable dispatch topic. This strategy ensures guaranteed message delivery by implementing a store-and-forward mechanism. When a message arrives, it's first stored in the chosen storage backend before being dispatched to subscribers.</p> RustGo <pre><code>use danube_client::{\nConfigReliableOptions, ConfigRetentionPolicy, DanubeClient, SchemaType };\n\nlet topic = \"/default/topic_test\";\nlet producer_name = \"producer_test\";\n\nlet reliable_options =\n    ConfigReliableOptions::new(5, ConfigRetentionPolicy::RetainUntilExpire, 3600);\n\n// Create the producer\nlet mut producer = danube_client\n    .new_producer()\n    .with_topic(topic)\n    .with_name(producer_name)\n    .with_schema(\"my_schema\".into(), SchemaType::String)\n    .with_reliable_dispatch(reliable_options)\n    .build();\n\nproducer.create().await?;\n\nproducer\n    .send(\"Hello Danube\".as_bytes().into(), None)\n    .await?;\n</code></pre> <pre><code>topic := \"/default/topic_test\"\nproducerName := \"producer_test\"\n\n// For reliable strategy\nreliableOpts := danube.NewReliableOptions(\n    10, // 10MB segment size\n    danube.RetainUntilExpire,\n    3600, // retention period in seconds\n    )\nreliableStrategy := danube.NewReliableDispatchStrategy(reliableOpts)\n\nproducer, err := client.NewProducer(ctx).\n    WithName(producerName).\n    WithTopic(topic).\n    WithSchema(\"test_schema\", danube.SchemaType_STRING).\n    WithDispatchStrategy(reliableStrategy).\n    Build()\nif err != nil {\n    log.Fatalf(\"unable to initialize the producer: %v\", err)\n}\n\nif err := producer.Create(ctx); err != nil {\n    log.Fatalf(\"Failed to create producer: %v\", err)\n}\n\npayload := fmt.Sprintf(\"Hello Danube %d\", i)\n// Convert string to bytes\nbytes_payload := []byte(payload)\n\nmessageID, err := producer.Send(ctx, bytes_payload , nil)\nif err != nil {\n    log.Fatalf(\"Failed to send message: %v\", err)\n}\n</code></pre>"},{"location":"client_libraries/producer/#complete-examples","title":"Complete examples","text":"<p>For complete code examples of using producers and consumers, check the links:</p> <ul> <li>Rust Examples</li> <li>Go Examples</li> </ul>"},{"location":"client_libraries/setup/","title":"Configure Danube Client","text":"<p>First you need to create the <code>DanubeClient</code>. The method <code>service_url</code> configures the base URI, that is used for connecting to the Danube Messaging System. The URI should include the protocol and address of the Danube service.</p> RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    // Setup tracing\n    tracing_subscriber::fmt::init();\n\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n}\n</code></pre> <pre><code>import \"github.com/danube-messaging/danube-go\"\n\nfunc main() {\n\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n}\n</code></pre>"},{"location":"client_libraries/setup/#configure-danube-client-with-tls","title":"Configure Danube client with TLS","text":"<p>To enable TLS for secure communication between the client and the Danube broker, you need to configure the client with the appropriate certificate.</p> RustGo <pre><code>use danube_client::DanubeClient;\nuse rustls::crypto;\nuse tokio::sync::OnceCell;\n\nstatic CRYPTO_PROVIDER: OnceCell&lt;()&gt; = OnceCell::const_new();\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    CRYPTO_PROVIDER.get_or_init(|| async {\n        let crypto_provider = crypto::ring::default_provider();\n        crypto_provider\n            .install_default()\n            .expect(\"Failed to install default CryptoProvider\");\n    })\n    .await;\n\nlet client = DanubeClient::builder()\n    .service_url(\"https://127.0.0.1:6650\")\n    .with_tls(\"../cert/ca-cert.pem\")?\n    .build()\n    .await?;\n}\n</code></pre> <pre><code>import \"github.com/danube-messaging/danube-go\"\n\nfunc main() {\n\n   //  TLS support not yet implemented\n\n}\n</code></pre>"},{"location":"client_libraries/setup/#configure-danube-client-with-tls-and-jwt-token-for-authentication","title":"Configure Danube client with TLS and JWT token for authentication","text":"<p>In addition to TLS connectivity in the below example we are using the JWT token to autheticate the requests. This token is usually obtained by logging into an application service and generating an API key, or provided by the admin of the service.</p> <p>The API key is used to request a JWT token from the authentication service. The JWT token includes claims that identify and authorize the client. Once a JWT token is obtained, the Danube client will include it in the <code>Authorization</code> header of all the next requests.</p> RustGo <pre><code>use danube_client::DanubeClient;\nuse rustls::crypto;\nuse tokio::sync::OnceCell;\n\n\nstatic CRYPTO_PROVIDER: OnceCell&lt;()&gt; = OnceCell::const_new();\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    CRYPTO_PROVIDER.get_or_init(|| async {\n        let crypto_provider = crypto::ring::default_provider();\n        crypto_provider\n            .install_default()\n            .expect(\"Failed to install default CryptoProvider\");\n    })\n    .await;\n\nlet client = DanubeClient::builder()\n    .service_url(\"https://127.0.0.1:6650\")\n    .with_tls(\"../cert/ca-cert.pem\")?\n    .with_api_key(\"provided_api_key\".to_string())\n    .build()\n    .await?;\n}\n</code></pre> <pre><code>import \"github.com/danube-messaging/danube-go\"\n\nfunc main() {\n\n   //  TLS support not yet implemented\n\n}\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/","title":"danube-admin: Brokers Commands","text":"<p>The <code>danube-admin-cli</code> tool provides commands to manage and view information about brokers in your Danube cluster. Below is the documentation for the commands related to brokers.</p>"},{"location":"danube_clis/danube_admin/brokers/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/brokers/#danube-admin-cli-brokers-list","title":"<code>danube-admin-cli brokers list</code>","text":"<p>List all active brokers in the cluster.</p> <p>Usage:</p> <pre><code>danube-admin-cli brokers list\n</code></pre> <p>Description:</p> <p>This command retrieves and displays a list of all active brokers in the cluster. The output is formatted into a table with the following columns:</p> <ul> <li>BROKER ID: The unique identifier for the broker.</li> <li>BROKER ADDRESS: The network address of the broker.</li> <li>BROKER ROLE: The role assigned to the broker (e.g., \"leader\", \"follower\").</li> </ul> <p>Example Output:</p> <pre><code>+------------+---------------------+-------------+\n| BROKER ID  | BROKER ADDRESS      | BROKER ROLE |\n+------------+---------------------+-------------+\n| 1          | 192.168.1.1:6650    | leader      |\n| 2          | 192.168.1.2:6650    | follower    |\n+------------+---------------------+-------------+\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#danube-admin-cli-brokers-leader-broker","title":"<code>danube-admin-cli brokers leader-broker</code>","text":"<p>Get information about the leader broker in the cluster.</p> <p>Usage:</p> <pre><code>danube-admin-cli brokers leader-broker\n</code></pre> <p>Description:</p> <p>This command fetches and displays the details of the current leader broker in the cluster. The information includes the broker ID, address, and role of the leader.</p> <p>Example Output:</p> <pre><code>Leader Broker: BrokerId: 1, Address: 192.168.1.1:6650, Role: leader\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#danube-admin-cli-brokers-namespaces","title":"<code>danube-admin-cli brokers namespaces</code>","text":"<p>List all namespaces in the cluster.</p> <p>Usage:</p> <pre><code>danube-admin-cli brokers namespaces\n</code></pre> <p>Description:</p> <p>This command retrieves and lists all namespaces associated with the cluster. Each namespace is printed on a new line.</p> <p>Example Output:</p> <pre><code>Namespace: default\nNamespace: public\nNamespace: my-namespace\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Make sure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"danube_clis/danube_admin/brokers/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List all brokers:</li> </ul> <pre><code>danube-admin-cli brokers list\n</code></pre> <ul> <li>Get the leader broker:</li> </ul> <pre><code>danube-admin-cli brokers leader-broker\n</code></pre> <ul> <li>List all namespaces:</li> </ul> <pre><code>danube-admin-cli brokers namespaces\n</code></pre> <p>For more detailed information or help with the <code>danube-admin-cli</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-admin-cli brokers --help\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/","title":"danube-admin: Namespaces Commands","text":"<p>The <code>danube-admin-cli</code> tool provides commands to manage and view information about namespaces in your Danube cluster. Below is the documentation for the commands related to namespaces.</p>"},{"location":"danube_clis/danube_admin/namespaces/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-cli-namespaces-topics-namespace","title":"<code>danube-admin-cli namespaces topics NAMESPACE</code>","text":"<p>Get the list of topics for a specified namespace.</p> <p>Usage:</p> <pre><code>danube-admin-cli namespaces topics NAMESPACE\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all topics associated with a specific namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Example Output:</p> <pre><code>Topic: topic1\nTopic: topic2\nTopic: topic3\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-cli-namespaces-policies-namespace","title":"<code>danube-admin-cli namespaces policies NAMESPACE</code>","text":"<p>Get the configuration policies for a specified namespace.</p> <p>Usage:</p> <pre><code>danube-admin-cli namespaces policies NAMESPACE\n</code></pre> <p>Description:</p> <p>This command fetches and displays the configuration policies for a specific namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Example Output:</p> <pre><code>Policy Name: policy1\nPolicy Description: Description of policy1\nPolicy Name: policy2\nPolicy Description: Description of policy2\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-cli-namespaces-create-namespace","title":"<code>danube-admin-cli namespaces create NAMESPACE</code>","text":"<p>Create a new namespace.</p> <p>Usage:</p> <pre><code>danube-admin-cli namespaces create NAMESPACE\n</code></pre> <p>Description:</p> <p>This command creates a new namespace with the specified name. Replace <code>NAMESPACE</code> with the desired name for the new namespace.</p> <p>Example Output:</p> <pre><code>Namespace Created: true\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#danube-admin-cli-namespaces-delete-namespace","title":"<code>danube-admin-cli namespaces delete NAMESPACE</code>","text":"<p>Delete a specified namespace. The namespace must be empty.</p> <p>Usage:</p> <pre><code>danube-admin-cli namespaces delete NAMESPACE\n</code></pre> <p>Description:</p> <p>This command deletes a namespace. The specified namespace must be empty before it can be deleted. Replace <code>NAMESPACE</code> with the name of the namespace you wish to delete.</p> <p>Example Output:</p> <pre><code>Namespace Deleted: true\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Make sure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"danube_clis/danube_admin/namespaces/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List all topics in a namespace:</li> </ul> <pre><code>danube-admin-cli namespaces topics my-namespace\n</code></pre> <ul> <li>Get the policies for a namespace:</li> </ul> <pre><code>danube-admin-cli namespaces policies my-namespace\n</code></pre> <ul> <li>Create a new namespace:</li> </ul> <pre><code>danube-admin-cli namespaces create my-new-namespace\n</code></pre> <ul> <li>Delete a namespace:</li> </ul> <pre><code>danube-admin-cli namespaces delete my-old-namespace\n</code></pre> <p>For more detailed information or help with the <code>danube-admin-cli</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-admin-cli namespaces --help\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/","title":"danube-admin: Topics Commands","text":"<p>The <code>danube-admin-cli</code> tool provides commands to manage and view information about topics in your Danube cluster. Below is the documentation for the commands related to topics.</p>"},{"location":"danube_clis/danube_admin/topics/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-list-namespace","title":"<code>danube-admin-cli topics list NAMESPACE</code>","text":"<p>Get the list of topics in a specified namespace.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics list NAMESPACE\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all topics within a specified namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Example Output:</p> <pre><code>Topic: topic1\nTopic: topic2\nTopic: topic3\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-create-topic","title":"<code>danube-admin-cli topics create TOPIC</code>","text":"<p>Create a non-partitioned topic.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics create TOPIC\n</code></pre> <p>Description:</p> <p>This command creates a new non-partitioned topic with the specified name. Replace <code>TOPIC</code> with the desired name for the new topic.</p> <p>Example Output:</p> <pre><code>Topic Created: true\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-delete-topic","title":"<code>danube-admin-cli topics delete TOPIC</code>","text":"<p>Delete a specified topic.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics delete TOPIC\n</code></pre> <p>Description:</p> <p>This command deletes the specified topic. Replace <code>TOPIC</code> with the name of the topic you want to delete.</p> <p>Example Output:</p> <pre><code>Topic Deleted: true\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-subscriptions-topic","title":"<code>danube-admin-cli topics subscriptions TOPIC</code>","text":"<p>Get the list of subscriptions on a specified topic.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics subscriptions TOPIC\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all subscriptions associated with a specified topic. Replace <code>TOPIC</code> with the name of the topic you want to query.</p> <p>Example Output:</p> <pre><code>Subscriptions: [subscription1, subscription2]\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#danube-admin-cli-topics-unsubscribe-subscription-subscription-topic","title":"<code>danube-admin-cli topics unsubscribe --subscription SUBSCRIPTION TOPIC</code>","text":"<p>Delete a subscription from a topic.</p> <p>Usage:</p> <pre><code>danube-admin-cli topics unsubscribe --subscription SUBSCRIPTION TOPIC\n</code></pre> <p>Description:</p> <p>This command deletes a subscription from a specified topic. Replace <code>SUBSCRIPTION</code> with the name of the subscription and <code>TOPIC</code> with the name of the topic.</p> <p>Example Output:</p> <pre><code>Unsubscribed: true\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Ensure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"danube_clis/danube_admin/topics/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List topics in a namespace:</li> </ul> <pre><code>danube-admin-cli topics list my-namespace\n</code></pre> <ul> <li>Create a topic:</li> </ul> <pre><code>danube-admin-cli topics create my-topic\n</code></pre> <ul> <li>Delete a topic:</li> </ul> <pre><code>danube-admin-cli topics delete my-topic\n</code></pre> <ul> <li>Unsubscribe from a topic:</li> </ul> <pre><code>danube-admin-cli topics unsubscribe --subscription my-subscription my-topic\n</code></pre> <ul> <li>List subscriptions for a topic:</li> </ul> <pre><code>danube-admin-cli topics subscriptions my-topic\n</code></pre> <ul> <li>Create a new subscription for a topic:</li> </ul> <pre><code>danube-admin-cli topics create-subscription --subscription my-subscription my-topic\n</code></pre> <p>For more detailed information or help with the <code>danube-admin-cli</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-admin-cli topics --help\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/","title":"Danube-Pubsub CLI - Consume messages","text":"<p>The <code>consume</code> command subscribes to a topic and receives messages with support for different subscription types, schema validation, and message attributes tracking.</p>"},{"location":"danube_clis/danube_cli/consumer/#basic-usage","title":"Basic Usage","text":"<pre><code>danube-cli consume [OPTIONS] --service-addr &lt;SERVICE_ADDR&gt; --subscription &lt;SUBSCRIPTION&gt;\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#required-arguments","title":"Required Arguments","text":"<ul> <li> <p><code>-s, --service-addr &lt;SERVICE_ADDR&gt;</code>   The service URL for the Danube broker (e.g., <code>http://127.0.0.1:6650</code>)</p> </li> <li> <p><code>-m, --subscription &lt;SUBSCRIPTION&gt;</code>   The subscription name to use for consuming messages</p> </li> </ul>"},{"location":"danube_clis/danube_cli/consumer/#optional-arguments","title":"Optional Arguments","text":"<ul> <li> <p><code>-t, --topic &lt;TOPIC&gt;</code>   Topic to consume messages from (default: /default/test_topic)</p> </li> <li> <p><code>-n, --consumer &lt;CONSUMER&gt;</code>   Consumer identifier (default: <code>consumer_pubsub</code>)</p> </li> <li> <p><code>--sub-type &lt;TYPE&gt;</code>   Subscription type: <code>exclusive</code>, <code>shared</code>, <code>fail-over</code> (default: <code>shared</code>)</p> </li> <li> <p><code>-h, --help</code>   Print help information.</p> </li> </ul>"},{"location":"danube_clis/danube_cli/consumer/#message-output-format","title":"Message Output Format","text":""},{"location":"danube_clis/danube_cli/consumer/#standard-messages","title":"Standard Messages","text":"<pre><code>Received message: \"message content\"\nSize: &lt;size&gt; bytes, Total received: &lt;total&gt; bytes\nAttributes: key1=value1, key2=value2\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#reliable-messages","title":"Reliable messages","text":"<pre><code>Received reliable message: \"message content\"\nSegment: &lt;id&gt;, Offset: &lt;offset&gt;, Size: &lt;size&gt; bytes, Total received: &lt;total&gt; bytes\nProducer: &lt;producer_id&gt;, Topic: &lt;topic_name&gt;\nAttributes: key1=value1, key2=value2\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#features","title":"Features","text":"<ul> <li>Schema Validation: Automatically validates messages against topic schema</li> <li>Large Message Handling: Messages over 1KB are displayed as [binary data]</li> <li>Message Tracking: Tracks total bytes received and message segments</li> <li>Attribute Display: Shows message attributes if present</li> <li>Multiple Schema Types: Supports <code>bytes</code>, <code>string</code>, <code>int64</code>, and <code>json</code> schemas</li> </ul>"},{"location":"danube_clis/danube_cli/consumer/#examples","title":"Examples","text":""},{"location":"danube_clis/danube_cli/consumer/#shared-subscription-default","title":"Shared Subscription (Default)","text":"<pre><code>danube-cli consume --service-addr http://localhost:6650 --subscription my_shared_subscription\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#exclusive-subscription","title":"Exclusive Subscription","text":"<pre><code>danube-cli consume -s http://localhost:6650 -m my_exclusive --sub-type exclusive\n</code></pre> <p>To create a new exclusive subscription on the same topic:</p> <pre><code>danube-cli consume -s http://127.0.0.1:6650 -m my_exclusive2 --sub-type exclusive\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#custom-consumer-name","title":"Custom Consumer Name","text":"<pre><code>danube-cli consume -s http://localhost:6650 -n my_consumer -m my_subscription\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#specific-topic","title":"Specific Topic","text":"<pre><code>danube-cli consume -s http://localhost:6650 -t my_topic -m my_subscription\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#notes","title":"Notes","text":"<ul> <li>Messages are automatically acknowledged after processing</li> <li>JSON messages are pretty-printed and validated against schema if available</li> <li>The consumer maintains message ordering for reliable delivery</li> <li>Connection errors and message processing failures are reported to stderr</li> </ul>"},{"location":"danube_clis/danube_cli/producer/","title":"Danube-Pubsub CLI - Produce messages","text":"<p>The <code>produce</code> command sends messages to a specified topic with support for reliable delivery, custom schemas, and message attributes.</p>"},{"location":"danube_clis/danube_cli/producer/#basic-usage","title":"Basic Usage","text":"<pre><code>danube-cli produce [OPTIONS] --service-addr &lt;SERVICE_ADDR&gt; --message &lt;MESSAGE&gt;\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#required-arguments","title":"Required Arguments","text":"<ul> <li> <p><code>-s, --service-addr &lt;SERVICE_ADDR&gt;</code>   The service URL for the Danube broker (e.g., <code>http://127.0.0.1:6650</code>)</p> </li> <li> <p><code>-m, --message &lt;MESSAGE&gt;</code>   The message content to send</p> </li> <li> <p><code>-f, --file &lt;FILE_PATH&gt;</code>   Binary file path to send (takes precedence over --message when specified)</p> </li> </ul>"},{"location":"danube_clis/danube_cli/producer/#basic-options","title":"Basic Options","text":"<ul> <li> <p><code>-n, --producer-name &lt;PRODUCER_NAME&gt;</code>   Producer identifier (default: <code>test_producer</code>)</p> </li> <li> <p><code>-t, --topic &lt;TOPIC&gt;</code>   Destination topic (default: <code>/default/test_topic</code>)</p> </li> <li> <p><code>-p, --partitions &lt;NUMBER&gt;</code>   Number of topic partitions</p> </li> </ul>"},{"location":"danube_clis/danube_cli/producer/#message-configuration","title":"Message Configuration","text":"<ul> <li> <p><code>-y, --schema &lt;SCHEMA&gt;</code>   Message schema type: <code>bytes</code>, <code>string</code>, <code>int64</code>, <code>json</code> (default: <code>string</code>)</p> </li> <li> <p><code>--json-schema &lt;JSON_SCHEMA&gt;</code>   Required JSON schema definition when using <code>json</code> schema type</p> </li> <li> <p><code>-a, --attributes &lt;ATTRIBUTES&gt;</code>   Message attributes in <code>key1:value1,key2:value2</code> format</p> </li> <li> <p><code>-c, --count &lt;COUNT&gt;</code>   Number of messages to send (default: <code>1</code>)</p> </li> <li> <p><code>-i, --interval &lt;INTERVAL&gt;</code>   Delay between messages in milliseconds (default: 500, minimum: 100)</p> </li> </ul>"},{"location":"danube_clis/danube_cli/producer/#reliable-delivery-options","title":"Reliable Delivery Options","text":"<ul> <li> <p><code>--reliable</code>   Enable reliable message delivery with storage persistence</p> </li> <li> <p><code>--segment-size &lt;SIZE&gt;</code>   Segment size in MB for reliable delivery (default: 20)</p> </li> <li> <p><code>--retention &lt;POLICY&gt;</code>   Retention policy: <code>ack</code> (retain until acknowledged) or <code>expire</code> (retain until time expires) (default: expire)</p> </li> <li> <p><code>--retention-period &lt;SECONDS&gt;</code>   Retention period in seconds for reliable delivery (default: <code>3600</code>)</p> </li> <li> <p><code>-h, --help</code>   Description: Print help information</p> </li> </ul>"},{"location":"danube_clis/danube_cli/producer/#example","title":"Example","text":""},{"location":"danube_clis/danube_cli/producer/#basic-message-production","title":"Basic Message Production","text":"<pre><code>danube-cli produce --service-addr http://localhost:6650 --count 100 --message \"Hello Danube\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#json-messages-with-schema","title":"JSON Messages with Schema","text":"<pre><code>danube-cli produce -s http://localhost:6650 -c 100 \\\n  -y json \\\n  --json-schema '{\"type\": \"object\", \"properties\": {\"field1\": {\"type\": \"string\"}}}' \\\n  -m '{\"field1\":\"Hello Danube\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#reliable-message-delivery","title":"Reliable Message Delivery","text":"<pre><code>danube-cli produce -s http://localhost:6650 -m \"Hello Danube\" -c 100 \\\n  --reliable \\\n  --segment-size 10 \\\n  --retention expire \\\n  --retention-period 7200\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#binary-file-send-with-reliable-delivery","title":"Binary File send with Reliable Delivery","text":"<pre><code>danube-cli produce -s http://localhost:6650 -m \"none\" -f ./data.blob -c 100 \\\n  --reliable \\\n  --segment-size 5 \\\n  --retention expire \\\n  --retention-period 7200\n</code></pre>"},{"location":"development/dev_environment/","title":"Development Environment Setup for Danube Broker","text":"<p>This document guides you through setting up the development environment, running danube broker instances, and be able to effectively contribute to the code.</p>"},{"location":"development/dev_environment/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following installed:</p> <ul> <li> <p>Rust: Ensure you have Rust installed. You can download and install it from the Rust website.</p> </li> <li> <p>Docker: Install Docker if you haven\u2019t already. Follow the installation instructions on the Docker website.</p> </li> </ul>"},{"location":"development/dev_environment/#contributing-to-the-repository","title":"Contributing to the Repository","text":"<ol> <li> <p>Fork the Repository:</p> </li> <li> <p>Go to the Danube Broker GitHub repository.</p> </li> <li> <p>Click the \"Fork\" button on the top right corner of the page to create your own copy of the repository.</p> </li> <li> <p>Clone Your Fork:</p> </li> </ol> <p>Once you have forked the repository, clone your forked repository:</p> <pre><code>git clone https://github.com/&lt;your-username&gt;/danube.git\ncd danube\n</code></pre> <ol> <li>Add the Original Repository as a Remote (optional but recommended for keeping up-to-date):</li> </ol> <pre><code>git remote add upstream https://github.com/danube-messaging/danube.git\n</code></pre>"},{"location":"development/dev_environment/#building-the-project","title":"Building the Project","text":"<ol> <li>Build the Project:</li> </ol> <p>To build the Danube Broker:</p> <pre><code>cargo build \nor  \ncargo build --release\n</code></pre>"},{"location":"development/dev_environment/#running-etcd","title":"Running ETCD","text":"<ol> <li>Start ETCD:</li> </ol> <p>Use the Makefile to start an ETCD instance. This will run ETCD in a Docker container.</p> <pre><code>make etcd\n</code></pre> <ol> <li>Clean Up ETCD:</li> </ol> <p>To stop and remove the ETCD instance and its data:</p> <pre><code>make etcd-clean\n</code></pre>"},{"location":"development/dev_environment/#running-a-single-broker-instance","title":"Running a Single Broker Instance","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. If not, use the <code>make etcd</code> command to start it.</p> <ol> <li>Run the Broker:</li> </ol> <p>Use the following command to start a single broker instance:</p> <pre><code>RUST_LOG=danube_broker=info target/debug/danube-broker --config-file config/danube_broker.yml\n</code></pre>"},{"location":"development/dev_environment/#running-multiple-broker-instances","title":"Running Multiple Broker Instances","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. Use:</p> <pre><code>make etcd\n</code></pre> <ol> <li>Run Multiple Brokers:</li> </ol> <p>Use the following Makefile command to start multiple broker instances:</p> <pre><code>make brokers\n</code></pre> <p>This will start brokers on ports 6650, 6651, and 6652. Logs for each broker will be saved in <code>temp/</code> directory.</p> <ol> <li>Clean Up Broker Instances:</li> </ol> <p>To stop all running broker instances:</p> <pre><code>make brokers-clean\n</code></pre>"},{"location":"development/dev_environment/#reading-logs","title":"Reading Logs","text":"<p>Logs for each broker instance are stored in the <code>temp/</code> directory. You can view them using:</p> <pre><code>cat temp/broker_&lt;port&gt;.log\n</code></pre> <p>Replace <code>&lt;port&gt;</code> with the actual port number (6650, 6651, or 6652).</p>"},{"location":"development/dev_environment/#inspecting-etcd-metadata","title":"Inspecting ETCD Metadata","text":"<ol> <li>Set Up <code>etcdctl</code>:</li> </ol> <p>Export the following environment variables:</p> <pre><code>export ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=http://localhost:2379\n</code></pre> <ol> <li>Inspect Metadata:</li> </ol> <p>Use <code>etcdctl</code> commands to inspect metadata. For example, to list all keys:</p> <pre><code>etcdctl get \"\" --prefix\n</code></pre> <p>To get a specific key:</p> <pre><code>etcdctl get &lt;key&gt;\n</code></pre>"},{"location":"development/dev_environment/#makefile-targets-summary","title":"Makefile Targets Summary","text":"<ul> <li><code>make etcd</code>: Starts an ETCD instance in Docker.</li> <li><code>make etcd-clean</code>: Stops and removes the ETCD instance and its data.</li> <li><code>make brokers</code>: Builds and starts broker instances on predefined ports.</li> <li><code>make brokers-clean</code>: Stops and removes all running broker instances.</li> </ul>"},{"location":"development/dev_environment/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>ETCD Not Starting: Check Docker logs and ensure no other service is using port 2379.</li> <li>Broker Not Starting: Ensure ETCD is running and accessible at the specified address and port.</li> </ul>"},{"location":"development/internal_resources/","title":"Resources mapping","text":"<p>This document describes how the resources are organized in the Metadata store</p>"},{"location":"development/internal_resources/#metadatastore-and-localcache","title":"MetadataStore and LocalCache","text":"<p>Basically the entire configuration and the metadata for all the cluster's objects (topics, namespaces, etc) are stored in MetadataStorage (ETCD) and in the LocalCache to ensure fast retrieval for the local broker and to reduce the number of request to the metadata database.</p> <p>The pattern:</p> <ul> <li>Put / Delete requests should use MetadataStore (ETCD), to ensure consistency across cluster</li> <li>Get requests should be served from the Local Cache</li> </ul> <p>The LocalCache continuously update from 2 sources for increase consistency:</p> <ul> <li>the Watch operation on ETCD</li> <li>the Syncronizer topic, where all Put / Delete requests are published and read by the brokers.</li> </ul>"},{"location":"development/internal_resources/#resources-types","title":"Resources Types","text":""},{"location":"development/internal_resources/#cluster-resources","title":"Cluster Resources","text":"<p>Holds information about the cluster and the cluster's brokers. Mainly read and write by Danube Service.</p> <ul> <li>/cluster/cluster-name</li> <li>holds a String with the name of the cluster</li> <li>/cluster/register/{broker-id}</li> <li>the broker register once it join the cluster, contain the broker metadata (broker id &amp; socket addr)  </li> <li>/cluster/brokers/{broker-id}/{namespace}/{topic}</li> <li>topics served by the broker, with value ()</li> <li>Load Manager updates the path, with topic assignments to brokers</li> <li>Brokers should watch it's own path like (/cluster/brokers/1122334455) - and perform the neccesary actions on adding or removing a topic</li> <li>/cluster/unassigned/{namespace}/{topic}</li> <li>New unassigned topics created by Broker</li> <li>Load Manager should watch this path, add assign the topic to a broker</li> <li>/cluster/load/{broker-id}</li> <li>broker periodically reports its load metrics on this path</li> <li>Load Manager watch this path to calculate broker load rankings for the cluster</li> <li>/cluster/load_balance</li> <li>the load_balance updated decision, posted by the Load Manager, contain a HashMap with keys the broker_id and value the list of topic_name</li> <li>/cluster/leader</li> <li>the value posted by Leader Election service, it holds broker_id of the current Leader of the CLuster</li> </ul> <p>Example:</p> <ul> <li>/cluster/brokers/1122334455/markets/trade-events - value is ()</li> <li>/cluster/brokers/1122334455/markets/trade-events - value is ()</li> </ul>"},{"location":"development/internal_resources/#namespace-resources","title":"Namespace Resources","text":"<p>Holds information about the namespace policy and the namespace's topics</p> <ul> <li>/namespaces/{namespace}/policy</li> <li>/namespaces/{namespace}/topics/{namespace}/{topic}</li> </ul> <p>Example:</p> <ul> <li>/namespaces/markets/policy - the value stores a Json like { \"retentionTimeInMinutes\": 1440 }</li> <li>/namespaces/markets/topics/markets/trade-events - topics part of the namespace, value is ()</li> </ul>"},{"location":"development/internal_resources/#topic-resources","title":"Topic Resources","text":"<p>Holds information about the topic policy and the associated producers / subscriptions, including partitioned topic.</p> <ul> <li>/topics/{namespace}/{topic}/policy</li> <li>holds the topic policy, the value stores a Json</li> <li>/topics/{namespace}/{topic}/schema</li> <li>holds the topic schema, the value stores the schema</li> <li>/topics/{namespace}/{topic}/producers/{producer_id}</li> <li>holds the producer config</li> <li>/topics/{namespace}/{topic}/subscriptions/{subscription_name}</li> <li>holds the subscription config</li> </ul> <p>Example:</p> <ul> <li>/topics/markets/trade-events/producers/1122334455 - with value Producer Metadata</li> <li>/topics/markets/trade-events/subscriptions/my_subscription - with value Subscription Metadata</li> <li>/topics/markets/trade-events-part-1/policy - where /markets/trade-events-part-1 is the partitioned topic that stores partition policy</li> </ul>"},{"location":"development/internal_resources/#subscriptions-resources","title":"Subscriptions Resources","text":"<p>Holds information about the topic subscriptions, including associated consumers</p> <ul> <li>/subscriptions/{subscription_name}/{consumer_id}</li> <li>holds the consumer metadata</li> </ul> <p>Example:</p> <ul> <li>/subscriptions/my_subscription/23232323</li> </ul>"},{"location":"getting_started/Danube_docker_compose/","title":"Run Danube with docker-compose and MINIO as persistence storage","text":"<p>This guide provides instructions on how to run Danube Messaging using Docker and Docker Compose. It sets up ETCD for metadata storage and MinIO for topic persistence storage.</p> <p>The MINIO storage is used as an example but can be used with any implemented storage layers.</p> <p>The Danube supports two dispatch strategies:</p> <ul> <li>Non-reliable dispatch: This strategy prioritizes speed and minimal resource usage by delivering messages directly from producers to subscribers without storing them. Messages flow through the broker in a \"fire and forget\" manner, achieving the lowest possible latency.</li> <li>Reliable dispatch: This strategy prioritizes message delivery and reliability, by implementing a store-and-forward mechanism.</li> </ul> <p>This guide focus on running Danube with a reliable dispatch strategy and MINIO as persistence storage.</p>"},{"location":"getting_started/Danube_docker_compose/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed on your system:</p> <ul> <li>Docker</li> <li>Docker Compose (version v2.32.0 or higher)</li> <li>The files mentioned below docker-compose.yml and danube_broker.yml are in this repository.</li> </ul>"},{"location":"getting_started/Danube_docker_compose/#docker-compose-architecture","title":"Docker Compose architecture","text":""},{"location":"getting_started/Danube_docker_compose/#docker-compose-configuration","title":"Docker Compose Configuration","text":""},{"location":"getting_started/Danube_docker_compose/#key-components","title":"Key Components","text":"<ul> <li>ETCD (etcd): Stores Danube's metadata.</li> <li>Danube Brokers (broker1, broker2): Handles message routing.</li> <li>MinIO (minio): Provides object storage for persistent topics.</li> <li>Danube MinIO Storage (danube-minio-storage): Bridges Danube and MinIO.</li> </ul> <p>The docker-compose.yml file defines the following services:</p>"},{"location":"getting_started/Danube_docker_compose/#etcd-metadata-storage","title":"ETCD (Metadata Storage)","text":"<p>ETCD is used for storing metadata.</p> <pre><code>  etcd:\n    image: quay.io/coreos/etcd:latest\n    container_name: etcd-danube\n    environment:\n      ETCDCTL_API: 3\n      ETCD_ADVERTISE_CLIENT_URLS: \"http://etcd:2379\"\n      ETCD_LISTEN_CLIENT_URLS: \"http://0.0.0.0:2379\"\n      ETCD_INITIAL_CLUSTER: \"etcd-danube=http://etcd-danube:2380\"\n      ETCD_NAME: \"etcd-danube\"\n      ETCD_LISTEN_PEER_URLS: \"http://0.0.0.0:2380\"\n      ETCD_INITIAL_ADVERTISE_PEER_URLS: \"http://etcd-danube:2380\"\n    ports:\n      - \"2379:2379\"\n      - \"2380:2380\"\n    healthcheck:\n      test: [\"CMD\", \"etcdctl\", \"--endpoints=http://127.0.0.1:2379\", \"endpoint\", \"health\"]\n      interval: 2s\n      timeout: 5s\n      retries: 10\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#danube-brokers-message-brokers","title":"Danube Brokers (Message Brokers)","text":"<p>Two brokers are defined (broker1 and broker2) that handle messaging. As the prerequisites, the etcd service must be healthy. Ensure that your <code>danube_broker.yml</code> configuration file is correctly set up.</p> <pre><code>  broker1:\n    image: ghcr.io/danube-messaging/danube-broker:latest\n    container_name: broker1\n    depends_on:\n      etcd:\n        condition: service_healthy\n    volumes:\n      - ./danube_broker.yml:/etc/danube_broker.yml:ro\n    environment:\n      RUST_LOG: danube_broker=info\n    command: [\n      \"--config-file\", \"/etc/danube_broker.yml\",\n      \"--broker-addr\", \"0.0.0.0:6650\",\n      \"--admin-addr\", \"0.0.0.0:50051\",\n      \"--prom-exporter\", \"0.0.0.0:3000\"\n    ]\n    ports:\n      - \"6650:6650\"\n      - \"50051:50051\"\n      - \"3000:3000\"\n</code></pre> <p>The second broker (broker2) is configured similarly but runs on different ports.</p>"},{"location":"getting_started/Danube_docker_compose/#minio-persistent-storage","title":"MinIO (Persistent Storage)","text":"<p>MinIO is used for message persistence. Danube-storage implements the storage grpc service for Danube and connects to MinIO.</p> <pre><code>  minio:\n    image: minio/minio\n    container_name: minio\n    environment:\n      MINIO_ROOT_USER: minioadmin\n      MINIO_ROOT_PASSWORD: minioadmin\n    command: server /data --console-address \":9001\"\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    volumes:\n      - minio_data:/data\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\n      interval: 30s\n      timeout: 20s\n      retries: 3\n\n  danube-minio-storage:\n    build:\n      context: ../.\n      dockerfile: Dockerfile\n    container_name: danube-minio-storage\n    environment:\n      STORAGE_TYPE: minio\n      GRPC_PORT: 50060\n      MINIO_ENDPOINT: minio:9000\n      MINIO_ACCESS_KEY_ID: minioadmin\n      MINIO_SECRET_ACCESS_KEY: minioadmin\n      MINIO_BUCKET_NAME: danube-messages\n      MINIO_LOCATION: us-east-1\n    depends_on:\n      minio:\n        condition: service_healthy\n    ports:\n      - \"50060:50060\"\n\nvolumes:\n  minio_data:\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#running-danube-messaging","title":"Running Danube Messaging","text":"<ol> <li> <p>Start the services using Docker Compose:</p> <pre><code>docker-compose up --build\n\n\u2714 danube-minio-storage                Built  \n\u2714 Network test_minio_storage_default  Created \n\u2714 Container etcd-danube               Created \n\u2714 Container minio                     Created \n\u2714 Container broker2                   Created \n\u2714 Container broker1                   Created  \n\u2714 Container danube-minio-storage      Created \n</code></pre> <p>This command will download the necessary images and start the containers. Docker Compose will ensure that ETCD and MinIO are healthy before starting the Danube brokers and storage component.</p> </li> <li> <p>Verify running containers:</p> <pre><code>docker ps\n</code></pre> <p>This command will show you the running containers and their status.</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                                           COMMAND                  CREATED          STATUS                    PORTS                                                                                                                                           NAMES\nd791e51e7a29   test_minio_storage-danube-minio-storage         \"./danube-minio-stor\u2026\"   35 seconds ago   Up 4 seconds              50051/tcp, 0.0.0.0:50060-&gt;50060/tcp, :::50060-&gt;50060/tcp                                                                                       danube-minio-storage\n\nef4fe0528a57   ghcr.io/danube-messaging/danube-broker:latest   \"/usr/local/bin/danu\u2026\"   35 seconds ago   Up 32 seconds             0.0.0.0:3000-&gt;3000/tcp, :::3000-&gt;3000/tcp, 0.0.0.0:6650-&gt;6650/tcp, :::6650-&gt;6650/tcp, 0.0.0.0:50051-&gt;50051/tcp, :::50051-&gt;50051/tcp, 6651/tcp   broker1\n\nca1a41d1ef62   ghcr.io/danube-messaging/danube-broker:latest   \"/usr/local/bin/danu\u2026\"   35 seconds ago   Up 32 seconds             0.0.0.0:3001-&gt;3001/tcp, :::3001-&gt;3001/tcp, 0.0.0.0:6651-&gt;6651/tcp, :::6651-&gt;6651/tcp, 0.0.0.0:50052-&gt;50052/tcp, :::50052-&gt;50052/tcp, 6650/tcp   broker2\n\n8a8c4a2e2d03   minio/minio                                     \"/usr/bin/docker-ent\u2026\"   35 seconds ago   Up 34 seconds (healthy)   0.0.0.0:9000-9001-&gt;9000-9001/tcp, :::9000-9001-&gt;9000-9001/tcp                                                                                   minio\n\nb7cc1c6a2a03   quay.io/coreos/etcd:latest                      \"/usr/local/bin/etcd\"    35 seconds ago   Up 34 seconds (healthy)   0.0.0.0:2379-2380-&gt;2379-2380/tcp, :::2379-2380-&gt;2379-2380/tcp  \n</code></pre> </li> <li> <p>Produce messages:</p> <p>Download the Danube CLI.</p> <p>Create a 100KB blob file, in order to fill faster the 5 MB segment size.</p> <pre><code>yes 'Danube messaging platform is awesome!' | head -c 100K &gt; test.blob\n</code></pre> <p>Produce messages, (creating also a topic with reliable dispatch, meaning that the messages are stored and then delivered to the consumers).</p> <pre><code>danube-cli produce -s http://localhost:6650 -m \"none\" -f ./test.blob -c 1000 --reliable  --segment-size 5  --retention expire --retention-period 7200\n</code></pre> </li> <li> <p>Consume messages:</p> <p>Consume messages from the topic, creating an exclusive subscription.</p> <pre><code>danube-cli consume -s http://localhost:6650 -m my_exclusive --sub-type exclusive\n</code></pre> <p>The output should look like this:</p> <pre><code>Received reliable message: [binary data] \nSegment: 2, Offset: 41, Size: 102400 bytes, Total received: 9728000 bytes\nProducer: 9791760036514492028, Topic: /default/test_topic\n\nReceived reliable message: [binary data] \nSegment: 2, Offset: 42, Size: 102400 bytes, Total received: 9830400 bytes\nProducer: 9791760036514492028, Topic: /default/test_topic\n\nReceived reliable message: [binary data] \nSegment: 2, Offset: 43, Size: 102400 bytes, Total received: 9932800 bytes\nProducer: 9791760036514492028, Topic: /default/test_topic\n</code></pre> </li> </ol>"},{"location":"getting_started/Danube_docker_compose/#checking-stored-messages","title":"Checking Stored Messages","text":"<p>Open the MinIO console in your browser: <code>http://localhost:9001/buckets</code>. Log in with <code>minioadmin</code> as both the username and password. You can see the buckets and objects created by Danube.</p>"},{"location":"getting_started/Danube_docker_compose/#stopping-danube-messaging","title":"Stopping Danube Messaging","text":"<p>To stop and remove the containers:</p> <pre><code>$ docker-compose down\n\n[+] Running 6/6\n \u2714 Container danube-minio-storage      Removed         \n \u2714 Container broker1                   Removed                  \n \u2714 Container broker2                   Removed    \n \u2714 Container minio                     Removed                                    \n \u2714 Container etcd-danube               Removed  \n \u2714 Network test_minio_storage_default  Removed   \n</code></pre>"},{"location":"getting_started/Danube_kubernetes/","title":"Run Danube messaging on Kubernetes","text":"<p>This documentation covers the instalation of the Danube cluster on the kubernetes. The Helm chart deploys the Danube Cluster with ETCD as metadata storage in the same namespace.</p> <p>The documentation assumes that you have a Kubernetes cluster running and that you have installed the Helm package manager. For local testing you can use kind.</p>"},{"location":"getting_started/Danube_kubernetes/#install-the-ngnix-ingress-controller","title":"Install the Ngnix Ingress controller","text":"<p>Using the Official NGINX Ingress Helm Chart. This is required in order to route traffic to each broker service in the cluster. The Broker configuration is provisioned in the danube_helm, you can tweak the values.yaml per your needs.</p> <p>The Danube messaging has no dependency on ngnix, can work with any ingress controller of your choice.</p> <p>Install the NGINX Ingress Controller using Helm:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#run-the-nginx-ingress-controller-on-your-local-kubernetes-cluster","title":"Run the NGINX Ingress Controller on your local kubernetes cluster","text":"<p>For local testing the NGINX Ingress Controller can be exposed using a NodePort service so that the traffic from the local machine (outside the cluster) can reach the Ingress controller.</p> <pre><code>helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.service.type=NodePort\n</code></pre> <p>You can find out which port is assigned by running</p> <pre><code>kubectl get svc\n\nNAME                                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nkubernetes                                         ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                      4m17s\nnginx-ingress-ingress-nginx-controller             NodePort    10.96.245.118   &lt;none&gt;        80:30115/TCP,443:30294/TCP   2m58s\nnginx-ingress-ingress-nginx-controller-admission   ClusterIP   10.96.169.82    &lt;none&gt;        443/TCP                      2m58s\n</code></pre> <p>If ngnix is running as NodePort (usually for testing), you need local port in this case 30115, in order to provide to danube_helm installation.</p>"},{"location":"getting_started/Danube_kubernetes/#run-the-nginx-ingress-controller-in-a-remote-cluster-cloud","title":"Run the NGINX Ingress Controller in a remote cluster (cloud)","text":"<pre><code>helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.publishService.enabled=true\n</code></pre> <ul> <li>The publishService feature enables the Ingress controller to publish information about itself (such as its external IP or hostname) in a Kubernetes Service resource.</li> <li>This is particularly useful when you are running the Ingress controller in a cloud environment (like AWS, GCP, or Azure) and need it to publish its external IP address to handle incoming traffic</li> </ul>"},{"location":"getting_started/Danube_kubernetes/#install-danube-messaging-brokers","title":"Install Danube Messaging Brokers","text":"<p>First, add the repository to your Helm client:</p> <pre><code>helm repo add danube https://danube-messaging.github.io/danube_helm\nhelm repo update\n</code></pre> <p>You can install the chart with the release name <code>my-danube-cluster</code> using the below command. This will deploy the Danube Broker and an ETCD instance with the default configuration.</p>"},{"location":"getting_started/Danube_kubernetes/#running-on-the-local-kubernetes-cluster","title":"Running on the local kubernetes cluster","text":"<pre><code>helm install my-danube-cluster danube/danube-helm-chart --set broker.service.advertisedPort=30115\n</code></pre> <p>The advertisedPort is used to allow the client to reach the brokers, through the ingress NodePort.</p>"},{"location":"getting_started/Danube_kubernetes/#running-on-the-remote-cluster-cloud","title":"Running on the remote cluster (cloud)","text":"<p>The Danube cluster configuration from the values.yaml file has to be adjusted for your needs.</p> <p>You can override the default values by providing a custom <code>values.yaml</code> file:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart -f custom-values.yaml\n</code></pre> <p>Alternatively, you can specify individual values using the <code>--set</code> flag:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart --set broker.service.type=\"ClusterIP\"\n</code></pre> <p>You can further customize the installation, check the readme file. The default configuration is running 3 Danube Brokers in cluster.</p>"},{"location":"getting_started/Danube_kubernetes/#check-the-install","title":"Check the install","text":"<p>Make sure that the brokers, etcd and the ngnix ingress are running properly in the cluster.</p>"},{"location":"getting_started/Danube_kubernetes/#example-running-on-local-kubernetes-cluster","title":"Example running on local kubernetes cluster","text":"<pre><code>kubectl get all\n\nNAME                                                          READY   STATUS    RESTARTS   AGE\npod/my-danube-cluster-danube-broker1-766665d6f4-qdbf6         1/1     Running   0          12s\npod/my-danube-cluster-danube-broker2-5774ff4dd6-dvx66         1/1     Running   0          12s\npod/my-danube-cluster-danube-broker3-6db6b5fccd-dkr2k         1/1     Running   0          12s\npod/my-danube-cluster-etcd-867f5b85f8-g4m9m                   1/1     Running   0          12s\npod/nginx-ingress-ingress-nginx-controller-7bc7c7776d-wqc5g   1/1     Running   0          47m\n\nNAME                                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                       AGE\nservice/kubernetes                                         ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                       48m\nservice/my-danube-cluster-danube-broker1                   ClusterIP   10.96.40.244    &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-danube-broker2                   ClusterIP   10.96.204.21    &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-danube-broker3                   ClusterIP   10.96.46.5      &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-etcd                             ClusterIP   10.96.232.70    &lt;none&gt;        2379/TCP                      12s\nservice/nginx-ingress-ingress-nginx-controller             NodePort    10.96.245.118   &lt;none&gt;        80:30115/TCP,443:30294/TCP    47m\nservice/nginx-ingress-ingress-nginx-controller-admission   ClusterIP   10.96.169.82    &lt;none&gt;        443/TCP                       47m\n\nNAME                                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/my-danube-cluster-danube-broker1         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-danube-broker2         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-danube-broker3         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-etcd                   1/1     1            1           12s\ndeployment.apps/nginx-ingress-ingress-nginx-controller   1/1     1            1           47m\n\nNAME                                                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/my-danube-cluster-danube-broker1-766665d6f4         1         1         1       12s\nreplicaset.apps/my-danube-cluster-danube-broker2-5774ff4dd6         1         1         1       12s\nreplicaset.apps/my-danube-cluster-danube-broker3-6db6b5fccd         1         1         1       12s\nreplicaset.apps/my-danube-cluster-etcd-867f5b85f8                   1         1         1       12s\nreplicaset.apps/nginx-ingress-ingress-nginx-controller-7bc7c7776d   1         1         1       47m\n</code></pre> <p>Validate that the brokers have started correctly:</p> <pre><code>kubectl logs pod/my-danube-cluster-danube-broker1-766665d6f4-qdbf6\n\ninitializing metrics exporter\n2024-08-28T04:30:22.969462Z  INFO danube_broker: Use ETCD storage as metadata persistent store\n2024-08-28T04:30:22.969598Z  INFO danube_broker: Start the Danube Service\n2024-08-28T04:30:22.969612Z  INFO danube_broker::danube_service: Setting up the cluster MY_CLUSTER\n2024-08-28T04:30:22.971978Z  INFO danube_broker::danube_service::local_cache: Initial cache populated\n2024-08-28T04:30:22.972013Z  INFO danube_broker::danube_service: Started the Local Cache service.\n2024-08-28T04:30:22.990763Z  INFO danube_broker::danube_service::broker_register: Broker 14150019297734190044 registered in the cluster\n2024-08-28T04:30:22.991620Z  INFO danube_broker::danube_service: Namespace default already exists.\n2024-08-28T04:30:22.991926Z  INFO danube_broker::danube_service: Namespace system already exists.\n2024-08-28T04:30:22.992480Z  INFO danube_broker::danube_service: Namespace default already exists.\n2024-08-28T04:30:22.992490Z  INFO danube_broker::danube_service: cluster metadata setup completed\n2024-08-28T04:30:22.992551Z  INFO danube_broker::danube_service:  Started the Broker GRPC server\n2024-08-28T04:30:22.992563Z  INFO danube_broker::broker_server: Server is listening on address: 0.0.0.0:6650\n2024-08-28T04:30:22.992605Z  INFO danube_broker::danube_service: Started the Leader Election service\n2024-08-28T04:30:22.993050Z  INFO danube_broker::danube_service: Started the Load Manager service.\n2024-08-28T04:30:22.993143Z  INFO danube_broker::danube_service:  Started the Danube Admin GRPC server\n2024-08-28T04:30:22.993274Z  INFO danube_broker::admin: Admin is listening on address: 0.0.0.0:50051\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#setup-in-order-to-communicate-with-cluster-danube-brokers","title":"Setup in order to communicate with cluster danube brokers","text":"<p>If you would like to communicate to the messaging sytem by using the danube-cli tool, or your own danube clients running locally, you can do the following:</p> <pre><code>kubectl get nodes -o wide\nNAME                 STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION       CONTAINER-RUNTIME\nkind-control-plane   Ready    control-plane   53m   v1.30.0   172.20.0.2    &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   5.15.0-118-generic   containerd://1.7.15\n</code></pre> <p>Use the INTERNAL-IP to route the traffic to broker hosts. Add the following in the hosts file, but make sure you match the number and the name of the brokers from the helm values.yaml file.</p> <pre><code>cat /etc/hosts\n172.20.0.2 broker1.example.com broker2.example.com broker3.example.com\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#inspect-the-etcd-instance","title":"Inspect the etcd instance","text":"<p>If you want to connect from your local machine, use kubectl port-forward to forward the etcd port to your local machine:</p> <p>Port Forward etcd Service:</p> <pre><code>kubectl port-forward service/my-danube-cluster-etcd 2379:2379\n</code></pre> <p>Once port forwarding is set up, you can run etcdctl commands from your local machine:</p> <pre><code>etcdctl --endpoints=http://localhost:2379 watch --prefix /\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#cleanup","title":"Cleanup","text":"<p>To uninstall the <code>my-danube-cluster</code> release:</p> <pre><code>helm uninstall my-danube-cluster\n</code></pre> <p>This command removes all the Kubernetes components associated with the chart and deletes the release.</p>"},{"location":"getting_started/Danube_local/","title":"Run Danube Broker on your local machine","text":""},{"location":"getting_started/Danube_local/#start-metadata-storage-etcd","title":"Start Metadata Storage (ETCD)","text":"<p>Danube uses ETCD for metadata storage to provide high availability and scalability. Run ETCD using Docker:</p> <pre><code>docker run -d --name etcd-danube -p 2379:2379 quay.io/coreos/etcd:latest etcd --advertise-client-urls http://0.0.0.0:2379 --listen-client-urls http://0.0.0.0:2379\n</code></pre> <p>Verify ETCD is running:</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS          PORTS                                                 NAMES\n27792bce6077   quay.io/coreos/etcd:latest   \"etcd --advertise-cl\u2026\"   35 seconds ago   Up 34 seconds   0.0.0.0:2379-&gt;2379/tcp, :::2379-&gt;2379/tcp, 2380/tcp   etcd-danube\n</code></pre>"},{"location":"getting_started/Danube_local/#configure-and-run-danube-broker","title":"Configure and Run Danube Broker","text":""},{"location":"getting_started/Danube_local/#create-and-configure-broker-config","title":"Create and configure broker config","text":"<p>Create a local config file, use the sample config file as a reference.</p> <pre><code>touch danube_broker.yml\n</code></pre>"},{"location":"getting_started/Danube_local/#download-and-run-the-danube-broker","title":"Download and run the Danube Broker","text":"<p>Download the latest binary from the releases page.</p> <p>If you would like to run Danube Brokers in cluster, you need to upload the binary to each machine and use the same cluster configuration name.</p> <p>Run the Danube Broker:</p> <pre><code>touch broker.log\n</code></pre> <pre><code>RUST_LOG=info ./danube-broker-linux --config-file danube_broker.yml --broker-addr \"0.0.0.0:6650\" --admin-addr \"0.0.0.0:50051\" &gt; broker.log 2&gt;&amp;1 &amp;\n</code></pre> <p>Check the logs:</p> <pre><code>tail -n 100 -f broker.log\n</code></pre> <pre><code>2025-01-12T06:15:53.705416Z  INFO danube_broker: Use ETCD storage as metadata persistent store\n2025-01-12T06:15:53.705665Z  INFO danube_broker: Start the Danube Service\n2025-01-12T06:15:53.705679Z  INFO danube_broker::danube_service: Setting up the cluster MY_CLUSTER\n2025-01-12T06:15:53.707988Z  INFO danube_broker::danube_service::local_cache: Initial cache populated\n2025-01-12T06:15:53.709521Z  INFO danube_broker::danube_service: Started the Local Cache service.\n2025-01-12T06:15:53.713329Z  INFO danube_broker::danube_service::broker_register: Broker 15139934490483381581 registered in the cluster\n2025-01-12T06:15:53.714977Z  INFO danube_broker::danube_service: Namespace default already exists.\n2025-01-12T06:15:53.716405Z  INFO danube_broker::danube_service: Namespace system already exists.\n2025-01-12T06:15:53.717979Z  INFO danube_broker::danube_service: Namespace default already exists.\n2025-01-12T06:15:53.718012Z  INFO danube_broker::danube_service: cluster metadata setup completed\n2025-01-12T06:15:53.718092Z  INFO danube_broker::danube_service:  Started the Broker GRPC server\n2025-01-12T06:15:53.718116Z  INFO danube_broker::broker_server: Server is listening on address: 0.0.0.0:6650\n2025-01-12T06:15:53.718191Z  INFO danube_broker::danube_service: Started the Leader Election service\n2025-01-12T06:15:53.722454Z  INFO danube_broker::danube_service: Started the Load Manager service.\n2025-01-12T06:15:53.724727Z  INFO danube_broker::danube_service:  Started the Danube Admin GRPC server\n2025-01-12T06:15:53.724727Z  INFO danube_broker::admin: Admin is listening on address: 0.0.0.0:50051\n</code></pre>"},{"location":"getting_started/Danube_local/#use-danube-cli-to-publish-and-consume-messages","title":"Use Danube CLI to Publish and Consume Messages","text":"<p>Download the latest Danube CLI binary from the releases page and run it:</p> <pre><code>./danube-cli-linux produce -s http://127.0.0.1:6650 -t /default/demo_topic -c 1000 -m \"Hello, Danube!\"\n</code></pre> <pre><code>Message sent successfully with ID: 9\nMessage sent successfully with ID: 10\nMessage sent successfully with ID: 11\nMessage sent successfully with ID: 12\n</code></pre> <p>Open a new terminal and run the below command to consume the messages:</p> <pre><code>./danube-cli-linux consume -s http://127.0.0.1:6650 -t /default/demo_topic -m my_subscription\n</code></pre> <pre><code>Received bytes message: 9, with payload: Hello, Danube!\nReceived bytes message: 10, with payload: Hello, Danube!\nReceived bytes message: 11, with payload: Hello, Danube!\nReceived bytes message: 12, with payload: Hello, Danube!\n</code></pre>"},{"location":"getting_started/Danube_local/#validate","title":"Validate","text":"<p>Ensure ETCD is running and accessible. You can check its status by accessing <code>http://&lt;ETCD_SERVER_IP&gt;:2379</code> from a browser or using <code>curl</code>:</p> <pre><code>curl http://&lt;ETCD_SERVER_IP&gt;:2379/v3/version\n</code></pre> <p>Ensure each broker instance is running and listening on the specified port. You can check this with <code>netstat</code> or <code>ss</code>:</p> <pre><code>netstat -tuln | grep 6650\n</code></pre> <p>For debugging, check the logs of each Danube broker instance.</p>"},{"location":"getting_started/Danube_local/#cleanup","title":"Cleanup","text":"<p>Stop the Danube Broker:</p> <pre><code>pkill danube-broker\n</code></pre> <p>Stop and remove ETCD container</p> <pre><code>docker stop etcd-danube\n</code></pre> <pre><code>docker rm -f etcd-danube\n</code></pre> <p>Verify cleanup</p> <pre><code>ps aux | grep danube-broker\ndocker ps | grep etcd-danube\n</code></pre>"}]}