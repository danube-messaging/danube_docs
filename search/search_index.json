{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Danube Messaging","text":"<p>\ud83c\udf0a Danube Messaging is a lightweight, cloud\u2011native messaging platform built in Rust. It delivers sub\u2011second dispatch with cloud economics by combining a Write\u2011Ahead Log (WAL) with object storage, so you get low\u2011latency pub/sub and durable streaming\u2014on one broker.</p> <p>Danube enables one or many producers publish to topics, and multiple consumers receive messages via named subscriptions. Choose Non\u2011Reliable (best\u2011effort pub/sub) or Reliable (at\u2011least\u2011once streaming) per topic to match your workload.</p> <p>For design details, see the Architecture.</p>"},{"location":"#try-danube-in-minutes","title":"Try Danube in minutes","text":"<p>Docker Compose Quickstart: Use the provided Docker Compose setup with MinIO and ETCD.</p>"},{"location":"#danube-architecture","title":"Danube architecture","text":""},{"location":"#cluster-broker-characteristics","title":"\ud83c\udfd7\ufe0f Cluster &amp; Broker Characteristics","text":"<ul> <li>Stateless brokers: Metadata in ETCD and data in WAL/Object Storage</li> <li>Horizontal scaling: Add brokers in seconds with zero-downtime expansion</li> <li>Intelligent load balancing: Automatic topic placement and rebalancing across brokers</li> <li>Rolling upgrades: Restart or replace brokers with minimal disruption</li> <li>Security-ready: TLS/mTLS support in Admin and data paths</li> <li>Leader election &amp; HA: Automatic failover and coordination via ETCD</li> <li>Multi-tenancy: Isolated namespaces with policy controls</li> </ul>"},{"location":"#write-ahead-log-cloud-persistence","title":"\ud83c\udf29\ufe0f Write-Ahead Log + Cloud Persistence","text":"<ul> <li>Cloud-Native by Design - Danube's architecture separates compute from storage</li> <li>Multi-cloud support: AWS S3, Google Cloud Storage, Azure Blob, MinIO</li> <li>Hot path optimization: Messages served from in-memory WAL cache</li> <li>Stream per subscription: WAL + cloud storage from selected offset</li> <li>Asynchronous background uploads to S3/GCS/Azure object storage</li> <li>Infinite retention without local disk constraints</li> </ul>"},{"location":"#intelligent-load-management","title":"\ud83c\udfaf Intelligent Load Management","text":"<ul> <li>Automated rebalancing: Detects cluster imbalances and redistributes topics automatically</li> <li>Smart topic assignment: Places new topics on least-loaded brokers using configurable strategies</li> <li>Resource monitoring: Tracks CPU, memory, throughput, and backlog per broker in real-time</li> <li>Configurable policies: Conservative, balanced, or aggressive rebalancing based on workload</li> <li>Graceful topic migration: Moves topics between brokers without downtime</li> </ul>"},{"location":"#core-capabilities","title":"Core Capabilities","text":""},{"location":"#message-delivery","title":"\ud83d\udce8 Message Delivery","text":"<ul> <li>Topics: Partitioned and non-partitioned with automatic load balancing</li> <li>Reliable Dispatch: At-least-once delivery with configurable storage backends</li> <li>Non-Reliable Dispatch: High-throughput, low-latency for real-time scenarios</li> </ul>"},{"location":"#subscription-models","title":"\ud83d\udd04 Subscription Models","text":"<ul> <li>Exclusive: Single consumer per subscription</li> <li>Shared: Load-balanced message distribution across consumers</li> <li>Failover: Automatic consumer failover with ordered delivery</li> </ul>"},{"location":"#schema-registry","title":"\ud83d\udccb Schema Registry","text":"<ul> <li>Centralized schema management: Single source of truth for message schemas across all topics</li> <li>Schema versioning: Automatic version tracking with compatibility enforcement</li> <li>Multiple formats: Bytes, String, Number, JSON Schema, Avro, Protobuf</li> <li>Validation &amp; governance: Prevent invalid messages and ensure data quality</li> </ul>"},{"location":"#ai-powered-administration","title":"\ud83e\udd16 AI-Powered Administration","text":"<p>Danube features the AI-native messaging platform administration through the Model Context Protocol (MCP):</p> <ul> <li>Natural language cluster management: Manage your cluster by talking to AI assistants (Claude, Cursor, Windsurf)</li> <li>32 intelligent tools: Full cluster operations accessible via AI - topics, schemas, brokers, diagnostics, metrics</li> <li>Automated troubleshooting: AI-guided workflows for consumer lag analysis, health checks, and performance optimization</li> <li>Multiple interfaces: CLI commands, Web UI, or AI conversation - your choice</li> </ul> <p>Example: Ask Claude \"What's the cluster balance?\" or \"Create a partitioned topic for analytics\" and watch it happen.</p>"},{"location":"#architecture-deep-dives","title":"Architecture Deep Dives","text":"<p>Explore how Danube works under the hood:</p> <p>System Overview - Complete architecture diagram and component interaction</p> <p>Load Manager &amp; Rebalancing - Smart topic assignment and automatic rebalancing</p> <p>Persistence (WAL + Cloud) - Two-tier storage architecture</p> <ul> <li>Write\u2011Ahead Log on local disk for fast durable writes</li> <li>Background uploads to object storage for durability and replay at cloud cost</li> <li>Seamless handoff from historical replay to live tail</li> </ul> <p>Schema Registry - Centralized schema management</p> <ul> <li>Schema versioning and compatibility checking</li> <li>Support for JSON Schema, Avro, and Protobuf</li> <li>Data validation and governance</li> </ul> <p>Internal Services - Service discovery and coordination</p>"},{"location":"#integrations","title":"Integrations","text":"<p>Danube Connect - Plug-and-play connector ecosystem</p> <ul> <li>Source connectors: Import data from MQTT, HTTP webhooks, databases, Kafka , etc.</li> <li>Sink connectors: Export to Delta Lake, ClickHouse, vector databases, APIs, etc.</li> <li>Pure Rust framework with automatic retries, metrics, and health checks</li> </ul> <p>Learn more: Architecture | Build Source Connector | Build Sink Connector</p>"},{"location":"#crates-in-the-workspace","title":"Crates in the workspace","text":"<p>Repository: https://github.com/danube-messaging/danube</p> <ul> <li>danube-broker \u2013 The broker service (topics, producers, consumers, subscriptions).</li> <li>danube-core \u2013 Core types, protocol, and shared logic.</li> <li>danube-metadata-store \u2013 Metadata storage and cluster coordination.</li> <li>danube-persistent-storage \u2013 WAL and cloud persistence backends.</li> </ul> <p>CLIs and client libraries:</p> <ul> <li>danube-client \u2013 Async Rust client library.</li> <li>danube-cli \u2013 Publish/consume client CLI.</li> <li>danube-admin \u2013 Unified admin tool (CLI + AI/MCP + Web UI)</li> </ul>"},{"location":"#client-libraries","title":"Client libraries","text":"<ul> <li>danube-client (Rust)</li> <li>danube-go (Go)</li> </ul> <p>Contributions for other languages (Python, Java, etc.) are welcome.</p>"},{"location":"architecture/architecture/","title":"Danube Messaging Architecture","text":"<p>The Danube messaging system is a distributed messaging system, based on a publish-subscribe model, aiming to provide high throughput and low latency.</p> <p>The Danube Messaging system's architecture is designed for flexibility and scalability, making it suitable for event-driven and cloud-native applications. Its decoupled and pluggable architecture allows for independent scaling and easy integration of various storage backends. Using the dispatch strategies and the subscription models the system can accommodate different messaging patterns.</p> <p></p>"},{"location":"architecture/architecture/#brokers","title":"Brokers","text":"<p>Brokers are the core of the Danube Messaging system, responsible for routing and distributing messages, managing client connections and subscriptions, and implementing both reliable and non-reliable dispatch strategies. They act as the main entry point for publishers and subscribers, ensuring efficient and effective message flow.</p> <p>The Producers and Consumers connect to the Brokers to publish and consume messages, and use the subscription and dispatch mechanisms to accommodate various messaging patterns and reliability requirements.</p>"},{"location":"architecture/architecture/#metadata-storage","title":"Metadata Storage","text":"<p>The ETCD cluster serves as the metadata storage for the system by maintaining configuration data, topic information, and broker coordination and load-balancing, ensuring the entire system operates with high availability and consistent state management across all nodes.</p>"},{"location":"architecture/architecture/#storage-layer","title":"Storage Layer","text":"<p>The Storage Layer is responsible for message durability and replay. Danube now uses a cloud-native model based on a local Write-Ahead Log (WAL) for the hot path and background persistence to cloud object storage via OpenDAL. This keeps publish/dispatch latency low while enabling durable, elastic storage across providers (S3, GCS, Azure Blob, local FS, memory).</p> <p>Readers use tiered access: if data is within local WAL retention it is served from WAL/cache; otherwise historical data is streamed from cloud objects (using ETCD metadata) and seamlessly handed off to the WAL tail.</p>"},{"location":"architecture/architecture/#non-reliable-dispatch","title":"Non-Reliable Dispatch","text":"<p>Non-Reliable Dispatch operates with zero storage overhead, as messages flow directly from publishers to subscribers without intermediate persistence. This mode delivers maximum performance and lowest latency, making it ideal for scenarios where occasional message loss is acceptable, such as real-time metrics or live streaming data.</p>"},{"location":"architecture/architecture/#reliable-dispatch","title":"Reliable Dispatch","text":"<p>Reliable Dispatch offers guaranteed message delivery backed by the WAL + cloud persistence:</p> <ul> <li>WAL on local disk for low-latency appends and fast replay from in-memory cache and WAL files.</li> <li>Cloud object storage via OpenDAL (S3/GCS/Azure/FS/Memory) for durable, scalable historical data, uploaded asynchronously by a background uploader with resumable checkpoints.</li> <li>ETCD metadata tracks cloud objects and sparse indexes for efficient historical reads.</li> </ul> <p>The ability to choose between these dispatch modes gives users the flexibility to optimize their messaging infrastructure based on their specific requirements for performance, reliability, and resource utilization.</p> <p>For details and provider-specific configuration, see Persistence (WAL + Cloud).</p>"},{"location":"architecture/architecture/#design-considerations","title":"Design Considerations","text":""},{"location":"architecture/architecture/#decoupled-architecture","title":"Decoupled Architecture","text":"<p>The Danube Messaging system features a decoupled architecture where components are loosely coupled, allowing for independent scaling, easy maintenance and upgrades, and failure isolation.</p>"},{"location":"architecture/architecture/#plugin-architecture","title":"Plugin Architecture","text":"<p>With a plugin architecture, the system supports flexible storage backend options, making it easy to extend and customize according to different use cases. This adaptability ensures that the system can meet diverse application requirements and is cloud-native ready.</p>"},{"location":"architecture/architecture/#event-driven-focus","title":"Event-Driven Focus","text":"<p>Optimized for event-driven systems, the Danube Messaging system supports various message delivery patterns and scalable message processing. Its design is well-suited for microservices, providing efficient and scalable handling of event-driven workloads.</p>"},{"location":"architecture/internal_danube_services/","title":"Danube Cluster Services Role","text":"<p>This document enumerates the principal internal components of the Danube Broker.</p>"},{"location":"architecture/internal_danube_services/#danube-service-components","title":"Danube Service Components","text":"<p>The Broker owns the topics and manages their lifecycle. It also facilitates the creation of producers, subscriptions, and consumers, ensuring that producers can publish messages to topics and consumers can consume messages from topics.</p>"},{"location":"architecture/internal_danube_services/#leader-election-service","title":"Leader Election Service","text":"<p>The Leader Election Service selects one broker from the cluster to act as the Leader. The Broker Leader is responsible for making decisions. This service is used by the Load Manager, ensuring only one broker in the cluster posts the cluster aggregated Load Report.</p> <p>Leader Election Flow:</p> <ul> <li>The first broker registered in the cluster becomes the Leader by registering itself on \"/cluster/leader\".</li> <li>The field is registered with a lease, so the leader broker must periodically renew its lease to maintain leadership.</li> <li>Subsequent brokers attempt to become leaders but become Followers if the path is already in use.</li> <li>All brokers periodically check the leader path. If there is no change, the state is maintained; otherwise, brokers attempt to become the leader.</li> </ul>"},{"location":"architecture/internal_danube_services/#load-manager-service","title":"Load Manager Service","text":"<p>The Load Manager monitors and distributes load across brokers by managing topic and partition assignments. It implements rebalancing logic to redistribute topics/partitions when brokers join or leave the cluster and is responsible for failover mechanisms to handle broker failures.</p> <p>Load Manager Flow:</p> <ul> <li>All brokers periodically post their Load Reports on the path \"/cluster/brokers/load/{broker-id}\".</li> <li>The leader broker watches for load reports from all brokers in the cluster.</li> <li>It calculates rankings using the selected Load Balance algorithm.</li> <li>It posts its calculations for the cluster on the \"/cluster/load_balance\" path.</li> </ul> <p>Creation of a New Topic:</p> <ul> <li>A broker registers the Topic on the \"/cluster/unassigned\" path.</li> <li>The Load Manager of the leader Broker watches this path and assigns the broker with the least load to host the new topic by posting the topic to the \"/cluster/brokers/{broker-id}/{topic_name}\" path.</li> <li>Each broker watches its own path: \"/cluster/brokers/{broker-id}\". For any event on that path, such as the addition or deletion of topics, it acts accordingly by creating a new topic locally or deleting the topic it owned and all related resources.</li> <li>On topic creation, the broker checks if the topic already exists locally. If not, it retrieves all data about the topic, including subscriptions and producers, from the Local Metadata Cache.</li> <li>On topic removal, the broker handles the disconnections of producers and consumers and removes the locally allocated resources.</li> </ul> <p>For further consideration: We may want the broker to ask the Load Manager to get the next broker and initiate topic creation. Either it just posts the topic on the \"/cluster/unassigned\" path, or if it is the selected broker, it also creates the topic locally.</p>"},{"location":"architecture/internal_danube_services/#local-metadata-cache","title":"Local Metadata Cache","text":"<p>This cache stores various types of metadata required by Danube brokers, such as topic and namespace data, which are frequently accessed during message production and consumption. This reduces the need for frequent queries to the central metadata store, ETCD.</p> <p>The docs/etc_metadata_structure.md document describes how the resources are organized in the Metadata Store.</p> <p>Updates/events are received via ETCD Watch events and/or the metadata event synchronizer.</p>"},{"location":"architecture/internal_danube_services/#syncronizer","title":"Syncronizer","text":"<p>Not yet implemented.</p> <p>The synchronizer ensures that metadata and configuration settings across different brokers remain consistent. It propagates changes to metadata and configuration settings using client Producers and Consumers.</p> <p>This is in addition to Metadata Storage watch events, allowing brokers to process metadata updates even if there was a communication glitch or the broker was unavailable for a short period, potentially missing the Store Watch events. The synchronizer allows for dynamic updates to configuration settings without requiring a broker service restart.</p>"},{"location":"architecture/load_manager_architecture/","title":"Danube Load Manager &amp; Rebalancing Architecture","text":"<p>Danube's Load Manager is a distributed system that ensures optimal topic placement across broker clusters. It combines intelligent topic assignment for new topics with automated rebalancing to maintain cluster health as workloads evolve. This creates a self-optimizing messaging infrastructure that adapts to changing conditions without manual intervention.</p> <p>This page explains what the Load Manager is, why it's essential for production Danube clusters, how it assigns topics to brokers, and how automated rebalancing keeps your cluster balanced over time.</p>"},{"location":"architecture/load_manager_architecture/#what-is-the-load-manager","title":"What is the Load Manager?","text":"<p>The Load Manager is a critical subsystem in Danube that handles the distribution of topics across brokers in a cluster. Think of it as the \"traffic controller\" that decides which broker should handle which topic, ensuring that no single broker becomes overwhelmed while others sit idle.</p>"},{"location":"architecture/load_manager_architecture/#core-responsibilities","title":"Core Responsibilities","text":"<ul> <li>Topic Assignment - Places new topics on the least loaded broker using intelligent ranking algorithms</li> <li>Load Monitoring - Continuously tracks broker resource utilization (CPU, memory, throughput, topic count)</li> <li>Cluster Rebalancing - Automatically moves topics between brokers to maintain balanced load distribution</li> <li>Failover Management - Reassigns topics from failed brokers to healthy ones</li> <li>Resource Optimization - Prevents hotspots and ensures efficient cluster utilization</li> </ul>"},{"location":"architecture/load_manager_architecture/#why-it-matters","title":"Why It Matters","text":"<p>Without intelligent load management, clusters suffer from:</p> <ul> <li>Hotspots - One broker handles most traffic while others are idle</li> <li>Resource waste - Underutilized brokers mean wasted infrastructure costs</li> <li>Performance degradation - Overloaded brokers cause latency spikes and message backlogs</li> <li>Manual toil - Operators spend time manually moving topics to fix imbalances</li> <li>Scaling challenges - Hard to add/remove brokers without disrupting service</li> </ul> <p>The Load Manager solves these problems automatically, keeping your cluster healthy 24/7.</p>"},{"location":"architecture/load_manager_architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        ETCD Metadata Store                       \u2502\n\u2502  \u2022 Broker registrations    \u2022 Topic assignments                  \u2502\n\u2502  \u2022 Load reports            \u2022 Rebalancing history                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n                 \u2502 Watch events\n                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Load Manager (Leader)      \u2502         \u2502   Other Brokers    \u2502\n\u2502                                 \u2502         \u2502  (Followers)       \u2502\n\u2502  1. Monitor Load Reports        \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2022 Publish loads   \u2502\n\u2502  2. Calculate Rankings          \u2502         \u2502  \u2022 Watch events    \u2502\n\u2502  3. Assign New Topics           \u2502         \u2502  \u2022 Execute unloads \u2502\n\u2502  4. Detect Imbalance (CV)       \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502  5. Select Topics to Move       \u2502\n\u2502  6. Execute Rebalancing         \u2502\n\u2502                                 \u2502\n\u2502  Components:                    \u2502\n\u2502  \u251c\u2500 Rankings Calculator         \u2502\n\u2502  \u251c\u2500 Imbalance Detector          \u2502\n\u2502  \u251c\u2500 Candidate Selector          \u2502\n\u2502  \u2514\u2500 Rebalancing Executor        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nFlow for New Topic Assignment:\n1. Topic created \u2192 Added to /unassigned/ path\n2. Load Manager (leader) watches unassigned topics\n3. Calculates broker rankings (least loaded first)\n4. Assigns topic to top-ranked broker\n5. Broker receives assignment and loads topic\n\nFlow for Automated Rebalancing:\n1. Load Manager checks cluster balance every N minutes\n2. Calculates imbalance (coefficient of variation)\n3. If CV &gt; threshold \u2192 Select topics to move\n4. Creates unassigned marker with target broker hint\n5. Deletes assignment from overloaded broker\n6. Topic moves to underloaded broker\n7. Records move in history for rate limiting\n</code></pre>"},{"location":"architecture/load_manager_architecture/#component-roles","title":"Component Roles","text":"<p>Load Manager (Leader-Only) Centralized coordinator that assigns topics and executes rebalancing. Only the elected cluster leader runs these operations to ensure consistency.</p> <p>Broker Load Reports Every broker publishes resource utilization metrics (CPU, memory, disk I/O, network I/O) and per-topic statistics (throughput, connections, backlog) to ETCD every 30 seconds.</p> <p>Rankings Calculator Scores all brokers based on their current load using configurable strategies (Fair, Balanced, WeightedLoad). Lower scores mean less loaded.</p> <p>Imbalance Detector Calculates statistical measures (coefficient of variation, standard deviation) to determine if cluster needs rebalancing.</p> <p>Rebalancing Executor Orchestrates topic moves using graceful unload workflow. Enforces rate limits and cooldowns to prevent disruption.</p> <p>ETCD Metadata Store Single source of truth for cluster state. Stores broker registrations, topic assignments, load reports, and rebalancing history.</p>"},{"location":"architecture/load_manager_architecture/#core-concepts","title":"Core Concepts","text":""},{"location":"architecture/load_manager_architecture/#brokers-and-broker-states","title":"Brokers and Broker States","text":"<p>A broker is a server instance running the Danube broker process. Each broker has a unique ID and maintains:</p> <ul> <li>Topic assignments - Which topics it currently serves</li> <li>Resource metrics - CPU, memory, disk I/O, network I/O usage</li> <li>Topic metrics - Per-topic throughput, connections, backlog</li> </ul> <p>Broker states:</p> <ul> <li>Active - Normal operation, can receive new topics</li> <li>Draining - Preparing to shut down, topics being unloaded</li> <li>Drained - All topics moved, ready for removal</li> </ul> <p>Only active brokers are considered for topic assignment and rebalancing calculations.</p>"},{"location":"architecture/load_manager_architecture/#load-reports","title":"Load Reports","text":"<p>Every broker generates a LoadReport every 30 seconds containing:</p> <p>System-Level Metrics:</p> <ul> <li>CPU usage percentage (0-100%)</li> <li>Memory usage percentage (0-100%)</li> <li>Disk I/O throughput (bytes/second)</li> <li>Network I/O throughput (bytes/second)</li> </ul> <p>Topic-Level Metrics:</p> <ul> <li>Message rate (messages/second)</li> <li>Byte rate (bytes/second, converted to Mbps)</li> <li>Producer count</li> <li>Consumer count</li> <li>Subscription count</li> <li>Backlog messages</li> </ul> <p>Aggregate Metrics:</p> <ul> <li>Total throughput (Mbps across all topics)</li> <li>Total message rate</li> <li>Total lag/backlog</li> </ul> <p>These reports are published to ETCD where the Load Manager consumes them for ranking and rebalancing decisions.</p>"},{"location":"architecture/load_manager_architecture/#broker-rankings","title":"Broker Rankings","text":"<p>Rankings are a sorted list of <code>(broker_id, load_score)</code> pairs ordered by load (ascending = less loaded first). The Load Manager maintains these rankings in-memory and updates them whenever new load reports arrive.</p> <p>Rankings determine:</p> <ul> <li>Which broker receives the next topic assignment</li> <li>Which brokers are overloaded (candidates for offloading)</li> <li>Which brokers are underloaded (candidates for receiving topics)</li> </ul> <p>The ranking algorithm is configurable (see Assignment Strategies).</p>"},{"location":"architecture/load_manager_architecture/#coefficient-of-variation-cv","title":"Coefficient of Variation (CV)","text":"<p>The coefficient of variation measures cluster balance:</p> <pre><code>CV = (standard_deviation / mean_load) \u00d7 100%\n</code></pre> <p>Interpretation:</p> <ul> <li>CV &lt; 20% - Excellent balance, loads are very similar</li> <li>CV 20-30% - Good balance, acceptable variance</li> <li>CV 30-40% - Moderate imbalance, consider rebalancing</li> <li>CV &gt; 40% - Significant imbalance, rebalancing recommended</li> </ul> <p>The Load Manager uses CV thresholds to trigger automated rebalancing based on the configured aggressiveness level.</p>"},{"location":"architecture/load_manager_architecture/#assignment-strategies","title":"Assignment Strategies","text":"<p>Three strategies control how new topics are assigned:</p> Strategy Algorithm Best For Overhead Fair Topic count only Development, testing, predictable placement Lowest Balanced Weighted: topic_load (30%) + CPU (35%) + Memory (35%) General production, mixed workloads Medium WeightedLoad Adaptive bottleneck detection Variable workloads, auto-optimization Highest <p>Fair Strategy: Simple topic counting. Broker with fewest topics gets the next assignment. Ignores actual resource usage and topic throughput.</p> <p>Balanced Strategy (RECOMMENDED): Multi-factor scoring combining weighted topic load (message rate, connections, backlog) with system resources. Most reliable for production clusters.</p> <p>WeightedLoad Strategy: Smart algorithm that detects which resource is under most pressure (CPU, memory, throughput, etc.) and prioritizes it in scoring. Adapts automatically to workload patterns.</p>"},{"location":"architecture/load_manager_architecture/#rebalancing-aggressiveness","title":"Rebalancing Aggressiveness","text":"<p>Three aggressiveness levels control automated rebalancing behavior:</p> Level CV Threshold Check Interval Max Moves/Hour Best For Conservative 40% 10 minutes 5 Stable clusters, minimal disruption Balanced 30% 5 minutes 10 General production use Aggressive 20% 2 minutes 20 Fast-changing workloads, test clusters <p>Higher aggressiveness means:</p> <ul> <li>More frequent balance checks</li> <li>Lower tolerance for imbalance</li> <li>More topic moves per hour</li> <li>Faster response to load changes</li> <li>Higher metadata store overhead</li> </ul>"},{"location":"architecture/load_manager_architecture/#topic-blacklists","title":"Topic Blacklists","text":"<p>Administrators can configure topic patterns that should never be rebalanced. This is useful for:</p> <ul> <li>Critical topics that must stay on specific brokers</li> <li>Low-latency topics where moves would cause disruption</li> <li>Topics with large backlogs (expensive to move)</li> <li>System topics used for coordination</li> </ul> <p>Blacklist patterns support wildcards:</p> <pre><code>blacklist_topics:\n  - \"/system/*\"           # All system topics\n  - \"/default/critical-*\" # Critical workload topics\n  - \"/analytics/logs\"     # Specific high-volume topic\n</code></pre>"},{"location":"architecture/load_manager_architecture/#how-it-works-topic-assignment","title":"How It Works: Topic Assignment","text":"<p>When a new topic is created, it must be assigned to a broker. This process ensures the topic lands on the broker best suited to handle it.</p>"},{"location":"architecture/load_manager_architecture/#step-by-step-flow","title":"Step-by-Step Flow","text":"<p>1. Topic Creation</p> <p>A producer creates a topic by sending a message to a topic that doesn't exist yet. The broker creates the topic metadata in ETCD under <code>/topics/{namespace}/{topic}</code>.</p> <p>2. Unassigned Entry Creation</p> <p>The broker creates an entry in ETCD at:</p> <pre><code>/cluster/unassigned/{namespace}/{topic}\n</code></pre> <p>This signals to the Load Manager that a topic needs assignment. The value is empty (no marker data).</p> <p>3. Load Manager Detection</p> <p>The Load Manager (running on the leader broker) watches the <code>/cluster/unassigned/</code> path. When a new entry appears, it triggers the assignment workflow.</p> <p>4. Rankings Calculation</p> <p>The Load Manager retrieves the latest load reports for all active brokers and calculates rankings using the configured strategy:</p> <p>Fair Strategy:</p> <pre><code>score = topic_count\n</code></pre> <p>Balanced Strategy:</p> <pre><code>weighted_topic_load = (count \u00d7 0.2 + throughput \u00d7 0.3 + \n                       connections \u00d7 0.3 + backlog \u00d7 0.2)\nscore = (weighted_topic_load \u00d7 0.3) + (CPU \u00d7 0.35) + (Memory \u00d7 0.35)\n</code></pre> <p>WeightedLoad Strategy:</p> <pre><code>1. Normalize all metrics (0.0 to 1.0)\n2. Find bottleneck (max utilization metric)\n3. If bottleneck &gt; 70% \u2192 Weight it heavily (50%)\n4. Otherwise \u2192 Balance evenly (25% each metric)\n</code></pre> <p>Brokers are sorted by score (ascending), so the least loaded broker appears first.</p> <p>5. Broker Selection</p> <p>The Load Manager selects brokers from the top of the rankings (lowest scores). To prevent all assignments going to one broker when multiple brokers have similar loads, it uses round-robin within threshold:</p> <pre><code>1. Get minimum score from rankings\n2. Find all brokers within 10% of minimum score\n3. Round-robin through these candidates\n4. Track last selected broker to ensure rotation\n</code></pre> <p>This ensures even distribution when brokers have comparable loads.</p> <p>6. Assignment Creation</p> <p>The Load Manager writes the assignment to ETCD:</p> <pre><code>/cluster/brokers/{broker_id}/{namespace}/{topic} \u2192 null\n</code></pre> <p>The target broker watches this path and receives the assignment event.</p> <p>7. Topic Loading</p> <p>The target broker:</p> <ol> <li>Creates the topic instance locally</li> <li>Initializes subscriptions and storage</li> <li>Begins accepting producer connections</li> <li>Starts reporting topic metrics in load reports</li> </ol> <p>8. Cleanup</p> <p>The Load Manager deletes the unassigned entry:</p> <pre><code>DELETE /cluster/unassigned/{namespace}/{topic}\n</code></pre> <p>This prevents duplicate assignments.</p> <p>9. Internal State Update</p> <p>The Load Manager updates its in-memory broker usage map:</p> <pre><code>brokers_usage[broker_id].topics.push(new_topic_placeholder)\n</code></pre> <p>This ensures the next assignment sees the updated load, even before the broker's next load report arrives.</p>"},{"location":"architecture/load_manager_architecture/#assignment-visualization","title":"Assignment Visualization","text":"<pre><code>New Topic Created\n       \u2502\n       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 /topics/ns/top  \u2502  Topic metadata created\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 /unassigned/... \u2502  Unassigned marker created\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502 Watch event\n       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502        Load Manager (Leader)         \u2502\n  \u2502                                      \u2502\n  \u2502  1. Read load reports from ETCD     \u2502\n  \u2502  2. Calculate broker rankings        \u2502\n  \u2502     Broker 1: score=20  \u2190 least     \u2502\n  \u2502     Broker 2: score=25              \u2502\n  \u2502     Broker 3: score=45  \u2190 most      \u2502\n  \u2502  3. Select Broker 1 (round-robin)   \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 /brokers/1/ns/top   \u2502  Assignment created\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502 Watch event\n       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502   Broker 1      \u2502  Loads topic, starts serving\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 DELETE /unassigned/... \u2502  Cleanup\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/load_manager_architecture/#special-cases","title":"Special Cases","text":"<p>Broker Unload (Manual): When an administrator unloads a broker, topics create unassigned markers with:</p> <pre><code>{\n  \"reason\": \"unload\",\n  \"from_broker\": 12345\n}\n</code></pre> <p>The Load Manager excludes the source broker when selecting the target.</p> <p>Broker Failure: When a broker crashes, the leader detects the registration deletion in ETCD and:</p> <ol> <li>Deletes all assignments from the failed broker</li> <li>Creates unassigned markers for all topics (no hint)</li> <li>Normal assignment workflow reassigns topics to healthy brokers</li> </ol> <p>All Brokers Equally Loaded: When all brokers have identical scores, the Load Manager uses stable round-robin rotation to distribute topics evenly.</p>"},{"location":"architecture/load_manager_architecture/#how-it-works-automated-rebalancing","title":"How It Works: Automated Rebalancing","text":"<p>As workloads change over time, clusters can become imbalanced. Some brokers handle more traffic than others, leading to hotspots and inefficiency. Automated rebalancing solves this by continuously monitoring cluster health and moving topics to restore balance.</p>"},{"location":"architecture/load_manager_architecture/#rebalancing-lifecycle","title":"Rebalancing Lifecycle","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Rebalancing Loop (Leader)                    \u2502\n\u2502                                                               \u2502\n\u2502  Every N minutes (configured by check_interval):             \u2502\n\u2502                                                               \u2502\n\u2502  1. Check Leadership                                          \u2502\n\u2502     \u251c\u2500 If follower \u2192 Skip cycle                              \u2502\n\u2502     \u2514\u2500 If leader \u2192 Continue                                   \u2502\n\u2502                                                               \u2502\n\u2502  2. Check Configuration                                       \u2502\n\u2502     \u251c\u2500 If rebalancing disabled \u2192 Skip cycle                  \u2502\n\u2502     \u2514\u2500 If enabled \u2192 Continue                                  \u2502\n\u2502                                                               \u2502\n\u2502  3. Check Cluster Health                                      \u2502\n\u2502     \u251c\u2500 If &lt; 2 brokers \u2192 Skip cycle                          \u2502\n\u2502     \u2514\u2500 If &gt;= 2 brokers \u2192 Continue                            \u2502\n\u2502                                                               \u2502\n\u2502  4. Calculate Imbalance Metrics                               \u2502\n\u2502     \u251c\u2500 Get broker rankings                                    \u2502\n\u2502     \u251c\u2500 Calculate mean, std_deviation, CV                     \u2502\n\u2502     \u2514\u2500 Identify overloaded/underloaded brokers               \u2502\n\u2502                                                               \u2502\n\u2502  5. Decide If Rebalancing Needed                             \u2502\n\u2502     \u251c\u2500 If CV &lt; threshold \u2192 Cluster balanced, skip            \u2502\n\u2502     \u2514\u2500 If CV &gt; threshold \u2192 Continue                          \u2502\n\u2502                                                               \u2502\n\u2502  6. Select Topics to Move                                     \u2502\n\u2502     \u251c\u2500 For each overloaded broker                            \u2502\n\u2502     \u251c\u2500 Filter blacklisted topics                             \u2502\n\u2502     \u251c\u2500 Filter recently moved topics (cooldown)               \u2502\n\u2502     \u251c\u2500 Filter topics too young (min_topic_age)               \u2502\n\u2502     \u251c\u2500 Sort by load (lightest first)                         \u2502\n\u2502     \u2514\u2500 Select lightest topic                                 \u2502\n\u2502                                                               \u2502\n\u2502  7. Select Target Brokers                                     \u2502\n\u2502     \u251c\u2500 Find underloaded brokers from rankings                \u2502\n\u2502     \u251c\u2500 Exclude source broker                                 \u2502\n\u2502     \u2514\u2500 Select least loaded active broker                     \u2502\n\u2502                                                               \u2502\n\u2502  8. Execute Rebalancing Moves                                 \u2502\n\u2502     \u251c\u2500 Check rate limit (max_moves_per_hour)                \u2502\n\u2502     \u251c\u2500 Create unassigned marker with target hint             \u2502\n\u2502     \u251c\u2500 Delete assignment from source broker                  \u2502\n\u2502     \u251c\u2500 Wait for topic reassignment                           \u2502\n\u2502     \u251c\u2500 Record move in history                                \u2502\n\u2502     \u251c\u2500 Apply cooldown delay                                  \u2502\n\u2502     \u2514\u2500 Repeat for next move                                  \u2502\n\u2502                                                               \u2502\n\u2502  9. Update Metrics &amp; Logs                                     \u2502\n\u2502     \u2514\u2500 Log cycle completion and move count                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/load_manager_architecture/#imbalance-detection","title":"Imbalance Detection","text":"<p>The Load Manager calculates statistical measures to determine if rebalancing is needed:</p> <p>Metrics Calculated:</p> <ul> <li>Mean load - Average load across all active brokers</li> <li>Standard deviation - Spread of loads around the mean</li> <li>Coefficient of variation (CV) - <code>(std_dev / mean) \u00d7 100%</code></li> <li>Max load - Highest loaded broker</li> <li>Min load - Lowest loaded broker</li> </ul> <p>Broker Classification:</p> <ul> <li>Overloaded - Load &gt; (mean + 1 std_dev)</li> <li>Underloaded - Load &lt; (mean - 1 std_dev) AND load &lt; (mean \u00d7 0.5)</li> <li>Normal - Between overloaded and underloaded</li> </ul> <p>Example:</p> <pre><code>Broker 1: load = 45\nBroker 2: load = 30\nBroker 3: load = 25\n\nmean = 33.33\nstd_dev = 8.50\nCV = (8.50 / 33.33) = 0.255 = 25.5%\n\nOverloaded: load &gt; 41.83 \u2192 Broker 1\nUnderloaded: load &lt; 25.00 AND load &lt; 16.67 \u2192 None\nNormal: Broker 2, Broker 3\n</code></pre> <p>If <code>CV &gt; threshold</code> (based on aggressiveness level), rebalancing is triggered.</p>"},{"location":"architecture/load_manager_architecture/#candidate-selection","title":"Candidate Selection","text":"<p>Once imbalance is detected, the Load Manager selects topics to move:</p> <p>Selection Criteria:</p> <ol> <li>Source brokers - Only overloaded brokers (load &gt; mean + std_dev)</li> <li>Topic age - Skip topics younger than <code>min_topic_age_seconds</code> (default 5 minutes)</li> <li>Blacklist - Skip topics matching blacklist patterns</li> <li>Cooldown - Skip topics moved in the last <code>cooldown_seconds</code> (default 60 seconds)</li> <li>Load score - Prefer lightest topics (easier to move, less disruption)</li> </ol> <p>Topic Load Scoring:</p> <pre><code>load_score = (topic_count \u00d7 0.2) + \n             (throughput_mbps \u00d7 0.3) + \n             (connections \u00d7 0.3) + \n             (backlog/10000 \u00d7 0.2)\n</code></pre> <p>Topics are sorted by score (ascending), and the lightest topic is selected first.</p> <p>Move Limits:</p> <ul> <li>Maximum 1 topic moved per rebalancing cycle (single-move design)</li> <li>Maximum N moves per hour (rate limiting, configurable)</li> <li>Cooldown delay between individual moves (prevents oscillations)</li> </ul>"},{"location":"architecture/load_manager_architecture/#target-broker-selection","title":"Target Broker Selection","text":"<p>For each topic to move, the Load Manager selects a target broker:</p> <ol> <li>Get rankings - Sorted list of brokers by load (ascending)</li> <li>Filter active - Exclude brokers in draining/drained state</li> <li>Exclude source - Can't move topic to same broker</li> <li>Select first - Pick the least loaded broker meeting criteria</li> </ol> <p>If no suitable target exists (e.g., only one broker active), the move is skipped with a warning.</p>"},{"location":"architecture/load_manager_architecture/#move-execution","title":"Move Execution","text":"<p>The Load Manager executes moves using the existing graceful unload workflow:</p> <p>Step 1: Create Unassigned Marker</p> <pre><code>Path: /cluster/unassigned/{namespace}/{topic}\nValue: {\n  \"reason\": \"rebalance\",\n  \"from_broker\": 12345,\n  \"to_broker\": 67890,\n  \"timestamp\": 1234567890\n}\n</code></pre> <p>The <code>to_broker</code> hint tells the assignment logic to prefer broker 67890 (if active).</p> <p>Step 2: Delete Source Assignment</p> <pre><code>DELETE /cluster/brokers/12345/{namespace}/{topic}\n</code></pre> <p>This triggers the source broker's topic watcher, which:</p> <ol> <li>Seals the topic (no new producers)</li> <li>Drains in-flight messages</li> <li>Unloads the topic from memory</li> <li>Releases resources</li> </ol> <p>Step 3: Target Receives Assignment</p> <p>The Load Manager's assignment logic sees the unassigned marker and:</p> <ol> <li>Checks if target broker (67890) is active</li> <li>If active \u2192 Assign to target broker (respecting hint)</li> <li>If not active \u2192 Select alternative broker from rankings</li> </ol> <p>Step 4: Topic Loads on Target</p> <p>The target broker:</p> <ol> <li>Receives assignment from ETCD watch</li> <li>Loads topic metadata</li> <li>Initializes subscriptions</li> <li>Accepts producer reconnections</li> <li>Begins serving traffic</li> </ol> <p>Step 5: Record Move in History</p> <p>The Load Manager records the move:</p> <pre><code>Path: /cluster/rebalancing_history/{timestamp}\nValue: {\n  \"topic\": \"/default/my-topic\",\n  \"from_broker\": 12345,\n  \"to_broker\": 67890,\n  \"reason\": \"LoadImbalance\",\n  \"estimated_load\": 15.3,\n  \"timestamp\": 1234567890\n}\n</code></pre> <p>This history is used for:</p> <ul> <li>Rate limiting (counting moves in last hour)</li> <li>Cooldown enforcement (preventing rapid re-moves)</li> <li>Audit trail (troubleshooting and analysis)</li> <li>Metrics (tracking rebalancing activity)</li> </ul> <p>Step 6: Cooldown Delay</p> <p>After a successful move, the Load Manager waits for <code>cooldown_seconds</code> before the next move. This prevents:</p> <ul> <li>Rapid oscillations (topic bouncing between brokers)</li> <li>Metadata store overload (too many writes)</li> <li>Cluster instability (constant topic movements)</li> </ul>"},{"location":"architecture/load_manager_architecture/#rebalancing-visualization","title":"Rebalancing Visualization","text":"<pre><code>Imbalance Detected (CV &gt; threshold)\n       \u2502\n       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Select Topics to Move           \u2502\n  \u2502                                  \u2502\n  \u2502  Broker 1 (overloaded):         \u2502\n  \u2502    \u251c\u2500 topic-A: load = 5.2       \u2502\n  \u2502    \u251c\u2500 topic-B: load = 8.1       \u2502\n  \u2502    \u2514\u2500 topic-C: load = 12.3      \u2502\n  \u2502                                  \u2502\n  \u2502  Select lightest: topic-A       \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Select Target Broker            \u2502\n  \u2502                                  \u2502\n  \u2502  Rankings:                       \u2502\n  \u2502    Broker 3: load = 20 \u2190 target \u2502\n  \u2502    Broker 2: load = 30          \u2502\n  \u2502    Broker 1: load = 45          \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Create Move Plan                 \u2502\n  \u2502  topic-A: Broker 1 \u2192 Broker 3    \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Execute Move                \u2502\n  \u2502  1. Create unassigned marker \u2502\n  \u2502     with target hint         \u2502\n  \u2502  2. Delete assignment        \u2502\n  \u2502     from Broker 1            \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u25bc                  \u25bc                  \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Broker 1 \u2502     \u2502Load Manager\u2502     \u2502 Broker 3 \u2502\n  \u2502          \u2502     \u2502            \u2502     \u2502          \u2502\n  \u2502 Unload   \u2502     \u2502 Assign to  \u2502     \u2502 Load     \u2502\n  \u2502 topic-A  \u2502     \u2502 Broker 3   \u2502     \u2502 topic-A  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                  \u2502                  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502 Record in       \u2502\n                 \u2502 History         \u2502\n                 \u2502 Apply Cooldown  \u2502\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/load_manager_architecture/#safety-mechanisms","title":"Safety Mechanisms","text":"<p>Automated rebalancing includes multiple safety mechanisms to prevent disruption:</p> <p>1. Single Move Per Cycle </p> <p>Only one topic is moved per rebalancing check cycle. This prevents:</p> <ul> <li>Overshooting (moving too many topics at once)</li> <li>Cascading imbalances (fixing one problem creates another)</li> <li>Metadata store overload</li> </ul> <p>2. Rate Limiting Maximum moves per hour enforced via history tracking. Prevents rebalancing storms during cluster instability.</p> <p>3. Cooldown Period Topics can't be moved again within <code>cooldown_seconds</code> (default 60s). Prevents oscillations where topics bounce back and forth.</p> <p>4. Minimum Topic Age Topics younger than <code>min_topic_age_seconds</code> (default 300s) are never moved. Allows topics to stabilize after creation before considering them for rebalancing.</p> <p>5. Blacklist Critical topics can be permanently excluded from rebalancing using pattern matching.</p> <p>6. Leader-Only Execution Only the elected cluster leader performs rebalancing. Prevents multiple brokers from executing conflicting moves.</p> <p>7. Active Broker Check Target brokers must be in \"active\" state. Draining or drained brokers are excluded.</p> <p>8. Graceful Unload Topic moves use the same graceful unload workflow as manual operations:</p> <ul> <li>Seal topic (no new producers)</li> <li>Drain in-flight messages</li> <li>Unload cleanly</li> </ul> <p>9. Metrics Logging All rebalancing activity is logged with full context for troubleshooting and auditing.</p>"},{"location":"architecture/load_manager_architecture/#configuration-and-tuning","title":"Configuration and Tuning","text":""},{"location":"architecture/load_manager_architecture/#basic-configuration","title":"Basic Configuration","text":"<p>Load Manager configuration in <code>config/danube_broker.yml</code>:</p> <pre><code>load_manager:\n  # How often brokers report load to ETCD\n  load_report_interval_seconds: 30\n\n  # Assignment strategy for new topics\n  # Options: fair, balanced, weighted_load\n  assignment_strategy: balanced\n\n  # Automated rebalancing settings\n  rebalancing:\n    enabled: false                    # Start disabled, enable after testing\n    aggressiveness: balanced          # conservative | balanced | aggressive\n    check_interval_seconds: 300       # How often to check balance (5 min)\n    max_moves_per_hour: 10            # Rate limit\n    cooldown_seconds: 60              # Wait between moves\n    min_brokers_for_rebalance: 2      # Need at least 2 brokers\n    min_topic_age_seconds: 300        # Don't move young topics (5 min)\n    blacklist_topics: []              # Topics to never rebalance\n</code></pre>"},{"location":"architecture/load_manager_architecture/#assignment-strategy-selection","title":"Assignment Strategy Selection","text":"Scenario Recommended Strategy Reason Development/testing <code>fair</code> Simple, predictable, low overhead Production (general) <code>balanced</code> Best balance of accuracy and overhead Heterogeneous hardware <code>balanced</code> Accounts for CPU/memory differences Uniform hardware <code>fair</code> or <code>balanced</code> Fair works well when brokers are identical Variable workloads <code>weighted_load</code> Adapts to changing resource bottlenecks High-throughput topics <code>balanced</code> or <code>weighted_load</code> Considers throughput in scoring"},{"location":"architecture/load_manager_architecture/#aggressiveness-tuning","title":"Aggressiveness Tuning","text":"<p>Conservative - Use when:</p> <ul> <li>Cluster is mostly stable</li> <li>Topic moves are expensive (large backlogs)</li> <li>Minimizing disruption is critical</li> <li>Resources are plentiful</li> </ul> <p>Balanced - Use when:</p> <ul> <li>General production workloads</li> <li>Normal mix of topic sizes</li> <li>Moderate load variability</li> <li>Default choice for most clusters</li> </ul> <p>Aggressive - Use when:</p> <ul> <li>Workloads change frequently</li> <li>Fast response to imbalance is needed</li> <li>Testing/development environments</li> <li>Cluster has capacity for frequent moves</li> </ul>"},{"location":"architecture/load_manager_architecture/#load-report-interval-tuning","title":"Load Report Interval Tuning","text":"<p>Lower intervals (5-10s):</p> <ul> <li>Faster response to load changes</li> <li>More accurate rankings</li> <li>Higher ETCD traffic</li> <li>Better for testing</li> </ul> <p>Higher intervals (30-60s):</p> <ul> <li>Lower overhead</li> <li>Suitable for stable production clusters</li> <li>Slower response to changes</li> <li>Industry standard</li> </ul> <p>Recommendation: Start with 30s, reduce only if needed for fast-changing workloads.</p>"},{"location":"architecture/load_manager_architecture/#rate-limiting-tuning","title":"Rate Limiting Tuning","text":"<p>The <code>max_moves_per_hour</code> setting prevents rebalancing storms:</p> <p>Conservative:</p> <ul> <li><code>max_moves_per_hour: 5</code></li> <li>Maximum 5 topics moved per hour</li> <li>Best for large topics with backlogs</li> </ul> <p>Balanced:</p> <ul> <li><code>max_moves_per_hour: 10</code></li> <li>Moderate rebalancing speed</li> <li>Good for most clusters</li> </ul> <p>Aggressive:</p> <ul> <li><code>max_moves_per_hour: 20</code></li> <li>Faster rebalancing</li> <li>Suitable for test clusters or small topics</li> </ul>"},{"location":"architecture/load_manager_architecture/#blacklist-patterns","title":"Blacklist Patterns","text":"<p>Exclude critical topics from rebalancing:</p> <pre><code>blacklist_topics:\n  # System topics\n  - \"/system/*\"\n\n  # Critical low-latency topics\n  - \"/default/critical-*\"\n\n  # High-volume topics (expensive to move)\n  - \"/analytics/logs\"\n  - \"/metrics/*\"\n\n  # Topics with specific placement requirements\n  - \"/pinned/special-topic\"\n</code></pre>"},{"location":"architecture/load_manager_architecture/#metrics-and-monitoring","title":"Metrics and Monitoring","text":""},{"location":"architecture/load_manager_architecture/#load-manager-metrics","title":"Load Manager Metrics","text":"<p>The Load Manager exposes Prometheus metrics for observability:</p> <p>Cluster Health Metrics:</p> <pre><code>danube_cluster_balance_cv            # Current coefficient of variation\ndanube_cluster_balance_mean_load     # Mean broker load\ndanube_cluster_balance_max_load      # Max broker load\ndanube_cluster_balance_min_load      # Min broker load\ndanube_cluster_balance_std_dev       # Standard deviation\ndanube_cluster_active_brokers        # Count of active brokers\n</code></pre> <p>Rebalancing Metrics:</p> <pre><code>danube_rebalancing_checks_total      # Total balance check cycles\ndanube_rebalancing_moves_total       # Total topic moves executed\ndanube_rebalancing_failures_total    # Failed move attempts\ndanube_rebalancing_cycle_duration_seconds  # Duration of rebalancing cycles\n</code></pre> <p>Topic Assignment Metrics:</p> <pre><code>danube_topic_assignments_total       # Total topic assignments\n  labels: broker_id, action (assign/unassign)\n</code></pre> <p>Broker Load Metrics:</p> <pre><code>danube_broker_topics_owned           # Topics per broker\ndanube_broker_cpu_usage              # CPU percentage\ndanube_broker_memory_usage           # Memory percentage\ndanube_broker_throughput_mbps        # Aggregate throughput\n</code></pre>"},{"location":"architecture/load_manager_architecture/#use-cases-and-best-practices","title":"Use Cases and Best Practices","text":""},{"location":"architecture/load_manager_architecture/#scenario-1-adding-brokers-to-a-cluster","title":"Scenario 1: Adding Brokers to a Cluster","text":"<p>Situation: You start with 3 brokers, each handling 100 topics. You add 2 new brokers to scale capacity.</p> <p>Without Rebalancing:</p> <ul> <li>New brokers sit idle (0 topics each)</li> <li>Old brokers remain at 100 topics</li> <li>Cluster is imbalanced (CV = 100%)</li> <li>New capacity is wasted</li> </ul> <p>With Rebalancing:</p> <ol> <li>New brokers join and register</li> <li>Load Manager detects imbalance (CV &gt; threshold)</li> <li>Selects 40 lightest topics from old brokers</li> <li>Moves them to new brokers over time (rate limited)</li> <li>Final state: 60 topics per broker (balanced)</li> <li>CV drops to &lt; 10%</li> </ol> <p>Best Practice:</p> <ul> <li>Enable rebalancing before adding brokers</li> <li>Use aggressive mode temporarily for faster rebalancing</li> <li>Monitor rebalancing progress via metrics</li> <li>Return to balanced mode after completion</li> </ul>"},{"location":"architecture/load_manager_architecture/#scenario-2-handling-broker-failure","title":"Scenario 2: Handling Broker Failure","text":"<p>Situation: A broker crashes, and its 150 topics need reassignment.</p> <p>Without Load Manager:</p> <ul> <li>Manual intervention required</li> <li>Topics go offline until reassigned</li> <li>Risk of creating new hotspots</li> </ul> <p>With Load Manager:</p> <ol> <li>Leader detects broker deregistration</li> <li>Deletes all assignments from failed broker</li> <li>Creates 150 unassigned markers</li> <li>Assignment workflow distributes topics across remaining brokers</li> <li>Topics reassigned in &lt; 1 minute</li> </ol> <p>Best Practice:</p> <ul> <li>Ensure at least 3 brokers for redundancy</li> <li>Use balanced or weighted_load strategy for failover</li> <li>Monitor failover metrics</li> <li>Set appropriate <code>min_brokers_for_rebalance</code></li> </ul>"},{"location":"architecture/load_manager_architecture/#scenario-3-workload-changes-over-time","title":"Scenario 3: Workload Changes Over Time","text":"<p>Situation: Topics created in the morning (low traffic) grow to high traffic in the afternoon, creating imbalance.</p> <p>Without Rebalancing:</p> <ul> <li>Initial assignment is fair</li> <li>As traffic grows, some brokers become hotspots</li> <li>Performance degrades on overloaded brokers</li> </ul> <p>With Rebalancing:</p> <ol> <li>Load reports show increasing CPU/throughput on some brokers</li> <li>CV rises above threshold</li> <li>Rebalancing moves high-traffic topics to underloaded brokers</li> <li>Cluster remains balanced throughout the day</li> </ol> <p>Best Practice:</p> <ul> <li>Use balanced or weighted_load strategy (accounts for throughput)</li> <li>Enable rebalancing for dynamic workloads</li> <li>Set check_interval to match workload variability (5-10 minutes)</li> <li>Use balanced aggressiveness</li> </ul>"},{"location":"architecture/load_manager_architecture/#scenario-4-heterogeneous-hardware","title":"Scenario 4: Heterogeneous Hardware","text":"<p>Situation: Cluster has mix of hardware: 2 high-memory brokers, 3 standard brokers.</p> <p>Without Intelligence:</p> <ul> <li>Fair strategy distributes topics evenly</li> <li>High-memory brokers are underutilized</li> <li>Standard brokers may run out of memory</li> </ul> <p>With Balanced Strategy:</p> <ul> <li>Memory usage is factored into scoring (35% weight)</li> <li>High-memory brokers receive more topics</li> <li>Standard brokers stay within capacity</li> <li>Resources are optimized</li> </ul> <p>Best Practice:</p> <ul> <li>Always use balanced or weighted_load for heterogeneous clusters</li> <li>Monitor per-broker resource utilization</li> <li>Adjust load_report_interval if needed (lower = more responsive)</li> </ul>"},{"location":"architecture/load_manager_architecture/#troubleshooting","title":"Troubleshooting","text":""},{"location":"architecture/load_manager_architecture/#rebalancing-not-triggering","title":"Rebalancing Not Triggering","text":"<p>Symptoms:</p> <ul> <li>CV is high but no moves occur</li> <li>Logs show \"cluster is balanced\"</li> </ul> <p>Possible Causes:</p> <ol> <li>Rebalancing disabled: Check <code>rebalancing.enabled: true</code></li> <li>Not leader: Only leader executes rebalancing</li> <li>CV below threshold: Increase aggressiveness or lower threshold</li> <li>Too few brokers: Check <code>min_brokers_for_rebalance</code></li> <li>All topics blacklisted: Review blacklist patterns</li> </ol>"},{"location":"architecture/load_manager_architecture/#topics-moving-too-frequently","title":"Topics Moving Too Frequently","text":"<p>Symptoms:</p> <ul> <li>Topics move multiple times per hour</li> <li>Cluster never stabilizes</li> <li>High metadata store traffic</li> </ul> <p>Possible Causes:</p> <ol> <li>Cooldown too short: Increase <code>cooldown_seconds</code></li> <li>Threshold too low: Use less aggressive mode</li> <li>Topic age check disabled: Set <code>min_topic_age_seconds &gt; 0</code></li> <li>Workload is truly unstable</li> </ol>"},{"location":"architecture/load_manager_architecture/#imbalance-after-rebalancing","title":"Imbalance After Rebalancing","text":"<p>Symptoms:</p> <ul> <li>Rebalancing executes but CV remains high</li> <li>Moves don't reduce imbalance</li> </ul> <p>Possible Causes:</p> <ol> <li>Moving wrong topics (heavy topics not moving)</li> <li>New traffic creating imbalance faster than moves fix it</li> <li>Assignment strategy doesn't match rebalancing goals</li> </ol> <p>Solutions:</p> <ul> <li>Review topic load scores (ensure heaviest topics are candidates)</li> <li>Increase <code>max_moves_per_hour</code></li> <li>Switch to weighted_load strategy</li> <li>Check for blacklisted heavy topics</li> </ul>"},{"location":"architecture/load_manager_architecture/#summary","title":"Summary","text":"<p>The Danube Load Manager is a sophisticated system that keeps broker clusters healthy and balanced automatically. By combining intelligent topic assignment with proactive rebalancing, it ensures:</p> <ul> <li>Optimal resource utilization - No broker is overloaded or idle</li> <li>High availability - Automatic failover when brokers crash</li> <li>Performance consistency - Even load distribution prevents hotspots</li> <li>Operational simplicity - Self-optimizing cluster reduces manual intervention</li> <li>Scalability - Easy to add/remove brokers without manual rebalancing</li> </ul> <p>The system is production-ready with multiple safety mechanisms, configurable strategies, and comprehensive monitoring. Start with defaults (balanced strategy, disabled rebalancing) and gradually tune based on your workload characteristics.</p>"},{"location":"architecture/persistence/","title":"Danube Persistence Architecture (WAL + Cloud)","text":"<p>Danube's persistence layer has been recently revamped to make the platform cloud\u2011native, aiming for sub\u2011second dispatch with cloud economics. </p> <p>Danube uses a cloud\u2011native persistence architecture that combines a local Write\u2011Ahead Log (WAL) for the hot path with background uploads to cloud object storage. This keeps publish/dispatch latency low while providing durable, elastic, and cost\u2011effective storage in the cloud.</p> <p>This page explains the main concepts, how the system works, why it scales in cloud environments, and how to configure it for major providers (S3, GCS, Azure). Danube relies on OpenDAL as a storage abstraction, so it can be plugged into any storage backend.</p>"},{"location":"architecture/persistence/#key-benefits","title":"Key Benefits","text":"<ul> <li>Fast hot path: Producers append to a local WAL and consumers read from memory/WAL; no remote writes on the critical path.</li> <li>Cloud-native durability: A background uploader persists WAL frames to cloud object storage.</li> <li>Seamless reader experience: Readers transparently stream historical data from cloud, then live data from WAL.</li> <li>Cost-effective and elastic: Object storage provides massive scale, lifecycle policies, and low TCO.</li> <li>Portable by design: Built on OpenDAL, enabling S3, GCS, Azure Blob, local FS, memory, and more.</li> </ul>"},{"location":"architecture/persistence/#high-level-architecture","title":"High-level Architecture","text":"<ul> <li>Local WAL: Append-only log with an in-memory cache for ultra-fast reads. Files periodically fsync and rotate.</li> <li>Cloud Uploader: Periodically streams complete WAL frames to cloud objects. Writes object descriptors and sparse indexes to ETCD.</li> <li>Cloud Reader: Reads historical messages from cloud using ETCD metadata, then hands off to local WAL for live data.</li> <li>ETCD: Tracks object descriptors, offsets, and indexes for efficient range reads.</li> </ul>"},{"location":"architecture/persistence/#core-concepts-and-components","title":"Core Concepts and Components","text":"<p>WalStorageFactory creates per-topic <code>WalStorage</code>, starts one uploader and one deleter (retention) task per topic, and normalizes each topic to its own WAL directory under the configured root.</p> <p>Wal is an append-only log with an in-memory ordered cache; records are framed as <code>[u64 offset][u32 len][u32 crc][bytes]</code> with CRC32 validation, and a background writer batches and fsyncs data, supporting rotation by size and/or time.</p> <p>WalStorage implements the <code>PersistentStorage</code> trait; when creating readers it applies tiered logic to serve from WAL if possible, otherwise transparently chains a Cloud\u2192WAL stream to cover historical then live data.</p> <p>Uploader streams safe frame prefixes from WAL files to cloud, finalizes objects atomically, builds sparse offset indexes for efficient seeks, and operates one object per cycle with resumable checkpoints.</p> <p>CloudReader reads objects referenced in ETCD metadata, uses sparse indexes to seek efficiently to the requested range, and validates each frame by CRC during decoding.</p> <p>Retention/Deleter enforces time/size retention on WAL files after they are safely uploaded to cloud, advancing the WAL <code>start_offset</code> accordingly.</p> <p>For persistent storage implementation details, check the source code.</p>"},{"location":"architecture/persistence/#how-reads-work-cloudwal-handoff","title":"How Reads Work (Cloud\u2192WAL Handoff)","text":"<ol> <li>A subscription requests a reader at a <code>StartPosition</code> (latest or from a specific offset).</li> <li><code>WalStorage</code> checks the WAL\u2019s local <code>start_offset</code>.</li> <li>If <code>start_offset</code> is older than requested, it:    Streams historical range from cloud objects via <code>CloudReader</code>.    Seamlessly chains into WAL tail for fresh/live data.</li> <li>Consumers see a single ordered stream with no gaps or duplicates.</li> </ol>"},{"location":"architecture/persistence/#configuration-overview","title":"Configuration Overview","text":"<p>Broker configuration is under the <code>wal_cloud</code> section of <code>config/danube_broker.yml</code>:</p> <pre><code>wal_cloud:\n  wal:\n    dir: \"./danube-data/wal\"\n    file_name: \"wal.log\"\n    cache_capacity: 1024\n    file_sync:\n      interval_ms: 5000         # fsync cadence (checkpoint frequency)\n      max_batch_bytes: 10485760 # 10 MiB write batch\n    rotation:\n      max_bytes: 536870912      # 512 MiB WAL file size\n      # max_hours: 24           # optional time-based rotation\n    retention:\n      time_minutes: 2880        # prune locally after 48h\n      size_mb: 20480            # or when size exceeds 20 GiB\n      check_interval_minutes: 5 # deleter tick interval\n\n  uploader:\n    enabled: true\n    interval_seconds: 300       # one upload cycle every 5 minutes\n    root_prefix: \"/danube-data\"\n    max_object_mb: 1024         # optional cap per object\n\n  # Cloud storage backend - MinIO S3 configuration\n  cloud:\n    backend: \"s3\"\n    root: \"s3://danube-messages/cluster-data\"  # S3 bucket and prefix\n    region: \"us-east-1\"\n    endpoint: \"http://minio:9000\"     # MinIO endpoint\n    access_key: \"minioadmin\"         # From environment variable\n    secret_key: \"minioadmin123\"     # From environment variable\n    anonymous: false\n    virtual_host_style: false\n\n  metadata:\n    etcd_endpoint: \"127.0.0.1:2379\"\n    in_memory: false\n</code></pre> <ul> <li>WAL: local durability + replay cache for hot reads.</li> <li>Uploader: cadence and object sizing knobs.</li> <li>Cloud: provider/backend specific settings (see below).</li> <li>Metadata: ETCD endpoint used to store object descriptors and indexes.</li> </ul>"},{"location":"architecture/persistence/#provider-examples-opendal-powered","title":"Provider Examples (OpenDAL-powered)","text":"<p>Danube uses OpenDAL under the hood. Switch providers by changing <code>wal_cloud.cloud</code>.</p> <ul> <li>Amazon S3</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"s3\"\n    root: \"s3://my-bucket/prefix\"\n    region: \"us-east-1\"\n    endpoint: \"https://s3.us-east-1.amazonaws.com\"   # optional\n    access_key: \"${AWS_ACCESS_KEY_ID}\"               # or depend on env/IMDS\n    secret_key: \"${AWS_SECRET_ACCESS_KEY}\"\n    profile: null\n    role_arn: null\n    session_token: null\n    anonymous: false\n</code></pre> <ul> <li>Google Cloud Storage (GCS)</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"gcs\"\n    root: \"gcs://my-bucket/prefix\"\n    project: \"my-gcp-project\"\n    credentials_json: null                 # inline JSON string\n    credentials_path: \"/path/to/creds.json\" # or path to file\n</code></pre> <ul> <li>Azure Blob Storage</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"azblob\"\n    root: \"my-container/prefix\"                       # container[/prefix]\n    endpoint: \"https://&lt;account&gt;.blob.core.windows.net\" # or Azurite endpoint\n    account_name: \"&lt;account_name&gt;\"\n    account_key: \"&lt;account_key&gt;\"\n</code></pre> <ul> <li>In-memory (development)</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"memory\"\n    root: \"mem-prefix\" # namespace-only; persisted in memory\n</code></pre> <ul> <li>Local filesystem</li> </ul> <pre><code>wal_cloud:\n  cloud:\n    backend: \"fs\"\n    root: \"./object_root\"   # local directory for persisted objects\n</code></pre>"},{"location":"architecture/persistence/#tuning-and-best-practices","title":"Tuning and Best Practices","text":"<ul> <li>WAL cache size (<code>wal.cache_capacity</code>): increase for higher consumer hit rates; memory-bound.</li> <li>Flush cadence (<code>wal.file_sync.interval_ms</code>): smaller values improve checkpoint freshness but increase fsync pressure.</li> <li>Rotation thresholds (<code>wal.rotation.*</code>): tune file sizes for smoother uploads and operational hygiene.</li> <li>Uploader interval (<code>uploader.interval_seconds</code>): shorter intervals reduce RPO and speed up historical availability in cloud.</li> <li>Object size (<code>uploader.max_object_mb</code>): bigger objects reduce listing overhead; </li> <li>Retention: ensure local retention is safely larger than upload interval so deleter never prunes data not yet uploaded.</li> <li>Credentials: prefer environment-based credentials for cloud providers</li> </ul>"},{"location":"architecture/persistence/#operational-notes","title":"Operational Notes","text":"<ul> <li>At-least-once delivery: With reliable topics, dispatch uses the WAL to guarantee at-least-once delivery; cloud persistence is asynchronous and does not block producers/consumers.</li> <li>Resilience: Uploader uses precise checkpoints <code>(file_seq, byte_pos)</code> and never re-uploads confirmed bytes; CloudReader validates CRCs.</li> <li>Observability: Checkpoints and rotation metadata are stored under the per-topic WAL directory; ETCD keeps object descriptors.</li> <li>Extensibility: Because Danube uses OpenDAL, adding a new backend typically means adding backend-specific options in config; no broker code changes needed.</li> </ul>"},{"location":"architecture/reliable_topic_move/","title":"Reliable Topic Workflow: Understanding Message Flow During Broker Moves","text":""},{"location":"architecture/reliable_topic_move/#document-purpose","title":"Document Purpose","text":"<p>This document explains how Danube's reliable topics work under the hood, with a focus on what happens when a topic is moved from one broker to another.</p>"},{"location":"architecture/reliable_topic_move/#topic-moves-why-and-when","title":"Topic Moves: Why and When","text":"<p>A topic may be moved between brokers for several reasons:</p> <ol> <li>Manual Load Balancing (Current): An operator uses <code>danube-admin-cli topics unload</code> to explicitly move a topic</li> <li>Automated Load Balancing (Future): The LoadManager detects imbalance and automatically reassigns topics</li> <li>Broker Maintenance: A broker needs to be taken offline for upgrades or repairs</li> <li>Failure Recovery: A broker crashes and topics need to be reassigned</li> </ol> <p>In all cases, the underlying mechanics are the same: the topic is unloaded from one broker and loaded onto another while preserving message offset continuity and consumer progress.</p>"},{"location":"architecture/reliable_topic_move/#core-concepts","title":"Core Concepts","text":"<p>Before diving into the workflow, let's establish key concepts:</p> <ul> <li>WAL (Write-Ahead Log): Local, per-topic storage on each broker for recent messages</li> <li>Cloud Storage: Long-term archival storage for historical messages (S3-compatible)</li> <li>Offset: A monotonically increasing ID (0, 1, 2, ...) assigned to each message in a topic</li> <li>Sealed State: Metadata written to ETCD when a topic is unloaded, capturing the last committed offset</li> <li>Subscription Cursor: Tracks which message a consumer last acknowledged</li> <li>Tiered Reading: Strategy where consumers read old messages from cloud, new messages from WAL</li> </ul> <p>The critical guarantee: Offsets must be globally unique and continuous for a topic, regardless of which broker hosts it.</p>"},{"location":"architecture/reliable_topic_move/#normal-operation-single-broker","title":"Normal Operation (Single Broker)","text":"<p>When a topic is hosted on a single broker with no moves, the workflow is straightforward. This section establishes the baseline behavior that topic moves must preserve.</p>"},{"location":"architecture/reliable_topic_move/#message-production-flow","title":"Message Production Flow","text":"<pre><code>Producer sends message\n    \u2193\nTopic.append_to_storage()\n    \u2193\nWAL.append(msg)\n    \u251c\u2500&gt; Assign offset: next_offset.fetch_add(1)  // e.g., 0, 1, 2, 3...\n    \u251c\u2500&gt; Store in cache: cache.insert(offset, msg)\n    \u251c\u2500&gt; Write to disk: Writer queue (async fsync)\n    \u2514\u2500&gt; Broadcast: tx.send(offset, msg)  // for live consumers\n    \u2193\nUploader (background task, every 30s)\n    \u251c\u2500&gt; Read WAL files\n    \u251c\u2500&gt; Stream frames to cloud storage\n    \u2514\u2500&gt; Write object descriptor to ETCD\n        \u2022 /storage/topics/default/topic/objects/00000000000000000000\n        \u2022 {start_offset: 0, end_offset: 21, object_id: \"data-0-21.dnb1\"}\n</code></pre> <p>What's happening here?</p> <p>Each message produced to a topic goes through a three-stage pipeline:</p> <ol> <li>Offset Assignment: The WAL atomically increments <code>next_offset</code> and assigns it to the message. This offset is the message's permanent ID.</li> <li>Local Persistence: The message is written to the WAL (both in-memory cache and on-disk log) and broadcast to live consumers via a channel.</li> <li>Cloud Upload: A background uploader periodically batches WAL segments and uploads them to cloud storage, writing object descriptors to ETCD for later retrieval.</li> </ol> <p>The key insight: The offset is assigned by the WAL on the hosting broker. When a topic moves, the new broker must continue the offset sequence from where the old broker left off.</p>"},{"location":"architecture/reliable_topic_move/#message-consumption-flow","title":"Message Consumption Flow","text":"<pre><code>Consumer subscribes\n    \u2193\nSubscriptionEngine.init_stream_from_progress_or_latest()\n    \u251c\u2500&gt; Check ETCD for cursor: /topics/.../subscriptions/sub_name/cursor\n    \u251c\u2500&gt; If exists: start_offset = cursor_value (e.g., 6)\n    \u2514\u2500&gt; If not exists: start_offset = latest\n    \u2193\nWalStorage.create_reader(start_offset=6)\n    \u251c\u2500&gt; Get WAL checkpoint: wal_start_offset = 0\n    \u251c\u2500&gt; Compare: 6 &gt;= 0 (within WAL retention)\n    \u2514\u2500&gt; Return: WAL.tail_reader(from=6, live=false)\n        \u251c\u2500&gt; Replay from cache/files: offsets 6, 7, 8...\n        \u2514\u2500&gt; Switch to live: broadcast channel for new messages\n    \u2193\nDispatcher.poll_next()\n    \u251c\u2500&gt; Read message from stream\n    \u251c\u2500&gt; Send to consumer\n    \u2514\u2500&gt; Wait for ACK\n    \u2193\nOn ACK received:\n    \u251c\u2500&gt; SubscriptionEngine.on_acked(offset)\n    \u2514\u2500&gt; Periodically flush cursor to ETCD (every 1000 acks or 5s)\n</code></pre> <p>What's happening here?</p> <p>Consumers track their progress through a topic using a cursor (subscription offset) stored in ETCD:</p> <ol> <li>Initialization: When a consumer subscribes, the subscription engine checks ETCD for an existing cursor. If found, it resumes from that offset; otherwise, it starts from the latest.</li> <li>Tiered Reading: The <code>WalStorage</code> determines where to read messages from:</li> <li>If the requested offset is within the WAL's retention range, read directly from WAL</li> <li>If older, read from cloud storage first, then chain to WAL for newer messages</li> <li>Progress Tracking: As messages are acknowledged, the cursor advances and is periodically persisted to ETCD</li> </ol> <p>This cursor-based design is broker-agnostic: it doesn't matter which broker hosts the topic, as long as the offset space is continuous.</p>"},{"location":"architecture/reliable_topic_move/#state-at-rest-single-broker","title":"State at Rest (Single Broker)","text":"<p>Broker Memory:</p> <pre><code>WAL:\n  next_offset: 22        \u2190 Ready for next message\n  cache: [17..21]        \u2190 Recent messages (capacity=1024)\n\nUploader:\n  checkpoint: 21         \u2190 Last uploaded offset\n\nSubscription \"subs_reliable\":\n  cursor_in_memory: 13   \u2190 Last acked by consumer\n</code></pre> <p>ETCD:</p> <pre><code>/topics/default/reliable_topic/delivery: \"Reliable\"\n/topics/default/reliable_topic/subscriptions/subs_reliable/cursor: 13\n/storage/topics/default/reliable_topic/objects/00000000000000000000:\n  {start_offset: 0, end_offset: 21, object_id: \"data-0-21.dnb1\"}\n</code></pre> <p>Cloud Storage:</p> <pre><code>s3://bucket/default/reliable_topic/data-0-21.dnb1\n  Contains: Binary frames for offsets 0-21\n</code></pre>"},{"location":"architecture/reliable_topic_move/#topic-move-workflow-with-fix","title":"Topic Move Workflow (With Fix)","text":"<p>This section describes the complete flow when a topic is moved from Broker A to Broker B. The move can be triggered in two ways:</p> <ol> <li>Manual (Current): An operator runs <code>danube-admin-cli topics unload /default/reliable_topic</code></li> <li>Automated: The LoadManager detects load imbalance and triggers reassignment programmatically</li> </ol> <p>Regardless of trigger, the mechanics are identical. The critical requirement is to preserve offset continuity so that:</p> <ul> <li>Producers on the new broker continue assigning offsets from where the old broker left off</li> <li>Consumers can read the entire message history seamlessly across both brokers</li> <li>No message is lost, duplicated, or assigned a conflicting offset</li> </ul> <p>The fix introduced a sealed state mechanism that captures the last committed offset when unloading and uses it to initialize the WAL on the new broker.</p>"},{"location":"architecture/reliable_topic_move/#step-1-unload-from-broker-a","title":"Step 1: Unload from Broker A","text":"<p>Trigger: Either <code>danube-admin-cli topics unload /default/reliable_topic</code> or automated LoadManager decision</p> <pre><code>BrokerService.topic_cluster.post_unload_topic()\n    \u2193\nCreate unload marker in ETCD\n    \u2022 /cluster/unassigned/default/reliable_topic\n    \u2022 {reason: \"unload\", from_broker: 10285063371164059634}\n    \u2193\nDelete broker assignment\n    \u2022 /cluster/brokers/10285063371164059634/default/reliable_topic\n    \u2193\nBroker A watcher sees DELETE event\n    \u2193\nTopicManager.unload_reliable_topic()\n    \u251c\u2500&gt; Flush subscription cursors to ETCD\n    \u2502   \u2022 /topics/.../subscriptions/subs_reliable/cursor: 13\n    \u2502\n    \u251c\u2500&gt; WalFactory.flush_and_seal()\n    \u2502   \u251c\u2500&gt; WAL.flush() \u2192 fsync all pending writes\n    \u2502   \u2502\n    \u2502   \u251c\u2500&gt; Uploader: cancel and drain\n    \u2502   \u2502   \u251c\u2500&gt; Upload remaining frames to cloud\n    \u2502   \u2502   \u2514\u2500&gt; Update ETCD descriptors\n    \u2502   \u2502       \u2022 objects/00000000000000000000: end_offset=21\n    \u2502   \u2502\n    \u2502   \u2514\u2500&gt; Write sealed state to ETCD \u2705\n    \u2502       \u2022 /storage/topics/default/reliable_topic/state\n    \u2502       \u2022 {\n    \u2502           sealed: true,\n    \u2502           last_committed_offset: 21,  \u2190 CRITICAL!\n    \u2502           broker_id: 10285063371164059634,\n    \u2502           timestamp: 1768625254\n    \u2502         }\n    \u2502\n    \u2514\u2500&gt; Delete local WAL files\n        \u2022 rm ./danube-data/wal/default/reliable_topic/*\n</code></pre> <p>Broker A Final State:</p> <ul> <li>\u274c No local WAL files</li> <li>\u2705 All messages 0-21 in cloud storage</li> <li>\u2705 Subscription cursor at 13 in ETCD</li> <li>\u2705 Sealed state at 21 in ETCD</li> </ul> <p>What just happened?</p> <p>The unload process ensures data durability and captures critical state:</p> <ol> <li>Flush Everything: All pending writes are committed, and the uploader drains its queue to ensure all messages reach cloud storage</li> <li>Write Sealed State: The broker writes <code>{sealed: true, last_committed_offset: 21}</code> to ETCD. This is the key piece of state that the new broker will use to initialize its WAL correctly.</li> <li>Clean Up Local State: The WAL files are deleted since they're now in cloud storage, and the topic is removed from the broker</li> </ol> <p>At this point, Broker A no longer owns the topic, but all messages 0-21 are safely stored in the cloud, and the metadata in ETCD contains everything needed to restore the topic elsewhere.</p>"},{"location":"architecture/reliable_topic_move/#step-2-assign-to-broker-b","title":"Step 2: Assign to Broker B","text":"<p>LoadManager assigns topic to Broker B (cluster decides which broker gets it based on load)</p> <pre><code>Write to ETCD:\n    \u2022 /cluster/brokers/10421046117770015389/default/reliable_topic: null\n    \u2193\nBroker B watcher sees PUT event\n    \u2193\nTopicManager.ensure_local(\"/default/reliable_topic\")\n    \u2193\nWalFactory.for_topic(\"default/reliable_topic\")\n    \u2193\nget_or_create_wal(\"default/reliable_topic\")\n    \u2502\n    \u251c\u2500&gt; \u2705 NEW: Check sealed state\n    \u2502   EtcdMetadata.get_storage_state_sealed(\"default/reliable_topic\")\n    \u2502   Returns: {sealed: true, last_committed_offset: 21, ...}\n    \u2502\n    \u251c\u2500&gt; \u2705 NEW: Calculate initial offset\n    \u2502   initial_offset = 21 + 1 = 22\n    \u2502\n    \u251c\u2500&gt; \u2705 NEW: Create CheckpointStore (empty on new broker)\n    \u2502   wal.ckpt: None\n    \u2502   uploader.ckpt: None\n    \u2502\n    \u2514\u2500&gt; \u2705 NEW: Create WAL with initial offset\n        Wal::with_config_with_store(cfg, ckpt_store, Some(22))\n        \u2514\u2500&gt; next_offset: AtomicU64::new(22)  \u2190 Continues from 22!\n        \u2193\n    Start Uploader\n    \u251c\u2500&gt; Check checkpoint: None (new broker)\n    \u251c\u2500&gt; \u2705 Create initial checkpoint from sealed state\n    \u2502   uploader_checkpoint: {last_committed_offset: 21, ...}\n    \u2514\u2500&gt; Ready to upload from offset 22+\n</code></pre> <p>Broker B Initial State:</p> <pre><code>WAL:\n  next_offset: 22        \u2705 Continues where A left off\n  cache: []              (empty initially)\n\nUploader:\n  checkpoint: 21         \u2705 From sealed state\n\nSubscription \"subs_reliable\":\n  Not loaded yet (no consumer connected)\n</code></pre> <p>What just happened?</p> <p>This is the critical fix that prevents offset collisions:</p> <ol> <li>Sealed State Discovery: When Broker B loads the topic, it checks ETCD for a sealed state marker</li> <li>Offset Calculation: Finding <code>last_committed_offset: 21</code>, it calculates <code>initial_offset = 22</code></li> <li>WAL Initialization: The WAL is created with <code>next_offset = 22</code> instead of the default <code>0</code></li> <li>Uploader Restoration: The uploader checkpoint is set to 21, so it knows messages 0-21 are already in cloud storage</li> </ol>"},{"location":"architecture/reliable_topic_move/#step-3-producer-reconnects-and-sends-messages","title":"Step 3: Producer Reconnects and Sends Messages","text":"<p>Producer automatic reconnection: The Danube client detects the topic has moved and automatically reconnects to Broker B</p> <pre><code>Producer connects to Broker B\n    \u2193\nSends 6 new messages (message IDs 2-7 in producer)\n    \u2193\nBroker B assigns offsets:\n    WAL.append(msg_2) \u2192 offset 22 \u2705\n    WAL.append(msg_3) \u2192 offset 23 \u2705\n    WAL.append(msg_4) \u2192 offset 24 \u2705\n    WAL.append(msg_5) \u2192 offset 25 \u2705\n    WAL.append(msg_6) \u2192 offset 26 \u2705\n    WAL.append(msg_7) \u2192 offset 27 \u2705\n    \u2193\nMessages stored:\n    \u251c\u2500&gt; WAL cache: [22, 23, 24, 25, 26, 27]\n    \u251c\u2500&gt; WAL file: ./danube-data/wal/default/reliable_topic/wal.log\n    \u2514\u2500&gt; Broadcast: (for live consumers)\n    \u2193\nUploader runs (30s later):\n    \u251c\u2500&gt; Stream offsets 22-27 from WAL files\n    \u251c\u2500&gt; Upload to cloud: data-22-27.dnb1\n    \u2514\u2500&gt; Write to ETCD:\n        \u2022 objects/00000000000000000022:\n          {start_offset: 22, end_offset: 27, ...}\n</code></pre> <p>State After Production:</p> <pre><code>Cloud Storage:\n  data-0-21.dnb1    (from Broker A)\n  data-22-27.dnb1   (from Broker B) \u2705 Continuous!\n\nETCD Objects:\n  /storage/.../objects/00000000000000000000: {start: 0, end: 21}\n  /storage/.../objects/00000000000000000022: {start: 22, end: 27} \u2705\n\nWAL on Broker B:\n  next_offset: 28\n  cache: [22, 23, 24, 25, 26, 27]\n</code></pre> <p>What just happened?</p> <p>The producer has no awareness that the topic moved\u2014from its perspective, it's just producing messages as normal:</p> <ol> <li>Client Routing: The Danube client queries ETCD for the topic's current broker assignment and connects to Broker B</li> <li>Continuous Offsets: Messages are assigned offsets 22-27, which is exactly what we want\u2014a continuation from Broker A's 0-21</li> <li>Cloud Upload: After 30 seconds (or when the segment is large enough), the uploader streams offsets 22-27 to cloud storage</li> </ol> <p>The offset space is continuous across both brokers: <code>[0..21] (Broker A) + [22..27] (Broker B)</code>. This is the foundation for consumers to work correctly.</p>"},{"location":"architecture/reliable_topic_move/#step-4-consumer-reconnects","title":"Step 4: Consumer Reconnects","text":"<p>Consumer automatic reconnection: The Danube client detects the topic has moved and reconnects to Broker B with the same subscription</p> <pre><code>Consumer subscribes to \"subs_reliable\"\n    \u2193\nSubscriptionEngine.init_stream_from_progress_or_latest()\n    \u251c\u2500&gt; Read cursor from ETCD: 13\n    \u2514\u2500&gt; start_offset = 14 (next unacked message)\n    \u2193\nWalStorage.create_reader(start_offset=14)\n    \u2502\n    \u251c\u2500&gt; Get WAL checkpoint on Broker B:\n    \u2502   wal_start_offset = 22 (WAL only has 22+)\n    \u2502\n    \u251c\u2500&gt; Compare: 14 &lt; 22 (requested offset is OLDER than WAL)\n    \u2502   \n    \u2514\u2500&gt; \u2705 Tiered Reading Strategy:\n        \u2502\n        \u251c\u2500&gt; Step 1: Read from CLOUD (14-21)\n        \u2502   CloudReader.read_range(14, 21)\n        \u2502   \u251c\u2500&gt; Query ETCD: get objects covering 14-21\n        \u2502   \u2502   Returns: objects/00000000000000000000 (0-21)\n        \u2502   \u251c\u2500&gt; Download: data-0-21.dnb1\n        \u2502   \u251c\u2500&gt; Extract frames: offsets 14-21\n        \u2502   \u2514\u2500&gt; Stream: [msg_14, msg_15, ..., msg_21]\n        \u2502\n        \u2514\u2500&gt; Step 2: Chain to WAL (22+)\n            WAL.tail_reader(22, false)\n            \u251c\u2500&gt; Read from cache: [22, 23, 24, 25, 26, 27]\n            \u2514\u2500&gt; Stream: [msg_22, msg_23, ..., msg_27]\n            \u2514\u2500&gt; Then switch to live broadcast for future messages\n    \u2193\nConsumer receives continuous stream:\n    [14, 15, 16, 17, 18, 19, 20, 21] \u2190 from cloud\n    [22, 23, 24, 25, 26, 27]         \u2190 from WAL\n    \u2705 No gaps, no duplicates!\n</code></pre> <p>What just happened?</p> <p>This is where the magic of tiered reading shines:</p> <ol> <li>Cursor Resume: The consumer's subscription cursor was at 13 (last ACKed message), so it resumes from offset 14</li> <li>Storage Decision: The consumer asks for offset 14, but Broker B's WAL only has offsets 22+. The system detects this gap.</li> <li>Cloud Fallback: For offsets 14-21, the reader automatically fetches data from cloud storage (uploaded by Broker A)</li> <li>WAL Handoff: Once caught up to offset 22, the reader seamlessly switches to reading from Broker B's local WAL</li> <li>Continuous Stream: From the consumer's perspective, it's a single continuous stream of messages,  it has no idea the topic moved!</li> </ol> <p>This works only because offsets are continuous: there's no collision between Broker A's offsets (0-21) and Broker B's offsets (22+).</p>"},{"location":"architecture/reliable_topic_move/#step-5-consumer-acks-and-cursor-updates","title":"Step 5: Consumer ACKs and Cursor Updates","text":"<pre><code>Consumer processes and ACKs messages:\n    ACK(14) \u2192 ACK(15) \u2192 ... \u2192 ACK(27)\n    \u2193\nSubscriptionEngine.on_acked(offset)\n    \u251c\u2500&gt; Update in-memory cursor: 27\n    \u2514\u2500&gt; Batch flush to ETCD (every 1000 acks or 5s):\n        \u2022 /topics/.../subscriptions/subs_reliable/cursor: 27\n</code></pre>"},{"location":"architecture/reliable_topic_move/#key-mechanisms-ensuring-continuity","title":"Key Mechanisms Ensuring Continuity","text":""},{"location":"architecture/reliable_topic_move/#1-offset-continuity-via-sealed-state","title":"1. Offset Continuity via Sealed State","text":"<pre><code>Broker A: Messages 0-21\n    \u2193 (sealed state: last=21)\nBroker B: Messages 22+ \u2705 Continuous!\n</code></pre>"},{"location":"architecture/reliable_topic_move/#2-consumer-read-strategy-tiered","title":"2. Consumer Read Strategy (Tiered)","text":"<pre><code>Consumer wants offset X\n    \u2193\nIs X in WAL range?\n    YES \u2192 Read from WAL only\n    NO  \u2192 Read from Cloud, then chain to WAL\n</code></pre>"},{"location":"architecture/reliable_topic_move/#3-uploader-state-preservation","title":"3. Uploader State Preservation","text":"<pre><code>Old Broker:\n  Uploader checkpoint: 21\n    \u2193 (written to sealed state)\nNew Broker:\n  Uploader checkpoint: 21 (from sealed state)\n  Won't re-upload 0-21 \u2705\n  Will upload 22+ \u2705\n</code></pre>"},{"location":"architecture/reliable_topic_move/#4-subscription-cursor-independence","title":"4. Subscription Cursor Independence","text":"<pre><code>Subscription cursor in ETCD: 13\n    \u2193 (persisted independently)\nConsumer reads from 14 regardless of which broker\n    \u2705 Works because offset space is continuous\n</code></pre>"},{"location":"architecture/reliable_topic_move/#complete-timeline-example","title":"Complete Timeline Example","text":"Time Event Broker A Broker B Cloud Consumer T1 Produce msgs Offsets 0-21 - - - T2 Upload - - 0-21 stored - T3 Consumer reads - - - Reads 0-13, cursor=13 T4 UNLOAD Seal state (21) - State saved - T5 Assign to B - Load, WAL=22 \u2705 - - T6 Produce msgs - Offsets 22-27 \u2705 - - T7 Upload - - 22-27 stored - T8 Consumer reconnect - - - Reads 14-21 (cloud) T9 Continue read - - - Reads 22-27 (WAL) \u2705 T10 ACK all - - - Cursor=27 \u2705"},{"location":"architecture/reliable_topic_move/#conclusion","title":"Conclusion","text":"<p>The reliable topic workflow is designed to be broker-agnostic: a topic can be moved between brokers without any impact on producers or consumers, as long as offset continuity is preserved.</p> <p>The sealed state mechanism is the linchpin that enables this:</p> <ul> <li>Old broker captures <code>last_committed_offset</code> when unloading</li> <li>New broker reads this state and initializes its WAL from <code>last_committed_offset + 1</code></li> <li>Offset space remains continuous across the move</li> <li>Consumers read seamlessly from cloud (old messages) and WAL (new messages)</li> </ul> <p>The architecture is built to handle topic mobility at scale, making Danube suitable for cloud-native deployments where resources are constantly shifting.</p>"},{"location":"architecture/schema_registry_architecture/","title":"Danube Schema Registry Architecture","text":"<p>Danube's Schema Registry is a centralized service that manages message schemas with versioning, compatibility checking, and validation capabilities. It ensures data quality, enables safe schema evolution, and provides a governance layer for all messages flowing through the Danube messaging system.</p> <p>This page explains what the Schema Registry is, why it's essential for Danube messaging systems, how it works at a high level, and how to interact with it through APIs and CLI tools.</p>"},{"location":"architecture/schema_registry_architecture/#what-is-a-schema-registry","title":"What is a Schema Registry?","text":"<p>A Schema Registry is a standalone service that stores, versions, and validates schemas, the contracts that define the structure of the messages. Instead of each topic managing its own schema independently, the Schema Registry provides:</p> <ul> <li>Centralized schema management - Single source of truth for all schemas across the messaging infrastructure</li> <li>Schema versioning - Track changes over time with automatic version management</li> <li>Compatibility enforcement - Prevent breaking changes that could crash consumers</li> <li>Schema reuse - Share schemas across multiple topics to reduce duplication</li> <li>Data governance - Audit who created schemas, when they changed, and track dependencies</li> </ul> <p>Think of it as a \"contract repository\" where producers and consumers agree on message structure, ensuring everyone speaks the same language.</p>"},{"location":"architecture/schema_registry_architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Producer   \u2502         \u2502  Schema Registry \u2502         \u2502   Consumer   \u2502\n\u2502             \u2502         \u2502                  \u2502         \u2502              \u2502\n\u2502 1. Register \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502  \u2022 Store schemas \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 4. Fetch     \u2502\n\u2502    Schema   \u2502         \u2502  \u2022 Version ctrl  \u2502         \u2502    Schema    \u2502\n\u2502             \u2502         \u2502  \u2022 Validate      \u2502         \u2502              \u2502\n\u2502 2. Get ID   \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2022 Check compat  \u2502         \u2502 5. Deserialize\u2502\n\u2502             \u2502         \u2502                  \u2502         \u2502    Messages   \u2502\n\u2502 3. Send msg \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502              \u2502\n\u2502  (with ID)  \u2502                  \u2502                   \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                         \u2502                          \u2502\n       \u2502                    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510                     \u2502\n       \u2502                    \u2502  ETCD   \u2502                     \u2502\n       \u2502                    \u2502Metadata \u2502                     \u2502\n       \u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n       \u2502                                                    \u2502\n       \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502  Danube Broker   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502                  \u2502\n                 \u2502 \u2022 Route messages \u2502\n                 \u2502 \u2022 Validate IDs   \u2502\n                 \u2502 \u2022 Enforce policy \u2502\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/schema_registry_architecture/#component-roles","title":"Component Roles","text":"<p>Schema Registry Service Standalone gRPC service managing all schema operations. Handles registration, retrieval, versioning, and compatibility checking independently from message routing.</p> <p>ETCD Metadata Store Persistent storage for all schema metadata, versions, and compatibility settings. Provides distributed consistency across broker cluster.</p> <p>Danube Broker Enforces schema validation on messages. Checks that message schema IDs match topic requirements before accepting or dispatching messages.</p> <p>Producers Register schemas, serialize messages according to schema, include schema ID in message metadata.</p> <p>Consumers Fetch schemas from registry, deserialize messages using correct schema version, optionally validate data structures.</p>"},{"location":"architecture/schema_registry_architecture/#core-concepts","title":"Core Concepts","text":""},{"location":"architecture/schema_registry_architecture/#subjects","title":"Subjects","text":"<p>A subject is a named container for schema versions. Typically, one subject corresponds to one message type (e.g., <code>user-events</code>, <code>payment-transactions</code>).</p> <ul> <li>Subjects enable multiple topics to share the same schema</li> <li>Subject names follow your naming conventions (commonly matches topic name)</li> <li>Each subject tracks its own version history and compatibility settings</li> </ul>"},{"location":"architecture/schema_registry_architecture/#versions","title":"Versions","text":"<p>Every time you register a schema, it creates a new version if the content differs from existing versions.</p> <ul> <li>Versions start at 1 and increment automatically</li> <li>Versions are immutable once created</li> <li>Full version history is preserved indefinitely</li> <li>Duplicate schemas are detected via fingerprinting (no duplicate versions created)</li> </ul>"},{"location":"architecture/schema_registry_architecture/#compatibility-modes-subject-level","title":"Compatibility Modes (Subject-Level)","text":"<p>Compatibility modes control what schema changes are allowed when registering new versions. This is a subject-level setting, meaning all topics sharing the same subject inherit the same compatibility rules.</p> Mode Description When to Use Example Change Backward New schema can read old data Consumers upgrade before producers Add optional field Forward Old schema can read new data Producers upgrade before consumers Remove optional field Full Both backward and forward Critical schemas needing both directions Only add optional fields None No validation Development/testing only Any change allowed <p>Default: Backward (industry standard, covers 90% of use cases)</p> <p>Each subject has its own compatibility mode, configurable by administrators only.</p>"},{"location":"architecture/schema_registry_architecture/#validation-policies-topic-level","title":"Validation Policies (Topic-Level)","text":"<p>Validation policies control how the broker enforces schema validation for messages on a specific topic. This is a topic-level setting, meaning different topics using the same schema subject can have different validation strictness.</p> Policy Behavior Use Case None No validation Development topics, unstructured data Warn Validate and log errors, but accept messages Monitoring/debugging production issues Enforce Reject invalid messages Production topics requiring strict data quality <p>Default: None</p> <p>Example: A <code>user-events-dev</code> topic might use <code>Warn</code> policy for flexibility, while <code>user-events-prod</code> uses <code>Enforce</code> for strict validation\u2014both sharing the same schema subject.</p>"},{"location":"architecture/schema_registry_architecture/#schema-types","title":"Schema Types","text":"<p>Danube supports multiple schema formats:</p> <ul> <li>JSON Schema - Fully validated and production-ready</li> <li>Avro - Apache Avro format (registration and storage ready)</li> <li>Protobuf - Protocol Buffers (registration and storage ready)</li> <li>String - UTF-8 text validation</li> <li>Number - Numeric types</li> <li>Bytes - Raw binary (no validation)</li> </ul>"},{"location":"architecture/schema_registry_architecture/#topic-schema-binding","title":"Topic-Schema Binding","text":"<p>Every topic can be associated with exactly one schema subject:</p> <ul> <li>One Topic \u2192 One Subject - Each topic uses a single schema subject</li> <li>One Subject \u2192 Many Topics - Multiple topics can share the same schema subject</li> <li>First Producer Privilege - The first producer to create a topic assigns its schema subject</li> <li>Immutable Binding - Once set, only administrators can change a topic's schema subject</li> </ul> <p>Example:</p> <pre><code>SUBJECT: \"user-events-value\"\n  \u251c\u2500 Version 1, 2, 3... (evolves over time)\n  \u251c\u2500 Compatibility: BACKWARD (subject-level)\n  \u2514\u2500 Used by:\n      \u251c\u2500 TOPIC: \"user-events-dev\" (ValidationPolicy: WARN)\n      \u2514\u2500 TOPIC: \"user-events-prod\" (ValidationPolicy: ENFORCE)\n</code></pre>"},{"location":"architecture/schema_registry_architecture/#how-it-works","title":"How It Works","text":""},{"location":"architecture/schema_registry_architecture/#1-schema-registration","title":"1. Schema Registration","text":"<p>When you register a schema:</p> <p>Using CLI:</p> <pre><code>danube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file user-events.json \\\n  --description \"User activity events\"\n</code></pre> <p>Using Rust SDK:</p> <pre><code>use danube_client::{SchemaRegistryClient, SchemaType};\n\nlet mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\n// Register the schema and get schema ID\nlet schema_id = schema_client\n    .register_schema(\"user-events\")\n    .with_type(SchemaType::Avro)\n    .with_schema_data(avro_schema.as_bytes())\n    .execute()\n    .await?;\n\nprintln!(\"\u2705 Registered schema with ID: {}\", schema_id);\n</code></pre> <p>The Schema Registry:</p> <ol> <li>Validates the schema definition (syntax, structure)</li> <li>Checks compatibility with existing versions (if mode != None)</li> <li>Computes fingerprint to detect duplicates</li> <li>Assigns a unique schema ID (global across all subjects)</li> <li>Creates new version number (auto-increment)</li> <li>Stores in ETCD with full metadata (creator, timestamp, description, tags)</li> <li>Returns schema ID and version to client</li> </ol>"},{"location":"architecture/schema_registry_architecture/#2-schema-evolution","title":"2. Schema Evolution","text":"<p>When you update a schema:</p> <p>Using CLI:</p> <pre><code># Test compatibility first\ndanube-admin-cli schemas check user-events \\\n  --file user-events-v2.json \\\n  --schema-type json_schema\n\n# If compatible, register new version\ndanube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file user-events-v2.json\n</code></pre> <p>Using Rust SDK:</p> <pre><code>// Check compatibility before registering\nlet compatibility_result = schema_client\n    .check_compatibility(\n        \"user-events\",\n        schema_v2.as_bytes().to_vec(),\n        SchemaType::Avro,\n        None,\n    )\n    .await?;\n\nif compatibility_result.is_compatible {\n    // Safe to register new version\n    let schema_id_v2 = schema_client\n        .register_schema(\"user-events\")\n        .with_type(SchemaType::Avro)\n        .with_schema_data(schema_v2.as_bytes())\n        .execute()\n        .await?;\n    println!(\"\u2705 Schema v2 registered with ID: {}\", schema_id_v2);\n} else {\n    println!(\"\u274c Schema incompatible: {:?}\", compatibility_result.errors);\n}\n</code></pre> <p>The compatibility checker:</p> <ol> <li>Retrieves latest version for the subject</li> <li>Compares old vs. new schema based on compatibility mode</li> <li>Validates the change is safe (e.g., adding optional field in Backward mode)</li> <li>Rejects incompatible changes with detailed error message</li> <li>If compatible, allows registration as new version</li> </ol>"},{"location":"architecture/schema_registry_architecture/#3-message-production","title":"3. Message Production","text":"<p>Producers reference schemas when sending messages:</p> <p>Using CLI:</p> <pre><code>danube-cli produce \\\n  -t /default/user-events \\\n  --schema-subject user-events \\\n  -m '{\"user_id\": \"123\", \"action\": \"login\"}'\n</code></pre> <p>Using Rust SDK:</p> <pre><code>use danube_client::DanubeClient;\n\n// Create producer with schema reference\nlet mut producer = client\n    .producer()\n    .with_topic(\"/default/user-events\")\n    .with_name(\"user_events_producer\")\n    .with_schema_subject(\"user-events\")  // Links to schema (latest version)\n    .build();\n\nproducer.create().await?;\n\n// Or pin to specific version\nlet mut producer_v2 = client\n    .producer()\n    .with_topic(\"/default/user-events\")\n    .with_name(\"user_events_producer_v2\")\n    .with_schema_version(\"user-events\", 2)  // Pin to version 2\n    .build();\n\nproducer_v2.create().await?;\n\n// Serialize and send message\nlet event = UserEvent { user_id: \"123\", action: \"login\", ... };\nlet avro_data = serde_json::to_vec(&amp;event)?;\n\nlet message_id = producer.send(avro_data, None).await?;\nprintln!(\"\ud83d\udce4 Sent message: {}\", message_id);\n</code></pre> <p>First Producer Privilege:</p> <p>The first producer to create a topic automatically assigns its schema subject to that topic. Subsequent producers must use the same subject:</p> <pre><code>// First producer - assigns schema subject to topic\nlet first = client.producer()\n    .with_topic(\"new-topic\")\n    .with_schema_subject(\"user-events\")  // \u2705 Sets topic's schema\n    .build();\n\n// Second producer - must match\nlet second = client.producer()\n    .with_topic(\"new-topic\")\n    .with_schema_subject(\"user-events\")  // \u2705 Matches, allowed\n    .build();\n\n// Third producer - mismatched subject\nlet third = client.producer()\n    .with_topic(\"new-topic\")\n    .with_schema_subject(\"order-events\")  // \u274c ERROR: Subject mismatch\n    .build();\n</code></pre> <p>The flow:</p> <ol> <li>Producer validates schema subject exists in registry</li> <li>If topic doesn't exist, creates it with the specified schema subject</li> <li>If topic exists, validates subject matches topic's assigned subject</li> <li>Retrieves schema ID from registry (cached locally)</li> <li>Serializes message according to schema (validation happens client-side)</li> <li>Includes schema ID in message metadata (8 bytes overhead)</li> <li>Sends message to broker</li> <li>Broker validates schema ID matches topic's subject</li> <li>Applies topic's validation policy (None/Warn/Enforce)</li> <li>Message is accepted and routed to consumers</li> </ol>"},{"location":"architecture/schema_registry_architecture/#4-message-consumption","title":"4. Message Consumption","text":"<p>Consumers fetch schemas to deserialize messages:</p> <p>Using CLI:</p> <pre><code>danube-cli consume \\\n  -t /default/user-events \\\n  -m my-subscription\n</code></pre> <p>Using Rust SDK:</p> <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SchemaInfo, SubType};\n\n// Create consumer\nlet mut consumer = client\n    .consumer()\n    .with_topic(\"/default/user-events\")\n    .with_consumer_name(\"user_events_consumer\")\n    .with_subscription(\"my-subscription\")\n    .with_subscription_type(SubType::Exclusive)\n    .build();\n\nconsumer.subscribe().await?;\nlet mut message_stream = consumer.receive().await?;\n\n// Create schema client for validation (optional)\nlet mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\n// Receive and deserialize messages\nwhile let Some(message) = message_stream.recv().await {\n    // Option 1: Trust broker validation (most common)\n    let event = serde_json::from_slice::&lt;UserEvent&gt;(&amp;message.payload)?;\n    println!(\"\ud83d\udce5 Received: {:?}\", event);\n\n    // Option 2: Client-side validation (if broker policy is Warn/None)\n    if let Some(schema_id) = message.schema_id {\n        let schema: SchemaInfo = schema_client.get_schema_by_id(schema_id).await?;\n        // Validate payload against schema if needed\n    }\n\n    // Acknowledge\n    consumer.ack(&amp;message).await?;\n}\n</code></pre> <p>The flow:</p> <ol> <li>Consumer receives message with schema ID and version</li> <li>Can fetch schema definition from registry (cached locally) if needed</li> <li>Deserializes message using schema</li> <li>Optional: Client-side validation for extra safety</li> <li>Processes validated message</li> </ol> <p>Consumer Validation Options:</p> <ul> <li>Trust broker - If topic has <code>Enforce</code> policy, no client validation needed</li> <li>Cache + validate - Fetch schemas once, cache locally, validate on consumption</li> <li>Always fetch - Query registry for every message (high latency, not recommended)</li> </ul>"},{"location":"architecture/schema_registry_architecture/#validation-layers","title":"Validation Layers","text":"<p>Danube provides three validation layers for maximum flexibility:</p>"},{"location":"architecture/schema_registry_architecture/#producer-side-validation","title":"Producer-Side Validation","text":"<ul> <li>Applications serialize data according to schema before sending</li> <li>Schema validation happens during serialization</li> <li>Catches errors at source before data enters system</li> <li>Recommended: Always validate at producer</li> </ul>"},{"location":"architecture/schema_registry_architecture/#broker-side-validation-topic-level-policy","title":"Broker-Side Validation (Topic-Level Policy)","text":"<ul> <li>Broker checks message schema ID matches topic's assigned subject</li> <li>Three policy levels: None, Warn, Enforce (configured per topic by admin)</li> <li>Enforce mode: Rejects invalid messages before routing (production)</li> <li>Warn mode: Logs warnings but allows message (monitoring/debugging)</li> <li>None mode: No validation (development only)</li> <li>Controlled by <code>ValidationPolicy</code> (topic-level setting)</li> <li>Configurable independently per topic using same schema subject</li> </ul>"},{"location":"architecture/schema_registry_architecture/#consumer-side-validation","title":"Consumer-Side Validation","text":"<ul> <li>Consumers deserialize messages using schema</li> <li>Optional struct validation at startup to ensure compatibility</li> <li>Prevents runtime deserialization errors</li> <li>Recommended: Validate structs at consumer startup</li> </ul> <p>This multi-layer approach ensures data quality at every stage while maintaining flexibility.</p>"},{"location":"architecture/schema_registry_architecture/#storage-model","title":"Storage Model","text":""},{"location":"architecture/schema_registry_architecture/#etcd-organization","title":"ETCD Organization","text":"<p>Schemas and topic configurations are stored hierarchically in ETCD:</p> <pre><code>/schemas/\n  \u251c\u2500\u2500 {subject}/\n  \u2502   \u251c\u2500\u2500 metadata                    # Subject-level metadata\n  \u2502   \u2502   \u251c\u2500\u2500 compatibility_mode      # BACKWARD/FORWARD/FULL/NONE\n  \u2502   \u2502   \u251c\u2500\u2500 created_at\n  \u2502   \u2502   \u2514\u2500\u2500 created_by\n  \u2502   \u251c\u2500\u2500 compatibility               # Current compatibility mode\n  \u2502   \u2514\u2500\u2500 versions/\n  \u2502       \u251c\u2500\u2500 1                       # Version 1 data\n  \u2502       \u251c\u2500\u2500 2                       # Version 2 data\n  \u2502       \u2514\u2500\u2500 3                       # Version 3 data\n  \u251c\u2500\u2500 _global/\n  \u2502   \u2514\u2500\u2500 next_schema_id              # Global schema ID counter\n  \u2514\u2500\u2500 _index/\n      \u2514\u2500\u2500 by_id/{schema_id}           # Reverse index: ID \u2192 subject\n\n/topics/\n  \u2514\u2500\u2500 {topic_name}/\n      \u2514\u2500\u2500 schema_config               # Topic-level schema settings\n          \u251c\u2500\u2500 subject                 # Schema subject for this topic\n          \u251c\u2500\u2500 validation_policy       # NONE/WARN/ENFORCE\n          \u2514\u2500\u2500 enable_payload_validation  # true/false\n</code></pre>"},{"location":"architecture/schema_registry_architecture/#caching-strategy","title":"Caching Strategy","text":"<p>To optimize performance, Danube uses distributed caching:</p> <ul> <li>Writes go directly to ETCD (source of truth)</li> <li>Reads served from local cache (eventually consistent)</li> <li>Updates propagated via ETCD watch mechanism (automatic invalidation)</li> <li>Schema IDs cached in topics for fast validation (no registry lookup per message)</li> </ul> <p>This pattern ensures cluster-wide consistency while maintaining low-latency reads.</p>"},{"location":"architecture/schema_registry_architecture/#use-cases","title":"Use Cases","text":""},{"location":"architecture/schema_registry_architecture/#microservices-event-bus","title":"Microservices Event Bus","text":"<p>Share schemas across microservices to ensure contract compliance:</p> <ul> <li>User Service publishes <code>user-registered</code> events</li> <li>Email Service subscribes and validates against schema</li> <li>Analytics Service subscribes and validates against same schema</li> <li>CRM Service subscribes and validates against same schema</li> </ul> <p>All services share one schema subject, ensuring consistent message structure.</p>"},{"location":"architecture/schema_registry_architecture/#schema-evolution-during-upgrades","title":"Schema Evolution During Upgrades","text":"<p>Safely evolve schemas during rolling deployments:</p> <ol> <li>v1 Schema: <code>{user_id, action}</code></li> <li>Add optional field: <code>{user_id, action, email?}</code> \u2192 Backward compatible</li> <li>Deploy consumers first (can read old + new messages)</li> <li>Deploy producers second (send new format)</li> <li>All consumers upgraded \u2192 Make field required in v3 if needed</li> </ol>"},{"location":"architecture/schema_registry_architecture/#multi-environment-management","title":"Multi-Environment Management","text":"<p>Use different compatibility modes per environment:</p> <ul> <li>Development: <code>mode: none</code> - Fast iteration, no restrictions</li> <li>Staging: <code>mode: backward</code> - Test compatibility checks</li> <li>Production: <code>mode: full</code> - Strictest safety</li> </ul> <p>Same schemas, different governance levels based on environment needs.</p>"},{"location":"architecture/schema_registry_architecture/#data-governance-and-compliance","title":"Data Governance and Compliance","text":"<p>Track all schema changes with audit trail:</p> <ul> <li>Who created each schema version (created_by)</li> <li>When it was created (timestamp)</li> <li>What changed (description, tags)</li> <li>Which topics use which schemas</li> <li>Full version history for compliance audits</li> </ul>"},{"location":"architecture/schema_registry_architecture/#metrics-and-monitoring","title":"Metrics and Monitoring","text":"<p>The Schema Registry exposes Prometheus metrics for observability:</p> <p>Schema Validation Metrics:</p> <ul> <li><code>schema_validation_total</code> - Total validation attempts</li> <li><code>schema_validation_failures_total</code> - Failed validations</li> </ul> <p>Labels:</p> <ul> <li><code>topic</code> - Which topic validation occurred on</li> <li><code>policy</code> - Validation policy (Warn/Enforce)</li> <li><code>reason</code> - Failure reason (missing_schema_id, schema_mismatch)</li> </ul> <p>Use these metrics to:</p> <ul> <li>Monitor schema adoption across topics</li> <li>Track validation failure rates</li> <li>Alert on breaking changes or misconfigurations</li> <li>Measure impact of schema updates</li> </ul>"},{"location":"architecture/schema_registry_architecture/#summary","title":"Summary","text":"<p>The Danube Schema Registry transforms schema management from an ad-hoc, error-prone process into a robust, centralized governance layer. It enables:</p> <ul> <li>Safe evolution of message contracts without breaking consumers</li> <li>Data quality guarantees through multi-layer validation</li> <li>Operational visibility with full audit trails and versioning</li> <li>Developer confidence with compatibility checking and type safety</li> <li>Production readiness following industry-standard patterns</li> </ul> <p>By centralizing schema management, Danube ensures that all participants in the messaging infrastructure speak the same language, evolving together safely over time.</p>"},{"location":"client_libraries/clients/","title":"Danube Client Libraries","text":"<p>Danube provides official client libraries for multiple programming languages, allowing you to integrate messaging capabilities into your applications. All clients follow consistent patterns and support core Danube features including topics, subscriptions, partitions, and schema registry.</p>"},{"location":"client_libraries/clients/#supported-languages","title":"Supported Languages","text":""},{"location":"client_libraries/clients/#rust-client","title":"Rust Client","text":"<p>The official danube-client is an asynchronous Rust client library built on Tokio.</p> <p>Installation:</p> <pre><code>cargo add danube-client\n</code></pre> <p>Features:</p> <ul> <li>\u2705 Full async/await support with Tokio</li> <li>\u2705 Type-safe schema registry integration</li> <li>\u2705 Partitioned topics</li> <li>\u2705 Reliable dispatch</li> <li>\u2705 TLS and JWT authentication</li> <li>\u2705 All subscription types (Exclusive, Shared, Failover)</li> <li>\u2705 Schema validation (JSON Schema, Avro, Protobuf)</li> </ul> <p>Learn more: Rust Examples</p>"},{"location":"client_libraries/clients/#go-client","title":"Go Client","text":"<p>The official danube-go library provides Go language bindings.</p> <p>Installation:</p> <pre><code>go get github.com/danube-messaging/danube-go\n</code></pre> <p>Features:</p> <ul> <li>\u2705 Context-based operations</li> <li>\u2705 Partitioned topics</li> <li>\u2705 Reliable dispatch</li> <li>\u2705 All subscription types (Exclusive, Shared, Failover)</li> <li>\u23f3 TLS support (coming soon)</li> <li>\u23f3 Schema registry (coming soon)</li> </ul> <p>Learn more: Go Examples</p>"},{"location":"client_libraries/clients/#feature-comparison-matrix","title":"Feature Comparison Matrix","text":"Feature Rust Go Python* Java* Core Messaging Producers \u2705 \u2705 \u23f3 \u23f3 Consumers \u2705 \u2705 \u23f3 \u23f3 Partitioned Topics \u2705 \u2705 \u23f3 \u23f3 Reliable Dispatch \u2705 \u2705 \u23f3 \u23f3 Subscriptions Exclusive \u2705 \u2705 \u23f3 \u23f3 Shared \u2705 \u2705 \u23f3 \u23f3 Failover \u2705 \u2705 \u23f3 \u23f3 Schema Registry JSON Schema \u2705 \u23f3 \u23f3 \u23f3 Avro \u2705 \u23f3 \u23f3 \u23f3 Protobuf \u2705 \u23f3 \u23f3 \u23f3 Compatibility Checking \u2705 \u23f3 \u23f3 \u23f3 Security TLS \u2705 \u23f3 \u23f3 \u23f3 JWT Authentication \u2705 \u23f3 \u23f3 \u23f3 <p>* Coming soon - community contributions welcome</p>"},{"location":"client_libraries/clients/#community-clients","title":"Community Clients","text":"<p>We encourage the community to develop and maintain clients for additional languages. If you're building a Danube client:</p> <ul> <li>Follow the protocol specification</li> <li>Reference existing clients for patterns</li> <li>Submit a PR to add your client to this list</li> </ul>"},{"location":"client_libraries/clients/#guidelines-for-client-development","title":"Guidelines for Client Development","text":"<p>Core Requirements:</p> <ul> <li>Support for producer/consumer operations</li> <li>Schema registry integration</li> <li>Topic lookup and partitioning</li> <li>Subscription management (Exclusive, Shared, Failover)</li> <li>Message acknowledgment</li> <li>Error handling and retries</li> </ul> <p>Recommended Features:</p> <ul> <li>TLS support</li> <li>JWT authentication</li> <li>Connection pooling</li> <li>Graceful shutdown</li> </ul>"},{"location":"client_libraries/clients/#next-steps","title":"Next Steps","text":"<ul> <li>Client Setup - Configure and connect your client</li> <li>Producer Guide - Send messages to topics</li> <li>Consumer Guide - Receive and process messages</li> <li>Schema Registry - Work with typed messages</li> </ul>"},{"location":"client_libraries/clients/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Danube Docs</li> <li>Examples: Rust | Go</li> <li>Issues: GitHub Issues</li> </ul>"},{"location":"client_libraries/consumer-advanced/","title":"Advanced Consumer Features","text":"<p>This guide covers advanced consumer capabilities including partitioned topics, multiple consumers, and integration with schemas.</p>"},{"location":"client_libraries/consumer-advanced/#consuming-from-partitioned-topics","title":"Consuming from Partitioned Topics","text":"<p>When a topic has partitions, consumers automatically receive from all partitions.</p>"},{"location":"client_libraries/consumer-advanced/#automatic-partition-handling","title":"Automatic Partition Handling","text":"RustGo <pre><code>use danube_client::{DanubeClient, SubType};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    // Topic has 3 partitions: my-topic-part-0, my-topic-part-1, my-topic-part-2\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/my-topic\")  // Parent topic name\n        .with_consumer_name(\"partition-consumer\")\n        .with_subscription(\"partition-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n    println!(\"\u2705 Subscribed to all partitions\");\n\n    // Automatically receives from all 3 partitions\n    let mut message_stream = consumer.receive().await?;\n\n    while let Some(message) = message_stream.recv().await {\n        println!(\"\ud83d\udce5 Received from partition: {}\", message.payload);\n        consumer.ack(&amp;message).await?;\n    }\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    consumer, err := client.NewConsumer(ctx).\n        WithConsumerName(\"partition-consumer\").\n        WithTopic(\"/default/my-topic\").  // Parent topic\n        WithSubscription(\"partition-sub\").\n        WithSubscriptionType(danube.Exclusive).\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create consumer: %v\", err)\n    }\n\n    if err := consumer.Subscribe(ctx); err != nil {\n        log.Fatalf(\"Failed to subscribe: %v\", err)\n    }\n    fmt.Println(\"\u2705 Subscribed to all partitions\")\n\n    stream, err := consumer.Receive(ctx)\n    if err != nil {\n        log.Fatalf(\"Failed to receive: %v\", err)\n    }\n\n    for msg := range stream {\n        fmt.Printf(\"\ud83d\udce5 Received from partition: %s\\n\", string(msg.GetPayload()))\n        consumer.Ack(ctx, msg)\n    }\n}\n</code></pre> <p>What happens:</p> <ul> <li>Client discovers all partitions automatically</li> <li>Creates one consumer per partition internally</li> <li>Messages from all partitions merged into single stream</li> <li>Ordering preserved per-partition, not cross-partition</li> </ul>"},{"location":"client_libraries/consumer-advanced/#shared-subscription-patterns","title":"Shared Subscription Patterns","text":""},{"location":"client_libraries/consumer-advanced/#load-balancing-across-consumers","title":"Load Balancing Across Consumers","text":"<p>With Shared subscription type, multiple consumers can subscribe to the same topic with the same subscription name. Messages are distributed round-robin across all active consumers.</p> <p>How it works:</p> <ol> <li>Deploy multiple consumer instances with:</li> <li>Same topic: <code>/default/work-queue</code></li> <li>Same subscription: <code>\"work-sub\"</code></li> <li>Subscription type: <code>SubType::Shared</code></li> <li> <p>Different consumer names: <code>\"worker-1\"</code>, <code>\"worker-2\"</code>, etc.</p> </li> <li> <p>Broker distributes messages:</p> </li> <li>Message 1 \u2192 Worker 1</li> <li>Message 2 \u2192 Worker 2  </li> <li>Message 3 \u2192 Worker 3</li> <li>Message 4 \u2192 Worker 1 (round-robin continues)</li> </ol> <p>Benefits:</p> <ul> <li>\u2705 Horizontal scaling - add more consumers to increase throughput</li> <li>\u2705 Load distribution - work shared across all consumers</li> <li>\u2705 Dynamic scaling - add/remove workers without coordination</li> <li>\u2705 High throughput - parallel processing</li> </ul> <p>Trade-offs:</p> <ul> <li>\u274c No ordering guarantee - messages may be processed out of order</li> <li>\u274c No affinity - same entity may go to different consumers</li> </ul> <p>Use cases:</p> <ul> <li>Log processing pipelines</li> <li>Analytics workloads</li> <li>Image processing queues</li> <li>Any workload where order doesn't matter</li> </ul>"},{"location":"client_libraries/consumer-advanced/#failover-pattern","title":"Failover Pattern","text":""},{"location":"client_libraries/consumer-advanced/#high-availability-setup","title":"High Availability Setup","text":"<p>With Failover subscription type, multiple consumers can subscribe to the same topic, but only one is active at a time. The others remain in standby. If the active consumer fails, the broker automatically promotes a standby consumer.</p> <p>How it works:</p> <ol> <li>Deploy multiple consumer instances with:</li> <li>Same topic: <code>/default/critical-orders</code></li> <li>Same subscription: <code>\"order-processor\"</code></li> <li>Subscription type: <code>SubType::FailOver</code></li> <li> <p>Different consumer names: <code>\"processor-1\"</code>, <code>\"processor-2\"</code>, etc.</p> </li> <li> <p>Broker manages active consumer:</p> </li> <li>First connected consumer becomes active (receives messages)</li> <li>Other consumers remain in standby (no messages received)</li> <li>If active disconnects/fails, broker promotes next standby instantly</li> <li>New active continues from last acknowledged message</li> </ol> <p>Behavior:</p> <ul> <li>\u2705 Only one consumer active at a time</li> <li>\u2705 Automatic failover - no manual intervention</li> <li>\u2705 Message ordering preserved (single active consumer)</li> <li>\u2705 High availability - standbys ready to take over</li> <li>\u2705 Zero message loss - standby resumes from last ack</li> </ul> <p>Use cases:</p> <ul> <li>Critical order processing (needs ordering + HA)</li> <li>Financial transactions</li> <li>State machine workflows</li> <li>Any workload requiring both ordering and high availability</li> </ul>"},{"location":"client_libraries/consumer-advanced/#schema-integration","title":"Schema Integration","text":"<p>Consume typed messages validated against schemas (see Schema Registry for details).</p> <p>Note: Schema Registry integration is not yet available in the Go client.</p>"},{"location":"client_libraries/consumer-advanced/#basic-schema-consumption","title":"Basic Schema Consumption","text":"Rust <pre><code>use danube_client::{DanubeClient, SubType};\nuse serde::Deserialize;\n\n#[derive(Deserialize, Debug)]\nstruct Event {\n    event_id: String,\n    timestamp: i64,\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/events\")\n        .with_consumer_name(\"event-consumer\")\n        .with_subscription(\"event-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n\n    let mut stream = consumer.receive().await?;\n\n    while let Some(message) = stream.recv().await {\n        // Deserialize JSON message\n        match serde_json::from_slice::&lt;Event&gt;(&amp;message.payload) {\n            Ok(event) =&gt; {\n                println!(\"\ud83d\udce5 Event: {:?}\", event);\n                consumer.ack(&amp;message).await?;\n            }\n            Err(e) =&gt; {\n                eprintln!(\"\u274c Deserialization failed: {}\", e);\n                // Don't ack invalid messages\n            }\n        }\n    }\n\n    Ok(())\n}\n</code></pre>"},{"location":"client_libraries/consumer-advanced/#validated-schema-consumption","title":"Validated Schema Consumption","text":"<p>Validate your Rust struct against the registered schema at startup to catch incompatibilities before processing messages (Rust only):</p> Rust <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SubType};\nuse serde::{Deserialize, Serialize};\nuse jsonschema::JSONSchema;\n\n#[derive(Deserialize, Serialize, Debug)]\nstruct MyMessage {\n    field1: String,\n    field2: i32,\n}\n\n/// Validates that consumer struct matches the schema in the registry\nasync fn validate_struct_against_registry&lt;T: Serialize&gt;(\n    schema_client: &amp;mut SchemaRegistryClient,\n    subject: &amp;str,\n    sample: &amp;T,\n) -&gt; Result&lt;u32, Box&lt;dyn std::error::Error&gt;&gt; {\n    println!(\"\ud83d\udd0d Fetching schema from registry: {}\", subject);\n\n    let schema_response = schema_client.get_latest_schema(subject).await?;\n    println!(\"\ud83d\udccb Schema version: {}\", schema_response.version);\n\n    // Parse schema definition\n    let schema_def: serde_json::Value = \n        serde_json::from_slice(&amp;schema_response.schema_definition)?;\n\n    // Compile JSON Schema validator\n    let validator = JSONSchema::compile(&amp;schema_def)\n        .map_err(|e| format!(\"Invalid schema: {}\", e))?;\n\n    // Validate sample struct\n    let sample_json = serde_json::to_value(sample)?;\n\n    if let Err(errors) = validator.validate(&amp;sample_json) {\n        eprintln!(\"\u274c VALIDATION FAILED: Struct incompatible with schema v{}\", \n            schema_response.version);\n        for error in errors {\n            eprintln!(\"   - {}\", error);\n        }\n        return Err(\"Struct validation failed\".into());\n    }\n\n    println!(\"\u2705 Struct validated against schema v{}\", schema_response.version);\n    Ok(schema_response.version)\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\n    // VALIDATE BEFORE CONSUMING - fails fast if struct is wrong!\n    let schema_version = validate_struct_against_registry(\n        &amp;mut schema_client,\n        \"my-app-events\",\n        &amp;MyMessage {\n            field1: \"test\".to_string(),\n            field2: 0,\n        },\n    ).await?;\n\n    println!(\"\u2705 Consumer validated - safe to deserialize\\n\");\n\n    // Now create consumer\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/test_topic\")\n        .with_consumer_name(\"validated-consumer\")\n        .with_subscription(\"validated-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n    let mut stream = consumer.receive().await?;\n\n    while let Some(message) = stream.recv().await {\n        match serde_json::from_slice::&lt;MyMessage&gt;(&amp;message.payload) {\n            Ok(msg) =&gt; {\n                println!(\"\ud83d\udce5 Message: {:?}\", msg);\n                consumer.ack(&amp;message).await?;\n            }\n            Err(e) =&gt; {\n                eprintln!(\"\u274c Deserialization failed: {}\", e);\n                eprintln!(\"   Schema drift detected - check version {}\", schema_version);\n                // Don't ack - message will retry or go to DLQ\n            }\n        }\n    }\n\n    Ok(())\n}\n</code></pre> <p>Why validate at startup?</p> <ul> <li>\u2705 Fail fast - catch schema mismatches before processing messages</li> <li>\u2705 Clear errors - know exactly which fields don't match</li> <li>\u2705 Prevent runtime failures - no surprises during message processing</li> <li>\u2705 Safe deployments - validates before going live</li> </ul> <p>Note: Requires <code>jsonschema</code> crate dependency.</p>"},{"location":"client_libraries/consumer-advanced/#performance-tuning","title":"Performance Tuning","text":""},{"location":"client_libraries/consumer-advanced/#batch-processing","title":"Batch Processing","text":"<p>Process messages in batches for efficiency:</p> Rust <pre><code>use tokio::time::{timeout, Duration};\n\nlet mut stream = consumer.receive().await?;\nlet mut batch = Vec::new();\nlet batch_size = 100;\nlet batch_timeout = Duration::from_millis(500);\n\nloop {\n    match timeout(batch_timeout, stream.recv()).await {\n        Ok(Some(message)) =&gt; {\n            batch.push(message);\n\n            if batch.len() &gt;= batch_size {\n                // Process batch\n                process_batch(&amp;batch).await?;\n\n                // Ack all\n                for msg in &amp;batch {\n                    consumer.ack(msg).await?;\n                }\n\n                batch.clear();\n            }\n        }\n        Ok(None) =&gt; break,  // Stream closed\n        Err(_) =&gt; {\n            // Timeout - process partial batch\n            if !batch.is_empty() {\n                process_batch(&amp;batch).await?;\n                for msg in &amp;batch {\n                    consumer.ack(msg).await?;\n                }\n                batch.clear();\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"client_libraries/consumer-basics/","title":"Consumer Basics","text":"<p>Consumers receive messages from Danube topics via subscriptions. This guide covers fundamental consumer operations.</p>"},{"location":"client_libraries/consumer-basics/#creating-a-consumer","title":"Creating a Consumer","text":""},{"location":"client_libraries/consumer-basics/#simple-consumer","title":"Simple Consumer","text":"<p>The minimal setup to receive messages:</p> RustGo <pre><code>use danube_client::{DanubeClient, SubType};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/my-topic\")\n        .with_consumer_name(\"my-consumer\")\n        .with_subscription(\"my-subscription\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n    println!(\"\u2705 Consumer subscribed\");\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    consumer, err := client.NewConsumer(ctx).\n        WithConsumerName(\"my-consumer\").\n        WithTopic(\"/default/my-topic\").\n        WithSubscription(\"my-subscription\").\n        WithSubscriptionType(danube.Exclusive).\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create consumer: %v\", err)\n    }\n\n    if err := consumer.Subscribe(ctx); err != nil {\n        log.Fatalf(\"Failed to subscribe: %v\", err)\n    }\n\n    fmt.Println(\"\u2705 Consumer subscribed\")\n}\n</code></pre> <p>Key concepts:</p> <ul> <li>Topic: Source of messages</li> <li>Consumer Name: Unique identifier for this consumer instance</li> <li>Subscription: Logical grouping of consumers (multiple consumers can share)</li> <li>Subscription Type: Controls how messages are distributed (Exclusive, Shared, Failover)</li> </ul>"},{"location":"client_libraries/consumer-basics/#subscription-types","title":"Subscription Types","text":""},{"location":"client_libraries/consumer-basics/#exclusive","title":"Exclusive","text":"<p>Only one consumer can be active. Guarantees message order.</p> RustGo <pre><code>use danube_client::SubType;\n\nlet mut consumer = client\n    .new_consumer()\n    .with_topic(\"/default/orders\")\n    .with_consumer_name(\"order-processor\")\n    .with_subscription(\"order-sub\")\n    .with_subscription_type(SubType::Exclusive)  // Only this consumer\n    .build();\n</code></pre> <pre><code>consumer, err := client.NewConsumer(ctx).\n    WithConsumerName(\"order-processor\").\n    WithTopic(\"/default/orders\").\n    WithSubscription(\"order-sub\").\n    WithSubscriptionType(danube.Exclusive).\n    Build()\nif err != nil {\n    log.Fatalf(\"Failed to create consumer: %v\", err)\n}\n</code></pre> <p>Characteristics:</p> <ul> <li>\u2705 Message ordering guaranteed</li> <li>\u2705 Simple failure handling</li> <li>\u274c No horizontal scaling</li> <li>Use case: Order processing, sequential workflows</li> </ul>"},{"location":"client_libraries/consumer-basics/#shared","title":"Shared","text":"<p>Multiple consumers receive messages in round-robin. Scales horizontally.</p> RustGo <pre><code>use danube_client::SubType;\n\nlet mut consumer = client\n    .new_consumer()\n    .with_topic(\"/default/logs\")\n    .with_consumer_name(\"log-processor-1\")\n    .with_subscription(\"log-sub\")\n    .with_subscription_type(SubType::Shared)  // Multiple consumers allowed\n    .build();\n</code></pre> <pre><code>consumer, err := client.NewConsumer(ctx).\n    WithConsumerName(\"log-processor-1\").\n    WithTopic(\"/default/logs\").\n    WithSubscription(\"log-sub\").\n    WithSubscriptionType(danube.Shared).\n    Build()\nif err != nil {\n    log.Fatalf(\"Failed to create consumer: %v\", err)\n}\n</code></pre> <p>Characteristics:</p> <ul> <li>\u2705 Horizontal scaling</li> <li>\u2705 Load distribution</li> <li>\u274c No ordering guarantee</li> <li>Use case: Log processing, analytics, parallel processing</li> </ul>"},{"location":"client_libraries/consumer-basics/#failover","title":"Failover","text":"<p>Like Exclusive, but allows standby consumers. One active, others wait.</p> RustGo <pre><code>use danube_client::SubType;\n\nlet mut consumer = client\n    .new_consumer()\n    .with_topic(\"/default/critical\")\n    .with_consumer_name(\"processor-1\")\n    .with_subscription(\"critical-sub\")\n    .with_subscription_type(SubType::FailOver)  // Hot standby\n    .build();\n</code></pre> <pre><code>consumer, err := client.NewConsumer(ctx).\n    WithConsumerName(\"processor-1\").\n    WithTopic(\"/default/critical\").\n    WithSubscription(\"critical-sub\").\n    WithSubscriptionType(danube.FailOver).\n    Build()\nif err != nil {\n    log.Fatalf(\"Failed to create consumer: %v\", err)\n}\n</code></pre> <p>Characteristics:</p> <ul> <li>\u2705 Message ordering guaranteed</li> <li>\u2705 Automatic failover to standby</li> <li>\u2705 High availability</li> <li>Use case: Critical services needing HA with ordering</li> </ul>"},{"location":"client_libraries/consumer-basics/#receiving-messages","title":"Receiving Messages","text":""},{"location":"client_libraries/consumer-basics/#basic-message-loop","title":"Basic Message Loop","text":"RustGo <pre><code>use danube_client::{DanubeClient, SubType};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/events\")\n        .with_consumer_name(\"event-processor\")\n        .with_subscription(\"event-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n\n    // Start receiving\n    let mut message_stream = consumer.receive().await?;\n\n    while let Some(message) = message_stream.recv().await {\n        // Access message data\n        let payload = String::from_utf8_lossy(&amp;message.payload);\n        println!(\"\ud83d\udce5 Received: {}\", payload);\n\n        // Acknowledge the message\n        consumer.ack(&amp;message).await?;\n    }\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    consumer, err := client.NewConsumer(ctx).\n        WithConsumerName(\"event-processor\").\n        WithTopic(\"/default/events\").\n        WithSubscription(\"event-sub\").\n        WithSubscriptionType(danube.Exclusive).\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create consumer: %v\", err)\n    }\n\n    if err := consumer.Subscribe(ctx); err != nil {\n        log.Fatalf(\"Failed to subscribe: %v\", err)\n    }\n\n    // Start receiving\n    stream, err := consumer.Receive(ctx)\n    if err != nil {\n        log.Fatalf(\"Failed to receive: %v\", err)\n    }\n\n    for msg := range stream {\n        payload := string(msg.GetPayload())\n        fmt.Printf(\"\ud83d\udce5 Received: %s\\n\", payload)\n\n        // Acknowledge the message\n        if _, err := consumer.Ack(ctx, msg); err != nil {\n            log.Printf(\"Failed to ack: %v\\n\", err)\n        }\n    }\n}\n</code></pre>"},{"location":"client_libraries/consumer-basics/#message-acknowledgment","title":"Message Acknowledgment","text":"<p>Acknowledgment tells the broker the message was processed successfully.</p>"},{"location":"client_libraries/consumer-basics/#ack-pattern","title":"Ack Pattern","text":"RustGo <pre><code>while let Some(message) = message_stream.recv().await {\n    // Process message\n    match process_message(&amp;message.payload).await {\n        Ok(_) =&gt; {\n            // Acknowledge success\n            consumer.ack(&amp;message).await?;\n            println!(\"\u2705 Processed and acked\");\n        }\n        Err(e) =&gt; {\n            // Don't ack on failure - message will be redelivered\n            eprintln!(\"\u274c Processing failed: {}\", e);\n            // Message will be redelivered to this or another consumer\n        }\n    }\n}\n</code></pre> <pre><code>for msg := range stream {\n    if err := processMessage(msg.GetPayload()); err != nil {\n        // Don't ack on failure - message will be redelivered\n        log.Printf(\"\u274c Processing failed: %v\", err)\n        continue\n    }\n\n    // Acknowledge success\n    if _, err := consumer.Ack(ctx, msg); err != nil {\n        log.Printf(\"Failed to ack: %v\", err)\n    }\n\n    fmt.Println(\"\u2705 Processed and acked\")\n}\n</code></pre> <p>Important:</p> <ul> <li>\u26a0\ufe0f Only ack after successful processing</li> <li>\u26a0\ufe0f Unacked messages are redelivered</li> <li>\u26a0\ufe0f Messages persist until acked or subscription expires</li> </ul>"},{"location":"client_libraries/consumer-basics/#reading-message-attributes","title":"Reading Message Attributes","text":"<p>Access metadata sent with messages:</p> RustGo <pre><code>while let Some(message) = message_stream.recv().await {\n    // Read payload\n    let payload = String::from_utf8_lossy(&amp;message.payload);\n\n    // Read attributes (if any)\n    if let Some(attributes) = &amp;message.attributes {\n        for (key, value) in attributes {\n            println!(\"  {}: {}\", key, value);\n        }\n    }\n\n    // Read other metadata\n    println!(\"Message ID: {:?}\", message.msg_id);\n    println!(\"Publish time: {}\", message.publish_time);\n\n    consumer.ack(&amp;message).await?;\n}\n</code></pre> <pre><code>for msg := range stream {\n    payload := string(msg.GetPayload())\n\n    // Read attributes\n    for key, value := range msg.GetAttributes() {\n        fmt.Printf(\"  %s: %s\\n\", key, value)\n    }\n\n    // Read metadata\n    fmt.Printf(\"Message ID: %v\\n\", msg.GetMessageId())\n    fmt.Printf(\"Publish time: %d\\n\", msg.GetPublishTime())\n\n    consumer.Ack(ctx, msg)\n}\n</code></pre>"},{"location":"client_libraries/consumer-basics/#complete-example","title":"Complete Example","text":"RustGo <pre><code>use danube_client::{DanubeClient, SubType};\nuse tokio::time::{sleep, Duration};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // 1. Setup client\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    // 2. Create consumer\n    let mut consumer = client\n        .new_consumer()\n        .with_topic(\"/default/events\")\n        .with_consumer_name(\"event-processor\")\n        .with_subscription(\"event-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n    println!(\"\u2705 Consumer subscribed and ready\");\n\n    // 3. Receive messages\n    let mut message_stream = consumer.receive().await?;\n    let mut count = 0;\n\n    while let Some(message) = message_stream.recv().await {\n        // Extract payload\n        let payload = String::from_utf8_lossy(&amp;message.payload);\n\n        // Log receipt\n        count += 1;\n        println!(\"\ud83d\udce5 Message #{}: {}\", count, payload);\n\n        // Check attributes\n        if let Some(attrs) = &amp;message.attributes {\n            if let Some(priority) = attrs.get(\"priority\") {\n                if priority == \"high\" {\n                    println!(\"  \u26a1 High priority message!\");\n                }\n            }\n        }\n\n        // Simulate processing\n        sleep(Duration::from_millis(100)).await;\n\n        // Acknowledge\n        match consumer.ack(&amp;message).await {\n            Ok(_) =&gt; println!(\"  \u2705 Acknowledged\"),\n            Err(e) =&gt; eprintln!(\"  \u274c Ack failed: {}\", e),\n        }\n    }\n\n    Ok(())\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"time\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    // 1. Setup client\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    // 2. Create consumer\n    consumer, err := client.NewConsumer(ctx).\n        WithConsumerName(\"event-processor\").\n        WithTopic(\"/default/events\").\n        WithSubscription(\"event-sub\").\n        WithSubscriptionType(danube.Exclusive).\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create consumer: %v\", err)\n    }\n\n    if err := consumer.Subscribe(ctx); err != nil {\n        log.Fatalf(\"Failed to subscribe: %v\", err)\n    }\n\n    fmt.Println(\"\u2705 Consumer subscribed and ready\")\n\n    // 3. Receive messages\n    stream, err := consumer.Receive(ctx)\n    if err != nil {\n        log.Fatalf(\"Failed to receive: %v\", err)\n    }\n\n    count := 0\n    for msg := range stream {\n        payload := string(msg.GetPayload())\n\n        count++\n        fmt.Printf(\"\ud83d\udce5 Message #%d: %s\\n\", count, payload)\n\n        // Check attributes\n        if priority, ok := msg.GetAttributes()[\"priority\"]; ok {\n            if priority == \"high\" {\n                fmt.Println(\"  \u26a1 High priority message!\")\n            }\n        }\n\n        // Simulate processing\n        time.Sleep(100 * time.Millisecond)\n\n        // Acknowledge\n        if _, err := consumer.Ack(ctx, msg); err != nil {\n            fmt.Printf(\"  \u274c Ack failed: %v\\n\", err)\n        } else {\n            fmt.Println(\"  \u2705 Acknowledged\")\n        }\n    }\n}\n</code></pre>"},{"location":"client_libraries/producer-advanced/","title":"Advanced Producer Features","text":"<p>This guide covers advanced producer capabilities including partitions, reliable dispatch, and schema integration.</p>"},{"location":"client_libraries/producer-advanced/#partitioned-topics","title":"Partitioned Topics","text":"<p>Partitions enable horizontal scaling by distributing messages across multiple brokers.</p>"},{"location":"client_libraries/producer-advanced/#creating-partitioned-producers","title":"Creating Partitioned Producers","text":"RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/high-throughput\")\n        .with_name(\"partitioned-producer\")\n        .with_partitions(3)  // Create 3 partitions\n        .build();\n\n    producer.create().await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    producer, err := client.NewProducer(ctx).\n        WithName(\"partitioned-producer\").\n        WithTopic(\"/default/high-throughput\").\n        WithPartitions(3).  // Create 3 partitions\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create producer: %v\", err)\n    }\n\n    if err := producer.Create(ctx); err != nil {\n        log.Fatalf(\"Failed to initialize producer: %v\", err)\n    }\n}\n</code></pre> <p>What happens:</p> <ul> <li>Topic <code>/default/high-throughput</code> becomes:</li> <li><code>/default/high-throughput-part-0</code></li> <li><code>/default/high-throughput-part-1</code></li> <li><code>/default/high-throughput-part-2</code></li> <li>Messages distributed using round-robin routing</li> <li>Each partition can be on different brokers</li> </ul>"},{"location":"client_libraries/producer-advanced/#sending-to-partitions","title":"Sending to Partitions","text":"<p>Messages are automatically routed:</p> RustGo <pre><code>// Automatic round-robin distribution\nfor i in 0..9 {\n    let message = format!(\"Message {}\", i);\n    producer.send(message.as_bytes().to_vec(), None).await?;\n}\n\n// Result:\n// Message 0 -&gt; partition 0\n// Message 1 -&gt; partition 1\n// Message 2 -&gt; partition 2\n// Message 3 -&gt; partition 0 (round-robin)\n// ...\n</code></pre> <pre><code>import \"fmt\"\n\n// Automatic round-robin distribution\nfor i := 0; i &lt; 9; i++ {\n    message := fmt.Sprintf(\"Message %d\", i)\n    payload := []byte(message)\n    producer.Send(ctx, payload, nil)\n}\n</code></pre>"},{"location":"client_libraries/producer-advanced/#when-to-use-partitions","title":"When to Use Partitions","text":"<p>\u2705 Use partitions when:</p> <ul> <li>High message throughput (&gt;1K msgs/sec)</li> <li>Messages can be processed in any order</li> <li>Horizontal scaling needed</li> <li>Multiple brokers available</li> </ul> <p>\u274c Avoid partitions when:</p> <ul> <li>Strict message ordering required</li> <li>Low message volume</li> <li>Single broker deployment</li> </ul>"},{"location":"client_libraries/producer-advanced/#reliable-dispatch","title":"Reliable Dispatch","text":"<p>Reliable dispatch guarantees message delivery by persisting messages before acknowledging sends.</p>"},{"location":"client_libraries/producer-advanced/#enabling-reliable-dispatch","title":"Enabling Reliable Dispatch","text":"RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/critical-events\")\n        .with_name(\"reliable-producer\")\n        .with_reliable_dispatch()  // Enable persistence\n        .build();\n\n    producer.create().await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    // Configure reliable dispatch strategy\n    reliableStrategy := danube.NewReliableDispatchStrategy()\n\n    producer, err := client.NewProducer(ctx).\n        WithName(\"reliable-producer\").\n        WithTopic(\"/default/critical-events\").\n        WithDispatchStrategy(reliableStrategy).\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create producer: %v\", err)\n    }\n\n    if err := producer.Create(ctx); err != nil {\n        log.Fatalf(\"Failed to initialize producer: %v\", err)\n    }\n}\n</code></pre>"},{"location":"client_libraries/producer-advanced/#how-reliable-dispatch-works","title":"How Reliable Dispatch Works","text":"<pre><code>1. Producer sends message\n2. Broker persists to WAL (Write-Ahead Log)\n3. Broker uploads to cloud storage (background)\n4. Broker acknowledges send\n5. Message delivered to consumers from WAL/cloud\n</code></pre> <p>Guarantees:</p> <ul> <li>\u2705 Message never lost (persisted to disk + cloud)</li> <li>\u2705 Survives broker restarts</li> <li>\u2705 Replay from historical offset</li> <li>\u2705 Consumer can restart and resume</li> </ul> <p>Trade-offs:</p> <ul> <li>Slightly higher latency (~5-10ms added)</li> <li>Storage costs for persistence</li> <li>Good for: Critical business events, audit logs, transactions</li> </ul>"},{"location":"client_libraries/producer-advanced/#when-to-use-reliable-dispatch","title":"When to Use Reliable Dispatch","text":"<p>\u2705 Use for:</p> <ul> <li>Financial transactions</li> <li>Order processing</li> <li>Audit logs</li> <li>Critical notifications</li> </ul> <p>\u274c Avoid for:</p> <ul> <li>Real-time metrics (can tolerate loss)</li> <li>High-frequency sensor data</li> <li>Live streaming (freshness &gt; durability)</li> </ul>"},{"location":"client_libraries/producer-advanced/#combining-features","title":"Combining Features","text":""},{"location":"client_libraries/producer-advanced/#partitions-reliable-dispatch","title":"Partitions + Reliable Dispatch","text":"<p>Scale and durability together:</p> Rust <pre><code>let mut producer = client\n    .new_producer()\n    .with_topic(\"/default/orders\")\n    .with_name(\"order-producer\")\n    .with_partitions(5)           // Scale across 5 partitions\n    .with_reliable_dispatch()     // Persist all messages\n    .build();\n\nproducer.create().await?;\n\n// High throughput + guaranteed delivery\nfor order_id in 1..=10000 {\n    let order = format!(\"{{\\\"order_id\\\": {}}}\", order_id);\n    producer.send(order.as_bytes().to_vec(), None).await?;\n}\n</code></pre>"},{"location":"client_libraries/producer-advanced/#schema-integration","title":"Schema Integration","text":"<p>Link producers to schemas for type safety (see Schema Registry for details).</p> <p>Note: Schema Registry integration is not yet available in the Go client.</p>"},{"location":"client_libraries/producer-advanced/#basic-schema-usage","title":"Basic Schema Usage","text":"Rust <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SchemaType};\nuse serde::Serialize;\n\n#[derive(Serialize)]\nstruct Event {\n    event_id: String,\n    timestamp: i64,\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    // 1. Register schema\n    let json_schema = r#\"{\n        \"type\": \"object\",\n        \"properties\": {\n            \"event_id\": {\"type\": \"string\"},\n            \"timestamp\": {\"type\": \"integer\"}\n        },\n        \"required\": [\"event_id\", \"timestamp\"]\n    }\"#;\n\n    let mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n    let schema_id = schema_client\n        .register_schema(\"event-schema\")\n        .with_type(SchemaType::JsonSchema)\n        .with_schema_data(json_schema.as_bytes())\n        .execute()\n        .await?;\n\n    println!(\"\ud83d\udccb Registered schema ID: {}\", schema_id);\n\n    // 2. Create producer with schema reference\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/events\")\n        .with_name(\"schema-producer\")\n        .with_schema_subject(\"event-schema\")  // Link to schema\n        .build();\n\n    producer.create().await?;\n\n    // 3. Send typed messages\n    let event = Event {\n        event_id: \"evt-123\".to_string(),\n        timestamp: 1234567890,\n    };\n\n    let json_bytes = serde_json::to_vec(&amp;event)?;\n    let msg_id = producer.send(json_bytes, None).await?;\n\n    println!(\"\ud83d\udce4 Sent validated message: {}\", msg_id);\n\n    Ok(())\n}\n</code></pre> <p>Benefits:</p> <ul> <li>Messages validated against schema before sending</li> <li>Schema ID included in message metadata (8 bytes vs KB of schema)</li> <li>Consumers know message structure</li> <li>Safe schema evolution with compatibility checking</li> </ul>"},{"location":"client_libraries/producer-basics/","title":"Producer Basics","text":"<p>Producers send messages to Danube topics. This guide covers fundamental producer operations.</p>"},{"location":"client_libraries/producer-basics/#creating-a-producer","title":"Creating a Producer","text":""},{"location":"client_libraries/producer-basics/#simple-producer","title":"Simple Producer","text":"<p>The minimal setup to send messages:</p> RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/my-topic\")\n        .with_name(\"my-producer\")\n        .build();\n\n    producer.create().await?;\n    println!(\"\u2705 Producer created\");\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    producer, err := client.NewProducer(ctx).\n        WithName(\"my-producer\").\n        WithTopic(\"/default/my-topic\").\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create producer: %v\", err)\n    }\n\n    if err := producer.Create(ctx); err != nil {\n        log.Fatalf(\"Failed to initialize producer: %v\", err)\n    }\n\n    fmt.Println(\"\u2705 Producer created\")\n}\n</code></pre> <p>Key concepts:</p> <ul> <li>Topic: Destination for messages (e.g., <code>/default/my-topic</code>)</li> <li>Name: Unique producer identifier</li> <li>Create: Registers producer with broker before sending</li> </ul>"},{"location":"client_libraries/producer-basics/#sending-messages","title":"Sending Messages","text":""},{"location":"client_libraries/producer-basics/#byte-messages","title":"Byte Messages","text":"<p>Send raw byte data:</p> RustGo <pre><code>let message = \"Hello Danube!\";\nlet message_id = producer\n    .send(message.as_bytes().to_vec(), None)\n    .await?;\n\nprintln!(\"\ud83d\udce4 Sent message ID: {}\", message_id);\n</code></pre> <pre><code>message := \"Hello Danube!\"\nmessageID, err := producer.Send(ctx, []byte(message), nil)\nif err != nil {\n    log.Fatalf(\"Failed to send: %v\", err)\n}\n\nfmt.Printf(\"\ud83d\udce4 Sent message ID: %v\\n\", messageID)\n</code></pre> <p>Returns: Unique message ID for tracking</p>"},{"location":"client_libraries/producer-basics/#messages-with-attributes","title":"Messages with Attributes","text":"<p>Add metadata to messages:</p> RustGo <pre><code>use std::collections::HashMap;\n\nlet mut attributes = HashMap::new();\nattributes.insert(\"source\".to_string(), \"app-1\".to_string());\nattributes.insert(\"priority\".to_string(), \"high\".to_string());\n\nlet message_id = producer\n    .send(b\"Important message\".to_vec(), Some(attributes))\n    .await?;\n</code></pre> <pre><code>attributes := map[string]string{\n    \"source\":   \"app-1\",\n    \"priority\": \"high\",\n}\n\nmessageID, err := producer.Send(ctx, []byte(\"Important message\"), attributes)\n</code></pre> <p>Use cases:</p> <ul> <li>Routing hints</li> <li>Message metadata</li> <li>Custom headers</li> <li>Tracing IDs</li> </ul>"},{"location":"client_libraries/producer-basics/#complete-example","title":"Complete Example","text":"RustGo <pre><code>use danube_client::DanubeClient;\nuse std::collections::HashMap;\nuse tokio::time::{sleep, Duration};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // 1. Setup client\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    // 2. Create producer\n    let mut producer = client\n        .new_producer()\n        .with_topic(\"/default/events\")\n        .with_name(\"event-producer\")\n        .build();\n\n    producer.create().await?;\n    println!(\"\u2705 Producer created\");\n\n    // 3. Send messages\n    for i in 1..=10 {\n        // Prepare message\n        let message = format!(\"Event #{}\", i);\n\n        // Add attributes\n        let mut attributes = HashMap::new();\n        attributes.insert(\"event_id\".to_string(), i.to_string());\n        attributes.insert(\"timestamp\".to_string(), \n            chrono::Utc::now().to_rfc3339());\n\n        // Send\n        match producer.send(message.as_bytes().to_vec(), Some(attributes)).await {\n            Ok(msg_id) =&gt; println!(\"\ud83d\udce4 Sent: {} (ID: {})\", message, msg_id),\n            Err(e) =&gt; eprintln!(\"\u274c Failed to send: {}\", e),\n        }\n\n        sleep(Duration::from_millis(500)).await;\n    }\n\n    println!(\"\u2705 Sent 10 messages\");\n    Ok(())\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"time\"\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    // 1. Setup client\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n\n    ctx := context.Background()\n\n    // 2. Create producer\n    producer, err := client.NewProducer(ctx).\n        WithName(\"event-producer\").\n        WithTopic(\"/default/events\").\n        Build()\n    if err != nil {\n        log.Fatalf(\"Failed to create producer: %v\", err)\n    }\n\n    if err := producer.Create(ctx); err != nil {\n        log.Fatalf(\"Failed to initialize producer: %v\", err)\n    }\n\n    fmt.Println(\"\u2705 Producer created\")\n\n    // 3. Send messages\n    for i := 1; i &lt;= 10; i++ {\n        message := fmt.Sprintf(\"Event #%d\", i)\n\n        attributes := map[string]string{\n            \"event_id\":  fmt.Sprintf(\"%d\", i),\n            \"timestamp\": time.Now().Format(time.RFC3339),\n        }\n\n        msgID, err := producer.Send(ctx, []byte(message), attributes)\n        if err != nil {\n            fmt.Printf(\"\u274c Failed to send: %v\\n\", err)\n            continue\n        }\n\n        fmt.Printf(\"\ud83d\udce4 Sent: %s (ID: %v)\\n\", message, msgID)\n        time.Sleep(500 * time.Millisecond)\n    }\n\n    fmt.Println(\"\u2705 Sent 10 messages\")\n}\n</code></pre>"},{"location":"client_libraries/producer-basics/#topic-naming","title":"Topic Naming","text":""},{"location":"client_libraries/producer-basics/#topic-format","title":"Topic Format","text":"<p>Topics follow a namespace structure:</p> <pre><code>/namespace/topic-name\n</code></pre> <p>Examples:</p> <ul> <li><code>/default/orders</code> - Orders in default namespace</li> <li><code>/production/user-events</code> - User events in production namespace</li> <li><code>/staging/logs</code> - Logs in staging namespace</li> </ul>"},{"location":"client_libraries/producer-basics/#best-practices","title":"Best Practices","text":"<p>\u2705 Do:</p> <ul> <li>Use descriptive names: <code>/default/payment-processed</code></li> <li>Group by domain: <code>/inventory/stock-updates</code></li> <li>Include environment in namespace: <code>/prod/...</code>, <code>/dev/...</code></li> </ul> <p>\u274c Don't:</p> <ul> <li>Use special characters except <code>-</code> and <code>_</code></li> <li>Make names too long (keep under 255 chars)</li> <li>Mix environments in same namespace</li> </ul>"},{"location":"client_libraries/schema-registry/","title":"Schema Registry Integration","text":"<p>The Schema Registry provides type safety, validation, and schema evolution for your messages. This guide shows how to use it from client applications.</p> <p>Note: Schema Registry is currently only available in the Rust client. Go client support is coming soon.</p>"},{"location":"client_libraries/schema-registry/#overview","title":"Overview","text":"<p>What you get:</p> <ul> <li>\u2705 Type-safe message serialization/deserialization</li> <li>\u2705 Automatic schema validation</li> <li>\u2705 Safe schema evolution with compatibility checking</li> <li>\u2705 Reduced bandwidth (schema ID vs. full schema)</li> <li>\u2705 Schema discovery and documentation</li> <li>\u2705 Flexible version control (latest, pinned, minimum)</li> </ul> <p>Workflow:</p> <ol> <li>Register schema with Schema Registry</li> <li>Link producer to schema subject (assigns to topic if first producer)</li> <li>Send validated messages (broker enforces schema matching)</li> <li>Consumer fetches schema and deserializes</li> </ol> <p>What Clients Can Do:</p> <ul> <li>\u2705 Register new schema versions</li> <li>\u2705 Check compatibility before registering</li> <li>\u2705 Assign schema subject to topic (first producer only)</li> <li>\u2705 Choose schema version for producer (latest/pinned/minimum)</li> <li>\u2705 Fetch schemas for validation</li> </ul> <p>What Clients Cannot Do (Admin-Only):</p> <ul> <li>\u274c Set compatibility mode (admin CLI only)</li> <li>\u274c Change topic's schema subject (admin CLI only)</li> <li>\u274c Set topic validation policy (admin CLI only)</li> <li>\u274c Delete schema versions (admin CLI only)</li> </ul>"},{"location":"client_libraries/schema-registry/#schema-registry-client","title":"Schema Registry Client","text":""},{"location":"client_libraries/schema-registry/#creating-the-client","title":"Creating the Client","text":"RustGo <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient};\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n// Coming soon\n</code></pre> <p>Note: Schema Registry client is separate from producer/consumer clients but shares the same connection pool.</p>"},{"location":"client_libraries/schema-registry/#registering-schemas","title":"Registering Schemas","text":""},{"location":"client_libraries/schema-registry/#json-schema","title":"JSON Schema","text":"RustGo <pre><code>use danube_client::{SchemaRegistryClient, SchemaType};\n\nlet json_schema = r#\"{\n    \"type\": \"object\",\n    \"properties\": {\n        \"user_id\": {\"type\": \"string\"},\n        \"event\": {\"type\": \"string\"},\n        \"timestamp\": {\"type\": \"integer\"}\n    },\n    \"required\": [\"user_id\", \"event\", \"timestamp\"]\n}\"#;\n\nlet schema_id = schema_client\n    .register_schema(\"user-events\")\n    .with_type(SchemaType::JsonSchema)\n    .with_schema_data(json_schema.as_bytes())\n    .with_description(\"User activity events v1\")\n    .execute()\n    .await?;\n\nprintln!(\"\u2705 Registered schema ID: {}\", schema_id);\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n</code></pre>"},{"location":"client_libraries/schema-registry/#avro-schema","title":"Avro Schema","text":"RustGo <pre><code>use danube_client::{SchemaRegistryClient, SchemaType};\n\nlet avro_schema = r#\"{\n    \"type\": \"record\",\n    \"name\": \"UserEvent\",\n    \"namespace\": \"com.example\",\n    \"fields\": [\n        {\"name\": \"user_id\", \"type\": \"string\"},\n        {\"name\": \"event\", \"type\": \"string\"},\n        {\"name\": \"timestamp\", \"type\": \"long\"},\n        {\"name\": \"metadata\", \"type\": [\"null\", \"string\"], \"default\": null}\n    ]\n}\"#;\n\nlet schema_id = schema_client\n    .register_schema(\"user-events-avro\")\n    .with_type(SchemaType::Avro)\n    .with_schema_data(avro_schema.as_bytes())\n    .execute()\n    .await?;\n\nprintln!(\"\u2705 Registered Avro schema: {}\", schema_id);\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n</code></pre>"},{"location":"client_libraries/schema-registry/#idempotent-registration","title":"Idempotent Registration","text":"<p>Registering the same schema content multiple times returns the existing schema ID:</p> Rust <pre><code>// First registration\nlet id1 = schema_client\n    .register_schema(\"events\")\n    .with_type(SchemaType::JsonSchema)\n    .with_schema_data(schema.as_bytes())\n    .execute()\n    .await?;\n\n// Subsequent registration of same content\nlet id2 = schema_client\n    .register_schema(\"events\")\n    .with_type(SchemaType::JsonSchema)\n    .with_schema_data(schema.as_bytes())\n    .execute()\n    .await?;\n\nassert_eq!(id1, id2);  // Same ID returned\n</code></pre>"},{"location":"client_libraries/schema-registry/#retrieving-schemas","title":"Retrieving Schemas","text":""},{"location":"client_libraries/schema-registry/#get-latest-version","title":"Get Latest Version","text":"Rust <pre><code>use danube_client::SchemaInfo;\n\nlet schema: SchemaInfo = schema_client\n    .get_latest_schema(\"user-events\")\n    .await?;\n\nprintln!(\"Schema ID: {}\", schema.schema_id);\nprintln!(\"Subject: {}\", schema.subject);\nprintln!(\"Version: {}\", schema.version);\nprintln!(\"Type: {}\", schema.schema_type);\n\n// Get schema definition as string (for JSON-based schemas)\nif let Some(schema_str) = schema.schema_definition_as_string() {\n    println!(\"Schema: {}\", schema_str);\n}\n</code></pre> <p>Returns: <code>SchemaInfo</code> - A user-friendly wrapper containing:</p> <ul> <li><code>schema_id</code> - Global schema identifier</li> <li><code>subject</code> - Schema subject name  </li> <li><code>version</code> - Schema version number</li> <li><code>schema_type</code> - Type (avro, json, protobuf)</li> <li><code>schema_definition</code> - Raw bytes</li> <li><code>fingerprint</code> - Deduplication hash</li> </ul>"},{"location":"client_libraries/schema-registry/#get-schema-by-id","title":"Get Schema by ID","text":"RustGo <pre><code>// When consuming messages, you get schema_id in message metadata\nif let Some(schema_id) = message.schema_id {\n    let schema: SchemaInfo = schema_client\n        .get_schema_by_id(schema_id)\n        .await?;\n\n    println!(\"Schema subject: {}\", schema.subject);\n    println!(\"Schema version: {}\", schema.version);\n}\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n</code></pre>"},{"location":"client_libraries/schema-registry/#list-all-versions","title":"List All Versions","text":"RustGo <pre><code>let versions = schema_client\n    .list_versions(\"user-events\")\n    .await?;\n\nprintln!(\"Available versions: {:?}\", versions);  // e.g., [1, 2, 3]\n</code></pre> <pre><code>// Schema Registry not yet available in Go client\n</code></pre>"},{"location":"client_libraries/schema-registry/#compatibility-checking","title":"Compatibility Checking","text":""},{"location":"client_libraries/schema-registry/#check-before-registering","title":"Check Before Registering","text":"Rust <pre><code>use danube_client::{SchemaType, CompatibilityMode};\n\nlet new_schema = r#\"{\n    \"type\": \"object\",\n    \"properties\": {\n        \"user_id\": {\"type\": \"string\"},\n        \"event\": {\"type\": \"string\"},\n        \"timestamp\": {\"type\": \"integer\"},\n        \"email\": {\"type\": \"string\"}\n    },\n    \"required\": [\"user_id\", \"event\", \"timestamp\"]\n}\"#;\n\n// Check compatibility before registering\nlet result = schema_client\n    .check_compatibility(\n        \"user-events\",\n        new_schema.as_bytes(),\n        SchemaType::JsonSchema,\n        None,  // Use subject's default mode\n    )\n    .await?;\n\nif result.is_compatible {\n    println!(\"\u2705 Safe to register!\");\n\n    // Now register\n    schema_client\n        .register_schema(\"user-events\")\n        .with_type(SchemaType::JsonSchema)\n        .with_schema_data(new_schema.as_bytes())\n        .execute()\n        .await?;\n} else {\n    eprintln!(\"\u274c Incompatible: {:?}\", result.errors);\n}\n</code></pre> <p>Compatibility Modes:</p> <p>Compatibility modes control schema evolution at the subject level. This is an admin-only operation via admin CLI.</p> Mode Description Use Case <code>Backward</code> New schema reads old data Consumers upgrade first (default) <code>Forward</code> Old schema reads new data Producers upgrade first <code>Full</code> Both backward + forward Critical schemas <code>None</code> No validation Development only <p>Note: Clients cannot set compatibility mode. This is controlled by administrators using the admin CLI:</p> <pre><code># Admin CLI command (not available in client SDK)\ndanube-admin-cli schema set-compatibility \\\n  --subject user-events \\\n  --mode BACKWARD\n</code></pre>"},{"location":"client_libraries/schema-registry/#producer-with-schema","title":"Producer with Schema","text":""},{"location":"client_libraries/schema-registry/#option-1-use-latest-schema-version-most-common","title":"Option 1: Use Latest Schema Version (Most Common)","text":"Rust <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SchemaType};\nuse serde::Serialize;\n\n#[derive(Serialize)]\nstruct UserEvent {\n    user_id: String,\n    event: String,\n    timestamp: i64,\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    // 1. Register schema\n    let schema = r#\"{\n        \"type\": \"object\",\n        \"properties\": {\n            \"user_id\": {\"type\": \"string\"},\n            \"event\": {\"type\": \"string\"},\n            \"timestamp\": {\"type\": \"integer\"}\n        },\n        \"required\": [\"user_id\", \"event\", \"timestamp\"]\n    }\"#;\n\n    let mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n    schema_client\n        .register_schema(\"user-events\")\n        .with_type(SchemaType::JsonSchema)\n        .with_schema_data(schema.as_bytes())\n        .execute()\n        .await?;\n\n    // 2. Create producer with schema reference (uses latest version)\n    let mut producer = client\n        .producer()\n        .with_topic(\"/default/user-events\")\n        .with_name(\"event-producer\")\n        .with_schema_subject(\"user-events\")  // Uses latest version\n        .build();\n\n    producer.create().await?;\n\n    // 3. Send typed messages\n    let event = UserEvent {\n        user_id: \"user-123\".to_string(),\n        event: \"login\".to_string(),\n        timestamp: 1234567890,\n    };\n\n    let json_bytes = serde_json::to_vec(&amp;event)?;\n    let msg_id = producer.send(json_bytes, None).await?;\n    println!(\"\ud83d\udce4 Sent message: {}\", msg_id);\n\n    Ok(())\n}\n</code></pre>"},{"location":"client_libraries/schema-registry/#option-2-pin-to-specific-version","title":"Option 2: Pin to Specific Version","text":"Rust <pre><code>// Pin producer to specific schema version\nlet mut producer = client\n    .producer()\n    .with_topic(\"/default/user-events\")\n    .with_name(\"producer-v2\")\n    .with_schema_version(\"user-events\", 2)  // Pin to version 2\n    .build();\n\nproducer.create().await?;\n\n// This producer will always use version 2, even if v3+ exists\n</code></pre> <p>Use cases:</p> <ul> <li>Legacy applications that haven't upgraded</li> <li>Testing specific schema versions</li> <li>Gradual rollout of new versions</li> </ul>"},{"location":"client_libraries/schema-registry/#option-3-use-minimum-version","title":"Option 3: Use Minimum Version","text":"Rust <pre><code>// Use version 2 or any newer compatible version\nlet mut producer = client\n    .producer()\n    .with_topic(\"/default/user-events\")\n    .with_name(\"producer-min-v2\")\n    .with_schema_min_version(\"user-events\", 2)  // v2 or newer\n    .build();\n\nproducer.create().await?;\n\n// Will use v2, v3, v4, etc. (latest compatible version)\n</code></pre> <p>Use cases:</p> <ul> <li>Require minimum feature set from schema</li> <li>Allow automatic upgrades to compatible versions</li> <li>Deprecate old schema versions</li> </ul>"},{"location":"client_libraries/schema-registry/#first-producer-privilege","title":"First Producer Privilege","text":"<p>Important: The first producer to create a topic assigns its schema subject to that topic.</p> Rust <pre><code>// First producer - assigns schema to topic\nlet first = client.producer()\n    .with_topic(\"new-topic\")\n    .with_schema_subject(\"user-events\")  // \u2705 Sets topic's schema\n    .build();\nfirst.create().await?;\n\n// Second producer - must match\nlet second = client.producer()\n    .with_topic(\"new-topic\")\n    .with_schema_subject(\"user-events\")  // \u2705 Matches, allowed\n    .build();\nsecond.create().await?;\n\n// Third producer - mismatch!\nlet third = client.producer()\n    .with_topic(\"new-topic\")\n    .with_schema_subject(\"order-events\")  // \u274c ERROR: Different subject!\n    .build();\nthird.create().await?;  // Returns error\n</code></pre> <p>Error:</p> <pre><code>Topic 'new-topic' is configured with subject 'user-events',\ncannot use subject 'order-events'. Only admin can change topic schema.\n</code></pre> <p>Note: Once a topic has a schema subject, only administrators can change it via admin CLI.</p> Go <pre><code>// Schema Registry not yet available in Go client\n</code></pre>"},{"location":"client_libraries/schema-registry/#consumer-with-schema","title":"Consumer with Schema","text":""},{"location":"client_libraries/schema-registry/#option-1-trust-broker-validation-recommended","title":"Option 1: Trust Broker Validation (Recommended)","text":"Rust <pre><code>use danube_client::{DanubeClient, SubType};\nuse serde::Deserialize;\n\n#[derive(Deserialize, Debug)]\nstruct UserEvent {\n    user_id: String,\n    event: String,\n    timestamp: i64,\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut consumer = client\n        .consumer()\n        .with_topic(\"/default/user-events\")\n        .with_consumer_name(\"event-consumer\")\n        .with_subscription(\"event-sub\")\n        .with_subscription_type(SubType::Exclusive)\n        .build();\n\n    consumer.subscribe().await?;\n\n    // If topic has ValidationPolicy::Enforce, broker validates everything\n    while let Some(message) = consumer.receive().await? {\n        // Broker already validated schema, safe to deserialize\n        let event: UserEvent = serde_json::from_slice(&amp;message.payload)?;\n        println!(\"\ud83d\udce5 Event: {:?}\", event);\n        consumer.ack(&amp;message).await?;\n    }\n\n    Ok(())\n}\n</code></pre> <p>When to use:</p> <ul> <li>Topic has <code>ValidationPolicy::Enforce</code> (strict broker validation)</li> <li>Trust broker-side validation</li> <li>Best performance (no extra schema lookups)</li> </ul>"},{"location":"client_libraries/schema-registry/#option-2-client-side-validation-untrusted-sources","title":"Option 2: Client-Side Validation (Untrusted Sources)","text":"Rust <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SchemaInfo, SubType};\nuse serde::Deserialize;\n\n#[derive(Deserialize, Debug)]\nstruct UserEvent {\n    user_id: String,\n    event: String,\n    timestamp: i64,\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut consumer = client\n        .consumer()\n        .with_topic(\"/default/user-events\")\n        .with_consumer_name(\"event-consumer\")\n        .with_subscription(\"event-sub\")\n        .build();\n\n    consumer.subscribe().await?;\n\n    // Create schema client for validation\n    let mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\n    while let Some(message) = consumer.receive().await? {\n        // Fetch schema for validation\n        if let Some(schema_id) = message.schema_id {\n            let schema: SchemaInfo = schema_client\n                .get_schema_by_id(schema_id)\n                .await?;\n\n            // Optional: Validate payload against schema definition\n            // (implement your validation logic here)\n\n            println!(\"Message from schema: {} v{}\", \n                schema.subject, schema.version);\n        }\n\n        // Deserialize and process\n        let event: UserEvent = serde_json::from_slice(&amp;message.payload)?;\n        println!(\"\ud83d\udce5 Event: {:?}\", event);\n        consumer.ack(&amp;message).await?;\n    }\n\n    Ok(())\n}\n</code></pre> <p>When to use:</p> <ul> <li>Topic has <code>ValidationPolicy::None</code> or <code>Warn</code></li> <li>Need extra validation beyond broker</li> <li>Compliance/audit requirements</li> <li>Untrusted data sources</li> </ul>"},{"location":"client_libraries/schema-registry/#option-3-cache-schemas-locally-best-performance-validation","title":"Option 3: Cache Schemas Locally (Best Performance + Validation)","text":"Rust <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SchemaInfo};\nuse std::collections::HashMap;\n\n// Schema cache\nstruct SchemaCache {\n    client: SchemaRegistryClient,\n    cache: HashMap&lt;u64, SchemaInfo&gt;,\n}\n\nimpl SchemaCache {\n    async fn get_schema(&amp;mut self, schema_id: u64) -&gt; Result&lt;SchemaInfo, Box&lt;dyn std::error::Error&gt;&gt; {\n        // Check cache first\n        if let Some(schema) = self.cache.get(&amp;schema_id) {\n            return Ok(schema.clone());  // Cache hit!\n        }\n\n        // Cache miss - fetch from registry\n        let schema = self.client.get_schema_by_id(schema_id).await?;\n        self.cache.insert(schema_id, schema.clone());\n        Ok(schema)\n    }\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    let mut consumer = client\n        .consumer()\n        .with_topic(\"/default/user-events\")\n        .with_consumer_name(\"cached-consumer\")\n        .with_subscription(\"cached-sub\")\n        .build();\n\n    consumer.subscribe().await?;\n\n    // Initialize cache\n    let mut cache = SchemaCache {\n        client: SchemaRegistryClient::new(&amp;client).await?,\n        cache: HashMap::new(),\n    };\n\n    while let Some(message) = consumer.receive().await? {\n        if let Some(schema_id) = message.schema_id {\n            // Fast cached lookup\n            let schema = cache.get_schema(schema_id).await?;\n            println!(\"Using schema {} v{}\", schema.subject, schema.version);\n        }\n\n        // Process message\n        let event: UserEvent = serde_json::from_slice(&amp;message.payload)?;\n        consumer.ack(&amp;message).await?;\n    }\n\n    Ok(())\n}\n</code></pre> <p>When to use:</p> <ul> <li>High-throughput consumers</li> <li>Need validation but want performance</li> <li>Schemas don't change frequently</li> </ul>"},{"location":"client_libraries/schema-registry/#comparison-of-validation-strategies","title":"Comparison of Validation Strategies","text":"Strategy Latency Safety Use Case Trust broker Lowest High (if Enforce) Production with strict policy Always fetch Highest Highest Untrusted sources, audit Cache locally Medium Highest High-throughput + validation"},{"location":"client_libraries/schema-registry/#schema-evolution-example","title":"Schema Evolution Example","text":""},{"location":"client_libraries/schema-registry/#adding-optional-field-backward-compatible","title":"Adding Optional Field (Backward Compatible)","text":"Rust <pre><code>use danube_client::{SchemaRegistryClient, SchemaType};\n\n// V1 Schema\nlet schema_v1 = r#\"{\n    \"type\": \"object\",\n    \"properties\": {\n        \"user_id\": {\"type\": \"string\"},\n        \"event\": {\"type\": \"string\"}\n    },\n    \"required\": [\"user_id\", \"event\"]\n}\"#;\n\n// Register V1\nlet mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\nschema_client\n    .register_schema(\"events\")\n    .with_type(SchemaType::JsonSchema)\n    .with_schema_data(schema_v1.as_bytes())\n    .execute()\n    .await?;\n\n// V2 Schema (add optional field)\nlet schema_v2 = r#\"{\n    \"type\": \"object\",\n    \"properties\": {\n        \"user_id\": {\"type\": \"string\"},\n        \"event\": {\"type\": \"string\"},\n        \"metadata\": {\"type\": \"string\"}\n    },\n    \"required\": [\"user_id\", \"event\"]\n}\"#;\n\n// Check compatibility\nlet compat = schema_client\n    .check_compatibility(\n        \"events\",\n        schema_v2.as_bytes(),\n        SchemaType::JsonSchema,\n        None,\n    )\n    .await?;\n\nif compat.is_compatible {\n    // Register V2\n    schema_client\n        .register_schema(\"events\")\n        .with_type(SchemaType::JsonSchema)\n        .with_schema_data(schema_v2.as_bytes())\n        .execute()\n        .await?;\n\n    println!(\"\u2705 Successfully evolved schema to V2\");\n}\n</code></pre> <p>Result:</p> <ul> <li>Old consumers can still read V2 messages (ignore extra field)</li> <li>New consumers can use <code>metadata</code> field</li> <li>No breaking changes</li> </ul>"},{"location":"client_libraries/schema-registry/#schema-types","title":"Schema Types","text":""},{"location":"client_libraries/schema-registry/#supported-types","title":"Supported Types","text":"Type Description Status <code>SchemaType::JsonSchema</code> JSON Schema validation \u2705 Production <code>SchemaType::Avro</code> Apache Avro binary \u2705 Registration ready <code>SchemaType::Protobuf</code> Protocol Buffers \u2705 Registration ready <code>SchemaType::String</code> UTF-8 text \u2705 Basic validation <code>SchemaType::Number</code> Numeric types \u2705 Basic validation <code>SchemaType::Bytes</code> Raw binary \u2705 No validation"},{"location":"client_libraries/schema-registry/#troubleshooting","title":"Troubleshooting","text":""},{"location":"client_libraries/schema-registry/#schema-registration-fails","title":"Schema Registration Fails","text":"<pre><code>Error: Schema validation failed\n</code></pre> <p>Solutions:</p> <ul> <li>Validate JSON/Avro schema syntax</li> <li>Check schema is well-formed</li> <li>Ensure schema type matches content</li> </ul>"},{"location":"client_libraries/schema-registry/#compatibility-check-fails","title":"Compatibility Check Fails","text":"<pre><code>Error: Incompatible schema: removing required field\n</code></pre> <p>Solutions:</p> <ul> <li>Make field optional instead of removing</li> <li>Use <code>CompatibilityMode::None</code> for development</li> <li>Review compatibility mode requirements</li> </ul>"},{"location":"client_libraries/schema-registry/#deserialization-errors","title":"Deserialization Errors","text":"<pre><code>Error: missing field `new_field`\n</code></pre> <p>Solutions:</p> <ul> <li>Make new fields <code>Option&lt;T&gt;</code> in Rust</li> <li>Add <code>#[serde(default)]</code> attribute</li> <li>Ensure schema evolution is backward compatible</li> </ul>"},{"location":"client_libraries/setup/","title":"Client Setup and Configuration","text":"<p>This guide covers how to configure and connect Danube clients to your broker.</p>"},{"location":"client_libraries/setup/#basic-connection","title":"Basic Connection","text":"<p>Connect to Danube broker with an gRPC endpoint:</p> RustGo <pre><code>use danube_client::DanubeClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = DanubeClient::builder()\n        .service_url(\"http://127.0.0.1:6650\")\n        .build()\n        .await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>import (\n    \"github.com/danube-messaging/danube-go\"\n)\n\nfunc main() {\n    client := danube.NewClient().ServiceURL(\"127.0.0.1:6650\").Build()\n}\n</code></pre> <p>Endpoint format: <code>http://host:port</code> or <code>https://host:port</code> for TLS</p>"},{"location":"client_libraries/setup/#tls-configuration","title":"TLS Configuration","text":"<p>For secure production environments, enable TLS encryption:</p> RustGo <pre><code>use danube_client::DanubeClient;\nuse rustls::crypto;\nuse tokio::sync::OnceCell;\n\nstatic CRYPTO_PROVIDER: OnceCell&lt;()&gt; = OnceCell::const_new();\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // Initialize crypto provider (required once)\n    CRYPTO_PROVIDER.get_or_init(|| async {\n        let crypto_provider = crypto::ring::default_provider();\n        crypto_provider\n            .install_default()\n            .expect(\"Failed to install default CryptoProvider\");\n    })\n    .await;\n\n    let client = DanubeClient::builder()\n        .service_url(\"https://127.0.0.1:6650\")\n        .with_tls(\"./certs/ca-cert.pem\")?\n        .build()\n        .await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>// TLS support coming soon\n</code></pre> <p>Requirements:</p> <ul> <li>CA certificate file (PEM format)</li> <li>HTTPS URL (<code>https://</code> instead of <code>http://</code>)</li> <li>Broker must be configured with TLS enabled</li> </ul> <p>Certificate paths:</p> <ul> <li>Relative: <code>./certs/ca-cert.pem</code></li> <li>Absolute: <code>/etc/danube/certs/ca-cert.pem</code></li> </ul>"},{"location":"client_libraries/setup/#jwt-authentication","title":"JWT Authentication","text":"<p>For authenticated environments, use API keys to obtain JWT tokens:</p> RustGo <pre><code>use danube_client::DanubeClient;\nuse rustls::crypto;\nuse tokio::sync::OnceCell;\n\nstatic CRYPTO_PROVIDER: OnceCell&lt;()&gt; = OnceCell::const_new();\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    CRYPTO_PROVIDER.get_or_init(|| async {\n        let crypto_provider = crypto::ring::default_provider();\n        crypto_provider\n            .install_default()\n            .expect(\"Failed to install default CryptoProvider\");\n    })\n    .await;\n\n    let api_key = std::env::var(\"DANUBE_API_KEY\")\n        .expect(\"DANUBE_API_KEY environment variable not set\");\n\n    let client = DanubeClient::builder()\n        .service_url(\"https://127.0.0.1:6650\")\n        .with_tls(\"./certs/ca-cert.pem\")?\n        .with_api_key(api_key)\n        .build()\n        .await?;\n\n    Ok(())\n}\n</code></pre> <pre><code>// JWT authentication support coming soon\n</code></pre> <p>How it works:</p> <ol> <li>Client exchanges API key for JWT token on first request</li> <li>Token is cached and automatically renewed when expired</li> <li>Token included in <code>Authorization</code> header for all requests</li> <li>Default token lifetime: 1 hour</li> </ol> <p>Security best practices:</p> <ul> <li>Store API keys in environment variables</li> <li>Never hardcode API keys in source code</li> <li>Use different API keys per environment (dev/staging/prod)</li> <li>Rotate API keys regularly</li> </ul>"},{"location":"client_libraries/setup/#connection-options","title":"Connection Options","text":""},{"location":"client_libraries/setup/#connection-pooling","title":"Connection Pooling","text":"<p>Clients automatically manage connection pools. Multiple producers/consumers share underlying connections efficiently.</p> Rust <pre><code>let client = DanubeClient::builder()\n    .service_url(\"http://127.0.0.1:6650\")\n    .build()\n    .await?;\n\n// All producers/consumers share the same connection pool\nlet producer1 = client.new_producer().with_topic(\"/topic1\").build();\nlet producer2 = client.new_producer().with_topic(\"/topic2\").build();\nlet consumer = client.new_consumer().with_topic(\"/topic1\").build();\n</code></pre>"},{"location":"client_libraries/setup/#service-discovery","title":"Service Discovery","text":"<p>For clustered deployments, the client performs automatic topic lookup:</p> <pre><code>// Client connects to any broker in the cluster\nlet client = DanubeClient::builder()\n    .service_url(\"&lt;http://broker1:6650&gt;\")\n    .build()\n    .await?;\n\n// Topic lookup finds the owning broker\nlet producer = client.new_producer()\n    .with_topic(\"/default/my-topic\")\n    .build();\n\n// Producer connects to the correct broker automatically\nproducer.create().await?;\n</code></pre>"},{"location":"client_libraries/setup/#environment-based-configuration","title":"Environment-Based Configuration","text":"<pre><code># Production\nexport DANUBE_URL=https://danube.example.com:6650\nexport DANUBE_CA_CERT=/etc/danube/certs/ca.pem\nexport DANUBE_API_KEY=your-secret-api-key\n</code></pre>"},{"location":"client_libraries/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"client_libraries/setup/#connection-refused","title":"Connection Refused","text":"<pre><code>Error: Connection refused (os error 111)\n</code></pre> <p>Solutions:</p> <ul> <li>Verify broker is running: <code>curl http://localhost:6650</code></li> <li>Check firewall rules</li> <li>Confirm correct host and port</li> </ul>"},{"location":"client_libraries/setup/#tls-certificate-errors","title":"TLS Certificate Errors","text":"<pre><code>Error: InvalidCertificate\n</code></pre> <p>Solutions:</p> <ul> <li>Verify CA certificate path is correct</li> <li>Ensure certificate is PEM format</li> <li>Check certificate hasn't expired</li> <li>Confirm broker TLS configuration matches client</li> </ul>"},{"location":"client_libraries/setup/#authentication-failures","title":"Authentication Failures","text":"<pre><code>Error: Unauthenticated\n</code></pre> <p>Solutions:</p> <ul> <li>Verify API key is valid</li> <li>Check broker authentication mode (tls vs tlswithjwt)</li> <li>Ensure token hasn't expired (client auto-renews, but check logs)</li> </ul>"},{"location":"client_libraries/setup/#next-steps","title":"Next Steps","text":"<ul> <li>Producer Basics - Start sending messages</li> <li>Consumer Basics - Start receiving messages</li> <li>Schema Registry - Add type safety with schemas</li> </ul>"},{"location":"concepts/dispatch_strategy/","title":"Dispatch Strategy","text":"<p>The dispatch strategies in Danube represent two distinct approaches to message delivery, each serving different use cases:</p>"},{"location":"concepts/dispatch_strategy/#non-reliable-dispatch-strategy","title":"Non-Reliable Dispatch Strategy","text":"<p>This strategy prioritizes speed and minimal resource usage by delivering messages directly from producers to subscribers without persistence. Messages flow through the broker in a \"fire and forget\" manner, achieving the lowest possible latency. It fits real-time metrics, live telemetry, or any workload where occasional loss is acceptable.</p> <p>Writer path (producer)</p> <ul> <li>The producer sends a message to the broker specifying the topic.</li> <li>The broker validates and routes the message to the topic\u2019s dispatcher.</li> <li>Depending on subscription type (Exclusive/Shared/Failover), the dispatcher selects the target consumer(s).</li> <li>The message is immediately forwarded to consumer channels. There is no on-disk persistence and no acknowledgment gating.</li> </ul> <p>Reader path (consumer)</p> <ul> <li>A consumer subscribes to a topic under an existing subscription (Exclusive/Shared/Failover).</li> <li>The broker registers the consumer and attaches a live message stream to it.</li> <li>The dispatcher pushes incoming messages directly to the consumer stream.</li> <li>Acknowledgments are optional and do not affect delivery; if a consumer disconnects, messages in flight may be lost.</li> </ul>"},{"location":"concepts/dispatch_strategy/#reliable-dispatch-strategy","title":"Reliable Dispatch Strategy","text":"<p>This strategy ensures at-least-once delivery using a WAL + Cloud store-and-forward design. Messages are appended to a local Write-Ahead Log (WAL) and asynchronously uploaded to cloud object storage. Delivery is coordinated by the subscription engine, which tracks progress and acknowledgments per subscription.</p> <p>Writer path (producer)</p> <ul> <li>The producer sends a message to the broker for a reliable topic.</li> <li>The message is appended to the local WAL (durable on disk) and becomes eligible for dispatch.</li> <li>The dispatcher prepares the message for the subscription type (Exclusive/Shared/Failover) while the subscription engine records it as pending.</li> <li>A background uploader asynchronously persists WAL frames to cloud object storage; this does not block producers.</li> </ul> <p>Reader path (consumer)</p> <ul> <li>A consumer subscribes to a reliable topic; the broker attaches a stream and initializes subscription progress.</li> <li>The dispatcher delivers messages according to the subscription type and ordering guarantees.</li> <li>The consumer acknowledges processed messages; the subscription engine advances progress and triggers redelivery if needed.</li> <li>If the consumer is late or reconnects after a gap, historical data is replayed from the WAL or, if needed, from cloud storage, then seamlessly handed off to the live WAL tail.</li> </ul> <p>These strategies embody Danube\u2019s flexibility, letting you choose the right balance between performance and reliability per topic. You can run non-reliable and reliable topics side by side in the same cluster.</p>"},{"location":"concepts/messages/","title":"Message Structure","text":"<p>Messages are the fundamental unit of data in Danube. Each message contains a payload plus rich metadata for routing, tracking, acknowledgment, and schema validation.</p>"},{"location":"concepts/messages/#streammessage-structure","title":"StreamMessage Structure","text":"<pre><code>pub struct StreamMessage {\n    pub request_id: u64,\n    pub msg_id: MessageID,\n    pub payload: Vec&lt;u8&gt;,\n    pub publish_time: u64,\n    pub producer_name: String,\n    pub subscription_name: Option&lt;String&gt;,\n    pub attributes: HashMap&lt;String, String&gt;,\n    pub schema_id: Option&lt;u64&gt;,\n    pub schema_version: Option&lt;u32&gt;,\n}\n</code></pre>"},{"location":"concepts/messages/#field-reference","title":"Field Reference","text":"Field Type Description <code>request_id</code> <code>u64</code> Unique identifier for tracking the message request across the system <code>msg_id</code> <code>MessageID</code> Composite identifier containing routing and location information <code>payload</code> <code>Vec&lt;u8&gt;</code> The actual message content in binary format <code>publish_time</code> <code>u64</code> Unix timestamp (milliseconds) when the message was published <code>producer_name</code> <code>String</code> Name of the producer that sent this message <code>subscription_name</code> <code>Option&lt;String&gt;</code> Name of the subscription (set by broker for consumer delivery) <code>attributes</code> <code>HashMap&lt;String, String&gt;</code> User-defined key-value pairs for custom metadata <code>schema_id</code> <code>Option&lt;u64&gt;</code> Schema Registry ID for message validation (see Schema Integration) <code>schema_version</code> <code>Option&lt;u32&gt;</code> Version of the schema used to serialize this message"},{"location":"concepts/messages/#messageid-structure","title":"MessageID Structure","text":"<p>The <code>MessageID</code> is a composite identifier that enables efficient routing and acknowledgment:</p> <pre><code>pub struct MessageID {\n    pub producer_id: u64,\n    pub topic_name: String,\n    pub broker_addr: String,\n    pub topic_offset: u64,\n}\n</code></pre>"},{"location":"concepts/messages/#field-reference_1","title":"Field Reference","text":"Field Type Description <code>producer_id</code> <code>u64</code> Unique identifier for the producer within this topic <code>topic_name</code> <code>String</code> Full topic name (e.g., <code>/default/events</code>) <code>broker_addr</code> <code>String</code> Address of the broker that delivered this message <code>topic_offset</code> <code>u64</code> Monotonic position of the message within the topic <p>Purpose:</p> <ul> <li>Routing: Broker address enables consumers to send acknowledgments to the correct broker</li> <li>Ordering: Topic offset provides strict ordering guarantees within a topic</li> <li>Deduplication: Combination of producer_id + topic_offset creates a unique message identifier</li> <li>Tracking: Request ID links messages across distributed tracing systems</li> </ul>"},{"location":"concepts/messages/#schema-integration","title":"Schema Integration","text":"<p>Danube integrates with the Schema Registry to provide type-safe messaging with schema validation.</p>"},{"location":"concepts/messages/#schema-fields-in-streammessage","title":"Schema Fields in StreamMessage","text":"<p><code>schema_id: Option&lt;u64&gt;</code></p> <p>Globally unique identifier assigned by the Schema Registry when a schema is registered.</p> <ul> <li>Present when producer uses schema-validated messages</li> <li>Consumers use this ID to fetch the schema from the registry</li> <li>Enables schema caching (8-byte overhead vs. sending full schema per message)</li> <li>Required when topic has validation policy set to <code>Enforce</code></li> </ul> <p><code>schema_version: Option&lt;u32&gt;</code></p> <p>Version number of the schema within its subject.</p> <ul> <li>Tracks which version of the schema was used to serialize this message</li> <li>Enables schema evolution tracking and debugging</li> <li>Allows consumers to handle multiple schema versions gracefully</li> <li>Starts at 1 and auto-increments with each schema update</li> </ul>"},{"location":"concepts/messages/#how-schema-validation-works","title":"How Schema Validation Works","text":""},{"location":"concepts/messages/#1-producer-flow","title":"1. Producer Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Producer   \u2502\n\u2502             \u2502\n\u2502 1. Register \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Schema   \u251c\u2500\u2500\u2500\u2500&gt;\u2502 Schema Registry  \u2502\n\u2502             \u2502     \u2502                  \u2502\n\u2502 2. Get ID   \u2502&lt;\u2500\u2500\u2500\u2500\u2524 Returns:         \u2502\n\u2502             \u2502     \u2502 - schema_id: 42  \u2502\n\u2502             \u2502     \u2502 - version: 1     \u2502\n\u2502 3. Serialize\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502    Message  \u2502\n\u2502             \u2502\n\u2502 4. Set      \u2502     StreamMessage {\n\u2502    Fields   \u2502       schema_id: Some(42),\n\u2502             \u2502       schema_version: Some(1),\n\u2502             \u2502       payload: &lt;serialized&gt;,\n\u2502 5. Send     \u2502       ...\n\u2502             \u2502     }\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       v\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Broker  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/messages/#2-broker-validation","title":"2. Broker Validation","text":"<p>When a message arrives, the broker validates schema fields based on the topic's validation policy:</p> Policy Behavior None No validation; schema fields optional Warn Logs warning if schema_id missing or invalid; allows message Enforce Rejects message if schema_id missing, invalid, or doesn't match topic requirements"},{"location":"concepts/messages/#3-consumer-flow","title":"3. Consumer Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Consumer   \u2502\n\u2502              \u2502\n\u2502 1. Receive   \u2502     StreamMessage {\n\u2502    Message   \u2502       schema_id: Some(42),\n\u2502              \u2502       schema_version: Some(1),\n\u2502 2. Extract   \u2502       payload: &lt;bytes&gt;,\n\u2502    schema_id \u2502       ...\n\u2502              \u2502     }\n\u2502 3. Fetch     \u2502\n\u2502    Schema    \u251c\u2500\u2500\u2500\u2500&gt;\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              \u2502     \u2502 Schema Registry  \u2502\n\u2502 4. Get Def   \u2502&lt;\u2500\u2500\u2500\u2500\u2524 Returns schema   \u2502\n\u2502              \u2502     \u2502 definition       \u2502\n\u2502 5. Deserialize\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502    Payload   \u2502\n\u2502              \u2502\n\u2502 6. Validate  \u2502     { user_id: 123, action: \"login\" }\n\u2502    Struct    \u2502\n\u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/messages/#benefits-of-schema-integration","title":"Benefits of Schema Integration","text":"<p>\u2705 Type Safety Messages are validated against a contract, preventing data corruption.</p> <p>\u2705 Bandwidth Efficiency Only 8-12 bytes overhead (schema_id + version) instead of full schema (potentially kilobytes).</p> <p>\u2705 Schema Evolution Consumers can handle multiple schema versions using the <code>schema_version</code> field.</p> <p>\u2705 Debugging Knowing which schema version produced a message simplifies troubleshooting.</p> <p>\u2705 Governance Centralized schema management with compatibility checking and audit trails.</p>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/","title":"Pub/Sub vs Streaming: Concepts and Danube Support","text":"<p>This page compares Pub/Sub messaging and Streaming architectures, highlights key differences and use cases, and indicates what Danube supports. Use it as a conceptual guide aligned with Danube\u2019s Reliable and Non\u2011Reliable dispatch modes.</p>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#pubsub-messaging","title":"Pub/Sub messaging","text":""},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#purpose-and-use-cases","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system.</li> <li>Use Cases: Event-driven architectures, real-time notifications, decoupled microservices, and distributed systems. Suitable when low latency is critical and some message loss is acceptable (e.g., monitoring, telemetry, ephemeral chat).</li> </ul> <p>Danube support: Yes, via Non\u2011Reliable dispatch.</p> <ul> <li>Delivery: best\u2011effort (at\u2011most\u2011once).</li> <li>Persistence: none; messages are not stored.</li> <li>Ordering: per topic/partition within a dispatcher.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#architecture-and-design","title":"Architecture and Design","text":"<ul> <li>Components: Producers, Subscriptions (consumer groups), and the broker.</li> <li>Message Flow: Producers send messages to a broker, which distributes them to active subscribers according to subscription semantics.</li> <li>Scaling: Scales by adding more brokers or distributing load (topics / partitions) across multiple brokers.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#data-handling-and-processing-models","title":"Data Handling and Processing Models","text":""},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#pubsub-messaging-producers","title":"Pub/Sub messaging Producers","text":"<ul> <li>Low Latency: No disk persistence on the hot path; minimal overhead.</li> <li>Ordering: Preserved per topic/partition through the dispatcher.</li> <li>Delivery: Best\u2011effort; messages can be lost on failure.</li> <li>Acks: Immediate, based on in\u2011memory handling.</li> <li>No Active Consumers: Publishing is allowed; messages are dropped if no consumers are attached.</li> </ul> <p>Danube: Matches Non\u2011Reliable dispatch semantics.</p>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#pubsub-messaging-consumers","title":"Pub/Sub messaging Consumers","text":"<ul> <li>Real-time: Messages are delivered only to connected consumers.</li> <li>No Replay: No historical fetch; process as they arrive.</li> <li>Throughput: High, with low overhead.</li> <li>Delivery: Only to active subscribers; otherwise dropped.</li> </ul> <p>Danube: Supported via Exclusive, Shared, Failover subscriptions in Non\u2011Reliable mode.</p>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#order-of-operations-of-pubsub-messaging","title":"Order of Operations of Pub/Sub messaging","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Broker Receives Message: The broker processes the message.</li> <li>Consumer Availability Check: If consumers are available, the message is delivered to them in real-time.</li> <li>No Consumers: If no consumers are connected, the message is discarded.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#streaming-design-considerations","title":"Streaming design considerations","text":""},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#purpose-and-use-cases_1","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for processing and analyzing large volumes of data in real-time as it is generated.</li> <li>Use Cases: Real-time analytics, data pipelines, event sourcing, continuous data processing, and stream processing applications. Ideal when high reliability and durability are required (e.g., financial transactions, orders, audit logs).</li> </ul> <p>Danube support: Yes, via Reliable dispatch (WAL + Cloud persistence).</p> <ul> <li>Delivery: at\u2011least\u2011once with ack\u2011gating.</li> <li>Persistence: local WAL for hot path, background uploads to object storage for durability and replay.</li> <li>Replay: historical fetch from WAL or Cloud with seamless handoff to live tail.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#architecture-and-design_1","title":"Architecture and Design","text":"<ul> <li>Components: Producers, consumers, stream processors, and a durable log.</li> <li>Data Flow: Producers append to a log; consumers/processors read continuously with tracked progress.</li> <li>Scaling: Designed to handle high throughput and scale horizontally by partitioning data across multiple nodes in a cluster.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#data-handling-and-processing-models_1","title":"Data Handling and Processing Models","text":""},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#streaming-producers","title":"Streaming Producers","text":"<ul> <li>Durability: Messages are appended to a persistent log. In Danube, the hot path is a local WAL with asynchronous uploads to cloud object storage, enabling replay and recovery.</li> <li>Acknowledgements: Acks are returned once the message is durably appended to the WAL (replication depends on deployment/storage backend). Latency is higher than non\u2011reliable but optimized for sub\u2011second dispatch.</li> <li>Ordering: Preserved per topic/partition.</li> <li>Delivery Guarantees: At\u2011least\u2011once (with redelivery on failure).</li> <li>Publishing Without Consumers: Allowed; messages are stored and remain available for later consumption within retention.</li> <li>Processing: Compatible with stream processing patterns (e.g., windowing, aggregations) layered above the durable log.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#streaming-consumers","title":"Streaming Consumers","text":"<ul> <li>Replay &amp; Retention: Consumers can fetch historical data within retention. In Danube, reads come from WAL if available, otherwise from Cloud with seamless handoff to the live tail.</li> <li>Acknowledgements: Consumers ack processed messages; the broker tracks subscription progress and manages redelivery.</li> <li>Fault Tolerance: On crash/reconnect, consumers resume from the last acknowledged position.</li> <li>Availability: Messages remain available even with no active consumers, subject to retention policies.</li> </ul>"},{"location":"concepts/messaging_modes_pubsub_vs_streaming/#order-of-operations-of-streaming","title":"Order of Operations of Streaming","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Durable Append: The broker appends the message to the WAL (durable on disk); background tasks upload frames to cloud storage.</li> <li>Producer Acknowledgment: The broker acknowledges once the append is durable.</li> <li>Delivery to Consumers: Messages are dispatched to consumers, gated by acknowledgments for reliable delivery.</li> <li>No Consumers: Messages remain stored and available for later consumption within retention.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/","title":"Messaging Patterns: Queuing vs Pub/Sub (Fan\u2011out)","text":"<p>This page explains the difference between queuing (point\u2011to\u2011point) and pub/sub (fan\u2011out) and shows how to enable each pattern in Danube using subscriptions and dispatch modes.</p> <ul> <li>For subscription details, see <code>architecture/subscriptions.md</code>.</li> <li>For delivery semantics, see <code>architecture/dispatch_strategy.md</code>.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#concepts-at-a-glance","title":"Concepts at a Glance","text":"<p>Queuing (Point\u2011to\u2011Point)</p> <ul> <li>One message is delivered to exactly one consumer in a group.</li> <li>Suited for work distribution (orders, tasks, jobs).</li> <li>Ordering is per consumer, and per partition when topics are partitioned.</li> </ul> <p>Pub/Sub (Fan\u2011out)</p> <ul> <li>Every subscription receives every message.</li> <li>Suited for broadcast (notifications, analytics, multiple services reacting to events).</li> <li>Each subscription processes the full stream independently.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#how-to-enable-in-danube","title":"How to enable in Danube","text":"<p>Both patterns are built with Danube subscriptions on a topic.</p> <p>Queuing (Point\u2011to\u2011Point)</p> <ul> <li>Set subscription type to <code>Shared</code>.</li> <li>Use the same subscription name across all workers (e.g., <code>orders-workers</code>).</li> <li>Result: Each message is delivered to one consumer in the group (round\u2011robin, per partition).</li> </ul> <p>Pub/Sub (Fan\u2011out)</p> <ul> <li>Create distinct subscriptions per downstream consumer/team (unique subscription names), typically using <code>Exclusive</code> (or <code>Failover</code> for HA).</li> <li>Result: Each subscription receives every message; consumers do not contend with other subscriptions.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#dispatch-modes-and-guarantees","title":"Dispatch modes and guarantees","text":"<p>Non\u2011Reliable (Pub/Sub\u2011style, best\u2011effort)</p> <ul> <li>Lowest latency; no persistence or replay.</li> <li>Messages are delivered only to active consumers. Disconnections may cause loss.</li> </ul> <p>Reliable (Streaming\u2011style, at\u2011least\u2011once)</p> <ul> <li>Messages are appended to the local WAL and asynchronously uploaded to cloud storage.</li> <li>Consumers can replay historical data; acknowledgments drive redelivery.</li> </ul> <p>See <code>architecture/persistence.md</code> for WAL + Cloud details.</p>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#partitioning-behavior","title":"Partitioning behavior","text":"<p>With partitioned topics, both patterns apply per partition.</p> <ul> <li>Queuing: messages are distributed round\u2011robin within each partition to the consumers in that subscription.</li> <li>Pub/Sub: each subscription receives each partition\u2019s messages independently.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#quick-examples","title":"Quick examples","text":"<p>Build a worker queue</p> <ul> <li>Topic: <code>/default/orders</code></li> <li>Subscription: <code>orders-workers</code> (type <code>Shared</code>)</li> <li>Run N consumers with the same subscription name. Messages are load\u2011balanced.</li> </ul> <p>Broadcast to multiple services</p> <ul> <li>Topic: <code>/default/events</code></li> <li>Subscriptions: <code>billing</code> (Exclusive), <code>analytics</code> (Exclusive), <code>monitoring</code> (Failover)</li> <li>Each subscription receives the full event stream independently.</li> </ul>"},{"location":"concepts/messaging_patterns_queuing_vs_pubsub/#notes-best-practices","title":"Notes &amp; best practices","text":"<ul> <li>Use Reliable mode for durability and replay; Non\u2011Reliable for minimal latency when loss is acceptable.</li> <li>Prefer <code>Failover</code> over <code>Exclusive</code> when you need quick takeover on consumer failure.</li> <li>Size partitions to match consumer parallelism for optimal throughput.</li> </ul>"},{"location":"concepts/schema_registry_guide/","title":"Schema Registry User Guide","text":"<p>This guide explains how to use Danube's Schema Registry from both the client SDK (application code) and the admin CLI (cluster administration). It clarifies what operations are available at which level and who can perform them.</p>"},{"location":"concepts/schema_registry_guide/#quick-reference-who-can-do-what","title":"Quick Reference: Who Can Do What?","text":"Operation Client SDK Admin CLI Level Register schema \u2705 Yes \u2705 Yes Subject Set compatibility mode \u274c No \u2705 Yes Subject Assign schema to topic \u2705 First producer \u2705 Yes (override) Topic Set validation policy \u274c No \u2705 Yes Topic Enable payload validation \u274c No \u2705 Yes Topic Choose schema version \u2705 Yes N/A Producer Fetch schemas \u2705 Yes \u2705 Yes - Delete schema version \u274c No \u2705 Yes Subject"},{"location":"concepts/schema_registry_guide/#understanding-policy-levels","title":"Understanding Policy Levels","text":"<p>Danube has two policy levels that control different aspects of schema management:</p>"},{"location":"concepts/schema_registry_guide/#subject-level-applies-to-schema-evolution","title":"Subject-Level (Applies to Schema Evolution)","text":"<p>What it controls: How schemas can evolve over time</p> <p>Set by: Administrators only</p> <p>Stored at: <code>/schemas/{subject}/compatibility</code></p> <p>Applies to: All topics using this subject inherit the same compatibility rules</p> <p>Settings:</p> <ul> <li>Compatibility Mode - <code>BACKWARD</code> | <code>FORWARD</code> | <code>FULL</code> | <code>NONE</code></li> </ul> <p>Example:</p> <pre><code>SUBJECT: \"user-events-value\"\n\u251c\u2500 Compatibility Mode: BACKWARD (subject-level)\n\u251c\u2500 Version 1, 2, 3...\n\u2514\u2500 Used by multiple topics (all inherit BACKWARD mode)\n</code></pre>"},{"location":"concepts/schema_registry_guide/#topic-level-applies-to-message-validation","title":"Topic-Level (Applies to Message Validation)","text":"<p>What it controls: How strictly the broker enforces schema validation for messages</p> <p>Set by: Administrators only</p> <p>Stored at: <code>/topics/{topic}/schema_config</code></p> <p>Applies to: Only that specific topic</p> <p>Settings:</p> <ul> <li>Schema Subject - Which schema this topic uses</li> <li>Validation Policy - <code>NONE</code> | <code>WARN</code> | <code>ENFORCE</code></li> <li>Enable Payload Validation - <code>true</code> | <code>false</code></li> </ul> <p>Example:</p> <pre><code>TOPIC: \"user-events-dev\"\n\u251c\u2500 Schema Subject: \"user-events-value\"\n\u251c\u2500 Validation Policy: WARN (lenient for dev)\n\u2514\u2500 Payload Validation: false\n\nTOPIC: \"user-events-prod\"\n\u251c\u2500 Schema Subject: \"user-events-value\" (SAME SUBJECT!)\n\u251c\u2500 Validation Policy: ENFORCE (strict for prod)\n\u2514\u2500 Payload Validation: true\n</code></pre>"},{"location":"concepts/schema_registry_guide/#client-sdk-operations-application-code","title":"Client SDK Operations (Application Code)","text":"<p>Use the client SDK in your application code to interact with schemas during normal operation.</p>"},{"location":"concepts/schema_registry_guide/#1-register-a-new-schema","title":"1. Register a New Schema","text":"<p>Who can: Any producer/application When: Before creating producers, or when evolving schemas Level: Subject</p> <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SchemaType};\n\nlet client = DanubeClient::builder()\n    .service_url(\"http://localhost:6650\")\n    .build()\n    .await?;\n\nlet mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\n// Register Avro schema\nlet avro_schema = r#\"\n{\n  \"type\": \"record\",\n  \"name\": \"UserEvent\",\n  \"fields\": [\n    {\"name\": \"user_id\", \"type\": \"string\"},\n    {\"name\": \"action\", \"type\": \"string\"}\n  ]\n}\n\"#;\n\nlet schema_id = schema_client\n    .register_schema(\"user-events-value\")\n    .with_type(SchemaType::Avro)\n    .with_schema_data(avro_schema.as_bytes())\n    .execute()\n    .await?;\n\nprintln!(\"\u2705 Registered schema ID: {}\", schema_id);\n</code></pre> <p>What happens:</p> <ol> <li>Creates new subject if it doesn't exist (with default <code>BACKWARD</code> compatibility)</li> <li>OR adds new version to existing subject (checks compatibility automatically)</li> <li>Returns globally unique schema ID</li> <li>Cannot set compatibility mode (admin-only)</li> </ol>"},{"location":"concepts/schema_registry_guide/#2-fetch-schema-information","title":"2. Fetch Schema Information","text":"<p>Who can: Any consumer/application When: To deserialize messages or validate payloads Level: N/A (read-only)</p> <pre><code>use danube_client::{SchemaRegistryClient, SchemaInfo};\n\nlet mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\n// Get latest schema for a subject\nlet schema: SchemaInfo = schema_client\n    .get_latest_schema(\"user-events-value\")\n    .await?;\n\nprintln!(\"Schema ID: {}\", schema.schema_id);\nprintln!(\"Version: {}\", schema.version);\nprintln!(\"Type: {}\", schema.schema_type);\n\n// Get schema by ID (from message metadata)\nif let Some(schema_id) = message.schema_id {\n    let schema: SchemaInfo = schema_client\n        .get_schema_by_id(schema_id)\n        .await?;\n\n    // Access schema definition\n    if let Some(schema_str) = schema.schema_definition_as_string() {\n        println!(\"Schema: {}\", schema_str);\n    }\n}\n</code></pre>"},{"location":"concepts/schema_registry_guide/#3-create-producer-with-schema-first-producer-privilege","title":"3. Create Producer with Schema (First Producer Privilege)","text":"<p>Who can: Any producer When: Creating a new producer Level: Topic (assigns schema subject to topic)</p> <pre><code>use danube_client::DanubeClient;\n\n// Option A: Use latest schema version (most common)\nlet mut producer = client\n    .producer()\n    .with_topic(\"/default/user-events\")\n    .with_name(\"user_events_producer\")\n    .with_schema_subject(\"user-events-value\")  // Links to schema\n    .build();\n\nproducer.create().await?;\n\n// Option B: Pin to specific version\nlet mut producer_v2 = client\n    .producer()\n    .with_topic(\"/default/user-events\")\n    .with_name(\"producer_v2\")\n    .with_schema_version(\"user-events-value\", 2)  // Use version 2\n    .build();\n\nproducer_v2.create().await?;\n\n// Option C: Use minimum version\nlet mut producer_min = client\n    .producer()\n    .with_topic(\"/default/user-events\")\n    .with_name(\"producer_min\")\n    .with_schema_min_version(\"user-events-value\", 2)  // v2 or newer\n    .build();\n\nproducer_min.create().await?;\n</code></pre> <p>First Producer Privilege:</p> <p>The first producer to create a topic automatically assigns its schema subject to that topic:</p> <pre><code>// First producer - assigns schema subject\nlet first = client.producer()\n    .with_topic(\"new-topic\")\n    .with_schema_subject(\"user-events-value\")  // \u2705 Sets topic's schema\n    .build();\nfirst.create().await?;\n\n// Second producer - must match\nlet second = client.producer()\n    .with_topic(\"new-topic\")\n    .with_schema_subject(\"user-events-value\")  // \u2705 Matches, allowed\n    .build();\nsecond.create().await?;\n\n// Third producer - mismatch!\nlet third = client.producer()\n    .with_topic(\"new-topic\")\n    .with_schema_subject(\"order-events-value\")  // \u274c ERROR: Different subject!\n    .build();\nthird.create().await?;  // Returns error\n</code></pre> <p>Error:</p> <pre><code>Topic 'new-topic' is configured with subject 'user-events-value',\ncannot use subject 'order-events-value'. Only admin can change topic schema.\n</code></pre>"},{"location":"concepts/schema_registry_guide/#4-consumer-schema-validation-optional","title":"4. Consumer Schema Validation (Optional)","text":"<p>Who can: Any consumer When: Receiving messages Level: Client-side validation</p> <pre><code>use danube_client::{DanubeClient, SchemaRegistryClient, SchemaInfo, SubType};\n\nlet mut consumer = client\n    .consumer()\n    .with_topic(\"/default/user-events\")\n    .with_consumer_name(\"my_consumer\")\n    .with_subscription(\"my-subscription\")\n    .build();\n\nconsumer.subscribe().await?;\n\n// Option 1: Trust broker validation (recommended if ValidationPolicy = ENFORCE)\nwhile let Some(message) = consumer.receive().await? {\n    let event: UserEvent = serde_json::from_slice(&amp;message.payload)?;\n    process_event(event).await?;\n    consumer.ack(&amp;message).await?;\n}\n\n// Option 2: Client-side validation (if broker policy is WARN or NONE)\nlet mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\nwhile let Some(message) = consumer.receive().await? {\n    // Fetch schema for validation\n    if let Some(schema_id) = message.schema_id {\n        let schema: SchemaInfo = schema_client.get_schema_by_id(schema_id).await?;\n\n        // Validate payload against schema\n        if !validate_payload(&amp;message.payload, &amp;schema) {\n            eprintln!(\"Invalid message: {:?}\", message.msg_id);\n            consumer.nack(&amp;message).await?;\n            continue;\n        }\n    }\n\n    // Process validated message\n    let event: UserEvent = serde_json::from_slice(&amp;message.payload)?;\n    process_event(event).await?;\n    consumer.ack(&amp;message).await?;\n}\n</code></pre> <p>Validation Strategies:</p> Strategy Performance Safety When to Use Trust broker Fastest High (if Enforce) Production with strict validation policy Fetch per message Slowest Highest Untrusted sources, audit requirements Cache locally Medium Highest High-throughput + validation needs"},{"location":"concepts/schema_registry_guide/#5-check-compatibility-before-registering","title":"5. Check Compatibility Before Registering","text":"<p>Who can: Any producer/application When: Before registering a new schema version Level: Subject</p> <pre><code>use danube_client::{SchemaRegistryClient, SchemaType};\n\nlet mut schema_client = SchemaRegistryClient::new(&amp;client).await?;\n\nlet new_schema = r#\"\n{\n  \"type\": \"record\",\n  \"name\": \"UserEvent\",\n  \"fields\": [\n    {\"name\": \"user_id\", \"type\": \"string\"},\n    {\"name\": \"action\", \"type\": \"string\"},\n    {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}\n  ]\n}\n\"#;\n\n// Check compatibility before registering\nlet result = schema_client\n    .check_compatibility(\n        \"user-events-value\",\n        new_schema.as_bytes().to_vec(),\n        SchemaType::Avro,\n        None,  // Use subject's compatibility mode\n    )\n    .await?;\n\nif result.is_compatible {\n    println!(\"\u2705 Schema is compatible!\");\n\n    // Safe to register\n    let schema_id = schema_client\n        .register_schema(\"user-events-value\")\n        .with_type(SchemaType::Avro)\n        .with_schema_data(new_schema.as_bytes())\n        .execute()\n        .await?;\n\n    println!(\"Registered as schema ID: {}\", schema_id);\n} else {\n    println!(\"\u274c Schema incompatible!\");\n    for error in result.errors {\n        println!(\"  - {}\", error);\n    }\n}\n</code></pre>"},{"location":"concepts/schema_registry_guide/#admin-cli-operations-cluster-administration","title":"Admin CLI Operations (Cluster Administration)","text":"<p>Use the admin CLI to configure cluster-wide schema settings and topic-level policies.</p>"},{"location":"concepts/schema_registry_guide/#1-set-compatibility-mode-for-a-subject","title":"1. Set Compatibility Mode for a Subject","text":"<p>Who can: Administrators only When: During initial schema setup or policy changes Level: Subject</p> <pre><code># Set compatibility mode for a subject\ndanube-admin-cli schema set-compatibility \\\n  --subject user-events-value \\\n  --mode FULL\n\n# Get current compatibility mode\ndanube-admin-cli schema get-compatibility \\\n  --subject user-events-value\n</code></pre> <p>Output:</p> <pre><code>\u2705 Set compatibility mode for subject 'user-events-value' to FULL\n</code></pre> <p>What happens:</p> <ul> <li>Updates compatibility mode for the subject</li> <li>Applies to all future schema registrations</li> <li>All topics using this subject inherit the compatibility rules</li> </ul> <p>Compatibility Modes:</p> Mode Description Use Case <code>BACKWARD</code> New schema reads old data Consumers upgrade before producers (default) <code>FORWARD</code> Old schema reads new data Producers upgrade before consumers <code>FULL</code> Both backward + forward Critical schemas needing both directions <code>NONE</code> No validation Development/testing only"},{"location":"concepts/schema_registry_guide/#2-configure-topic-schema-settings","title":"2. Configure Topic Schema Settings","text":"<p>Who can: Administrators only When: Initial topic setup or policy changes Level: Topic</p> <pre><code># Configure schema for a topic\ndanube-admin-cli topic configure-schema \\\n  --topic /default/user-events-prod \\\n  --subject user-events-value \\\n  --validation-policy enforce \\\n  --enable-payload-validation\n\n# Update only validation policy\ndanube-admin-cli topic set-validation-policy \\\n  --topic /default/user-events-dev \\\n  --policy warn\n\n# View topic schema configuration\ndanube-admin-cli topic get-schema-config \\\n  --topic /default/user-events-prod\n</code></pre> <p>Output:</p> <pre><code>Topic: /default/user-events-prod\nSchema Subject: user-events-value\nCompatibility Mode: BACKWARD (from subject)\nValidation Policy: ENFORCE (topic-level)\nPayload Validation: ENABLED (topic-level)\n</code></pre> <p>What happens:</p> <ol> <li>Associates topic with schema subject</li> <li>Sets validation policy (topic-level)</li> <li>Enables/disables payload validation (topic-level)</li> <li>Stores configuration in ETCD at <code>/topics/{topic}/schema_config</code></li> </ol> <p>Validation Policies:</p> Policy Behavior Use Case <code>none</code> No validation Development topics, unstructured data <code>warn</code> Validate and log errors, accept anyway Monitoring/debugging production <code>enforce</code> Reject invalid messages Production requiring strict quality"},{"location":"concepts/schema_registry_guide/#3-change-topics-schema-subject-dangerous","title":"3. Change Topic's Schema Subject (Dangerous!)","text":"<p>Who can: Administrators only When: Migration or schema refactoring Level: Topic</p> <pre><code># Change schema subject (dangerous operation)\ndanube-admin-cli topic update-schema-subject \\\n  --topic /default/user-events \\\n  --new-subject user-events-v2-value \\\n  --force\n\n# Requires --force flag to confirm\n</code></pre> <p>\u26a0\ufe0f Warning:</p> <ul> <li>Existing producers will fail if their schema subject no longer matches</li> <li>Should be done during maintenance window</li> <li>Coordinate with application teams</li> </ul>"},{"location":"concepts/schema_registry_guide/#4-different-policies-for-different-environments","title":"4. Different Policies for Different Environments","text":"<p>Common Pattern: Same schema, different validation strictness</p> <pre><code># Development: Lenient validation\ndanube-admin-cli topic configure-schema \\\n  --topic /default/user-events-dev \\\n  --subject user-events-value \\\n  --validation-policy warn \\\n  --no-payload-validation\n\n# Staging: Moderate validation\ndanube-admin-cli topic configure-schema \\\n  --topic /default/user-events-staging \\\n  --subject user-events-value \\\n  --validation-policy warn \\\n  --enable-payload-validation\n\n# Production: Strict validation\ndanube-admin-cli topic configure-schema \\\n  --topic /default/user-events-prod \\\n  --subject user-events-value \\\n  --validation-policy enforce \\\n  --enable-payload-validation\n</code></pre> <p>Result:</p> <pre><code>SUBJECT: \"user-events-value\" (shared)\n  \u251c\u2500 Compatibility Mode: BACKWARD (applies to all)\n  \u2514\u2500 Used by:\n      \u251c\u2500 /default/user-events-dev (WARN, no payload check)\n      \u251c\u2500 /default/user-events-staging (WARN, with payload check)\n      \u2514\u2500 /default/user-events-prod (ENFORCE, with payload check)\n</code></pre>"},{"location":"concepts/schema_registry_guide/#5-delete-schema-version-dangerous","title":"5. Delete Schema Version (Dangerous!)","text":"<p>Who can: Administrators only When: Deprecating old versions Level: Subject</p> <pre><code># Delete a specific schema version\ndanube-admin-cli schema delete-version \\\n  --subject user-events-value \\\n  --version 2 \\\n  --force\n</code></pre> <p>\u26a0\ufe0f Warning:</p> <ul> <li>May break existing consumers using that version</li> <li>Cannot be undone</li> <li>Requires <code>--force</code> flag</li> </ul>"},{"location":"concepts/schema_registry_guide/#complete-workflow-example","title":"Complete Workflow Example","text":""},{"location":"concepts/schema_registry_guide/#scenario-setting-up-schema-registry-for-production","title":"Scenario: Setting up Schema Registry for Production","text":"<p>Step 1: Admin sets up subject with compatibility mode</p> <pre><code># Admin: Configure subject-level policy\ndanube-admin-cli schema set-compatibility \\\n  --subject user-events-value \\\n  --mode BACKWARD\n</code></pre> <p>Step 2: Developer registers initial schema</p> <pre><code>// Developer: Register v1 schema from application\nlet schema_v1 = r#\"\n{\n  \"type\": \"record\",\n  \"name\": \"UserEvent\",\n  \"fields\": [\n    {\"name\": \"user_id\", \"type\": \"string\"},\n    {\"name\": \"action\", \"type\": \"string\"}\n  ]\n}\n\"#;\n\nlet schema_id_v1 = schema_client\n    .register_schema(\"user-events-value\")\n    .with_type(SchemaType::Avro)\n    .with_schema_data(schema_v1.as_bytes())\n    .execute()\n    .await?;\n\nprintln!(\"Registered v1 with ID: {}\", schema_id_v1);\n</code></pre> <p>Step 3: Admin configures topic validation policies</p> <pre><code># Admin: Configure dev topic (lenient)\ndanube-admin-cli topic configure-schema \\\n  --topic /default/user-events-dev \\\n  --subject user-events-value \\\n  --validation-policy warn\n\n# Admin: Configure prod topic (strict)\ndanube-admin-cli topic configure-schema \\\n  --topic /default/user-events-prod \\\n  --subject user-events-value \\\n  --validation-policy enforce \\\n  --enable-payload-validation\n</code></pre> <p>Step 4: Developer creates producer (first producer privilege)</p> <pre><code>// Developer: Create producer (assigns schema to topic if new)\nlet mut producer = client\n    .producer()\n    .with_topic(\"/default/user-events-prod\")\n    .with_name(\"prod_producer\")\n    .with_schema_subject(\"user-events-value\")  // Must match admin config\n    .build();\n\nproducer.create().await?;\n\n// Send messages\nlet event = UserEvent {\n    user_id: \"123\".to_string(),\n    action: \"login\".to_string(),\n};\n\nlet payload = serde_json::to_vec(&amp;event)?;\nproducer.send(payload, None).await?;\n</code></pre> <p>Step 5: Developer evolves schema (backward compatible)</p> <pre><code>// Developer: Check compatibility first\nlet schema_v2 = r#\"\n{\n  \"type\": \"record\",\n  \"name\": \"UserEvent\",\n  \"fields\": [\n    {\"name\": \"user_id\", \"type\": \"string\"},\n    {\"name\": \"action\", \"type\": \"string\"},\n    {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}\n  ]\n}\n\"#;\n\nlet compat = schema_client\n    .check_compatibility(\n        \"user-events-value\",\n        schema_v2.as_bytes().to_vec(),\n        SchemaType::Avro,\n        None,\n    )\n    .await?;\n\nif compat.is_compatible {\n    // Register new version\n    let schema_id_v2 = schema_client\n        .register_schema(\"user-events-value\")\n        .with_type(SchemaType::Avro)\n        .with_schema_data(schema_v2.as_bytes())\n        .execute()\n        .await?;\n\n    println!(\"Registered v2 with ID: {}\", schema_id_v2);\n}\n</code></pre> <p>Step 6: Developer consumes with validation</p> <pre><code>// Developer: Consumer validates messages\nlet mut consumer = client\n    .consumer()\n    .with_topic(\"/default/user-events-prod\")\n    .with_consumer_name(\"prod_consumer\")\n    .with_subscription(\"prod-subscription\")\n    .build();\n\nconsumer.subscribe().await?;\n\n// Trust broker validation (topic has ENFORCE policy)\nwhile let Some(message) = consumer.receive().await? {\n    // Broker already validated, safe to deserialize\n    let event: UserEvent = serde_json::from_slice(&amp;message.payload)?;\n    println!(\"Received: {:?}\", event);\n    consumer.ack(&amp;message).await?;\n}\n</code></pre>"},{"location":"concepts/schema_registry_guide/#best-practices","title":"Best Practices","text":""},{"location":"concepts/schema_registry_guide/#for-developers-client-sdk","title":"For Developers (Client SDK)","text":"<ol> <li>Always register schemas before creating producers</li> </ol> <pre><code>// Register schema first\nlet schema_id = schema_client.register_schema(\"my-subject\")...\n\n// Then create producer\nlet producer = client.producer()\n    .with_schema_subject(\"my-subject\")...\n</code></pre> <ol> <li>Check compatibility before evolving schemas</li> </ol> <pre><code>let compat = schema_client.check_compatibility(...).await?;\nif compat.is_compatible {\n    schema_client.register_schema(...).await?;\n}\n</code></pre> <ol> <li>Use version pinning for critical producers</li> </ol> <pre><code>// Pin to known-good version\n.with_schema_version(\"my-subject\", 3)\n</code></pre> <ol> <li>Cache schemas at consumer startup</li> </ol> <pre><code>// Fetch once at startup\nlet schema = schema_client.get_latest_schema(\"my-subject\").await?;\n\n// Reuse for all messages\n</code></pre>"},{"location":"concepts/schema_registry_guide/#for-administrators-admin-cli","title":"For Administrators (Admin CLI)","text":"<ol> <li>Set compatibility mode early</li> </ol> <pre><code># Set before first schema registration\ndanube-admin-cli schema set-compatibility \\\n  --subject my-subject \\\n  --mode BACKWARD\n</code></pre> <ol> <li>Use different validation policies per environment</li> </ol> <pre><code># Dev: lenient\ndanube-admin-cli topic configure-schema \\\n  --topic /dev/my-topic \\\n  --validation-policy warn\n\n# Prod: strict\ndanube-admin-cli topic configure-schema \\\n  --topic /prod/my-topic \\\n  --validation-policy enforce\n</code></pre> <ol> <li>Never delete schema versions in production</li> <li>Consumers may still reference old versions</li> <li> <p>Breaks message deserialization</p> </li> <li> <p>Document schema changes</p> </li> </ol> <pre><code># Use descriptive commit messages\ngit commit -m \"Add optional email field to UserEvent schema (v2)\"\n</code></pre>"},{"location":"concepts/schema_registry_guide/#summary","title":"Summary","text":""},{"location":"concepts/schema_registry_guide/#policy-levels","title":"Policy Levels","text":"Setting Level Set By Can Differ Per Topic? Compatibility Mode Subject Admin N/A (applies to subject) Validation Policy Topic Admin \u2705 Yes Payload Validation Topic Admin \u2705 Yes Schema Subject Topic First Producer or Admin \u274c No (one per topic) Schema Version Producer Each Producer \u2705 Yes"},{"location":"concepts/schema_registry_guide/#who-can-do-what","title":"Who Can Do What","text":"Operation Client Admin Reason Register schema \u2705 \u2705 Enable schema evolution Set compatibility \u274c \u2705 Governance control Assign schema to topic \u2705 (first) \u2705 Flexibility + control Set validation policy \u274c \u2705 Operational policy Choose schema version \u2705 N/A Producer autonomy Delete schemas \u274c \u2705 Prevent accidents <p>This separation ensures developers can iterate quickly while administrators maintain governance and operational safety.</p>"},{"location":"concepts/subscriptions/","title":"Subscription","text":"<p>A Danube subscription is a named configuration rule that determines how messages are delivered to consumers. It is a lease on a topic established by a group of consumers.</p> <p>Danube permits multiple producers and subscribers to the same topic. The Subscription Types can be combined to obtain message queueing or fan-out pub-sub messaging patterns.</p> <p></p>"},{"location":"concepts/subscriptions/#exclusive","title":"Exclusive","text":"<p>The Exclusive type is a subscription type that only allows a single consumer to attach to the subscription. If multiple consumers subscribe to a topic using the same subscription, an error occurs. This consumer has exclusive access to all messages published to the topic or partition.</p>"},{"location":"concepts/subscriptions/#exclusive-subscription-on-non-partitioned-topic","title":"Exclusive subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumer</code>: Only one consumer can be attached to the topic with an Exclusive subscription.</li> <li><code>Message Handling</code>: The single consumer handles all messages from the topic, receiving every message published to that topic.</li> </ul>"},{"location":"concepts/subscriptions/#exclusive-subscription-on-partitioned-topic-multiple-partitions","title":"Exclusive subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumer</code>: One consumer is allowed to connect to the subscription across all partitions of the partitioned topic.</li> <li><code>Message Handling</code> : This single consumer processes messages from all partitions of the partitioned topic. If a topic is partitioned into multiple partitions, the exclusive consumer handles messages from every partition.</li> </ul>"},{"location":"concepts/subscriptions/#shared","title":"Shared","text":"<p>In Danube, the Shared subscription type allows multiple consumers to attach to the same subscription. Messages are delivered in a round-robin distribution across consumers, and any given message is delivered to only one consumer.</p>"},{"location":"concepts/subscriptions/#shared-subscription-on-non-partitioned-topic","title":"Shared subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumers</code>: Multiple consumers can subscribe to the same topic.</li> <li><code>Message Handling</code>: Messages are distributed among all consumers in a round-robin fashion.</li> </ul>"},{"location":"concepts/subscriptions/#shared-subscription-on-partitioned-topic-multiple-partitions","title":"Shared subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumers</code>: Multiple consumers can subscribe to the partitioned topic.</li> <li><code>Message Handling</code>: Messages are distributed across all partitions, and then among consumers in a round-robin fashion. Each message from any partition is delivered to only one consumer.</li> </ul>"},{"location":"concepts/subscriptions/#failover","title":"Failover","text":"<p>The Failover subscription type allows multiple consumers to attach to the same subscription, with one active consumer at a time. If the active consumer disconnects or becomes unhealthy, another consumer automatically takes over. This preserves ordering and minimizes downtime.</p>"},{"location":"concepts/subscriptions/#failover-subscription-on-non-partitioned-topic","title":"Failover subscription on Non-Partitioned Topic","text":"<ul> <li><code>Consumers</code>: One active consumer processes all messages; additional consumers are in standby.</li> <li><code>Message Handling</code>: If the active consumer fails, a standby consumer takes over and continues from the last acknowledged position.</li> </ul>"},{"location":"concepts/subscriptions/#failover-subscription-on-partitioned-topic-multiple-partitions","title":"Failover subscription on Partitioned Topic (Multiple Partitions)","text":"<ul> <li><code>Consumers</code>: One active consumer per partition; other consumers remain on standby for each partition.</li> <li><code>Message Handling</code>: Failover occurs independently per partition, ensuring continuity and ordering within each partition.</li> </ul>"},{"location":"concepts/topics/","title":"Topic","text":"<p>A topic is a unit of storage that organizes messages into a stream. As in other messaging systems, topics are named channels for transmitting messages from producers to consumers. Topic names are URLs that have a well-defined structure:</p>"},{"location":"concepts/topics/#namespacetopic_name","title":"/{namespace}/{topic_name}","text":"<p>Example: /default/markets (where default is the namespace and markets the topic)</p>"},{"location":"concepts/topics/#partitioned-topics","title":"Partitioned Topics","text":"<p>Danube support both partitioned and non-partitioned topics. The non-partitioned topics are served by a single broker, while the partitioned topic has partitiones that are served by multiple brokers within the cluster, thus allowing for higher throughput.</p> <p>A partitioned topic is implemented as N internal topics, where N is the number of partitions. When publishing messages to a partitioned topic, each message is routed to one of several brokers. The distribution of partitions across brokers is handled automatically.</p> <p></p> <p>Messages for the topic are broadcast to two consumers. The routing mode determines each message should be published to which partition, while the subscription type determines which messages go to which consumers.</p>"},{"location":"concepts/topics/#benefits-of-the-partitioned-topics","title":"Benefits of the Partitioned topics","text":"<ul> <li><code>Scalability</code>: Partitioned topics enable horizontal scaling by distributing the load across multiple partitions. This is essential for high-throughput systems that need to handle large volumes of data efficiently.</li> <li><code>Parallel Processing</code>: It allows multiple consumers to process different partitions of the same topic concurrently, improving throughput and processing efficiency.</li> <li><code>Data Locality</code>: Partitioning can help in maintaining data locality and reducing processing latency, as consumers handle a specific subset of the data (key-shared distribution not yet supported).</li> </ul>"},{"location":"concepts/topics/#creation-of-partitioned-topics","title":"Creation of Partitioned Topics","text":"<p>Partitioned topics are created with a predefined number of partitions. When you create a partitioned topic, you specify the number of partitions it should have. This number remains fixed for the lifetime of the topic, although you can configure this number at topic creation time.</p>"},{"location":"concepts/topics/#producers","title":"Producers","text":"<p>The producers routing mechanism determine which messages go to which partition.</p>"},{"location":"concepts/topics/#routing-modes","title":"Routing modes","text":"<p>When publishing to partitioned topics, you must specify a routing mode. The routing mode determines each message should be published to which partition or which internal topic.</p> <ul> <li>RoundRobinPartition - The producer will publish messages across all partitions in round-robin fashion to achieve maximum throughput. If a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> <li>SinglePartition - If no key is provided, the producer will randomly pick one single partition and publish all the messages into that partition. While if a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> </ul>"},{"location":"concepts/topics/#consumers-subscriptions","title":"Consumers (subscriptions)","text":"<p>The subscription type determines which messages go to which consumers.</p> <p>Check the Subscription documentation for details on how messages are distributed to consumers based on the subscription type.</p>"},{"location":"contributing/dev_environment/","title":"Development Environment Setup for Danube Broker","text":"<p>This document guides you through setting up the development environment, running danube broker instances, and be able to effectively contribute to the code.</p>"},{"location":"contributing/dev_environment/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following installed:</p> <ul> <li> <p>Rust: Ensure you have Rust installed. You can download and install it from the Rust website.</p> </li> <li> <p>Docker: Install Docker if you haven\u2019t already. Follow the installation instructions on the Docker website.</p> </li> </ul>"},{"location":"contributing/dev_environment/#contributing-to-the-repository","title":"Contributing to the Repository","text":"<ol> <li> <p>Fork the Repository:</p> </li> <li> <p>Go to the Danube Broker GitHub repository.</p> </li> <li> <p>Click the \"Fork\" button on the top right corner of the page to create your own copy of the repository.</p> </li> <li> <p>Clone Your Fork:</p> </li> </ol> <p>Once you have forked the repository, clone your forked repository:</p> <pre><code>git clone https://github.com/&lt;your-username&gt;/danube.git\ncd danube\n</code></pre> <ol> <li>Add the Original Repository as a Remote (optional but recommended for keeping up-to-date):</li> </ol> <pre><code>git remote add upstream https://github.com/danube-messaging/danube.git\n</code></pre>"},{"location":"contributing/dev_environment/#building-the-project","title":"Building the Project","text":"<ol> <li>Build the Project:</li> </ol> <p>To build the Danube Broker:</p> <pre><code>cargo build \nor  \ncargo build --release\n</code></pre>"},{"location":"contributing/dev_environment/#running-etcd","title":"Running ETCD","text":"<ol> <li>Start ETCD:</li> </ol> <p>Use the Makefile to start an ETCD instance. This will run ETCD in a Docker container.</p> <pre><code>make etcd\n</code></pre> <ol> <li>Clean Up ETCD:</li> </ol> <p>To stop and remove the ETCD instance and its data:</p> <pre><code>make etcd-clean\n</code></pre>"},{"location":"contributing/dev_environment/#running-a-single-broker-instance","title":"Running a Single Broker Instance","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. If not, use the <code>make etcd</code> command to start it.</p> <ol> <li>Run the Broker:</li> </ol> <p>Use the following command to start a single broker instance:</p> <pre><code>RUST_LOG=danube_broker=info target/debug/danube-broker --config-file config/danube_broker.yml\n</code></pre>"},{"location":"contributing/dev_environment/#running-multiple-broker-instances","title":"Running Multiple Broker Instances","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. Use:</p> <pre><code>make etcd\n</code></pre> <ol> <li>Run Multiple Brokers:</li> </ol> <p>Use the following Makefile command to start multiple broker instances:</p> <pre><code>make brokers\n</code></pre> <p>This will start brokers on ports 6650, 6651, and 6652. Logs for each broker will be saved in <code>temp/</code> directory.</p> <ol> <li>Clean Up Broker Instances:</li> </ol> <p>To stop all running broker instances:</p> <pre><code>make brokers-clean\n</code></pre>"},{"location":"contributing/dev_environment/#reading-logs","title":"Reading Logs","text":"<p>Logs for each broker instance are stored in the <code>temp/</code> directory. You can view them using:</p> <pre><code>cat temp/broker_&lt;port&gt;.log\n</code></pre> <p>Replace <code>&lt;port&gt;</code> with the actual port number (6650, 6651, or 6652).</p>"},{"location":"contributing/dev_environment/#inspecting-etcd-metadata","title":"Inspecting ETCD Metadata","text":"<ol> <li>Set Up <code>etcdctl</code>:</li> </ol> <p>Export the following environment variables:</p> <pre><code>export ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=http://localhost:2379\n</code></pre> <ol> <li>Inspect Metadata:</li> </ol> <p>Use <code>etcdctl</code> commands to inspect metadata. For example, to list all keys:</p> <pre><code>etcdctl get \"\" --prefix\n</code></pre> <p>To get a specific key:</p> <pre><code>etcdctl get &lt;key&gt;\n</code></pre>"},{"location":"contributing/dev_environment/#makefile-targets-summary","title":"Makefile Targets Summary","text":"<ul> <li><code>make etcd</code>: Starts an ETCD instance in Docker.</li> <li><code>make etcd-clean</code>: Stops and removes the ETCD instance and its data.</li> <li><code>make brokers</code>: Builds and starts broker instances on predefined ports.</li> <li><code>make brokers-clean</code>: Stops and removes all running broker instances.</li> </ul>"},{"location":"contributing/dev_environment/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>ETCD Not Starting: Check Docker logs and ensure no other service is using port 2379.</li> <li>Broker Not Starting: Ensure ETCD is running and accessible at the specified address and port.</li> </ul>"},{"location":"contributing/etcd_metadata_structure/","title":"ETCD Metadata Structure","text":"<p>This document explains how Danube organizes metadata in ETCD. Understanding this structure is essential for developers working on cluster management, persistence, schema registry, or debugging production issues.</p>"},{"location":"contributing/etcd_metadata_structure/#overview","title":"Overview","text":"<p>Danube uses ETCD as its distributed metadata store to maintain cluster state, topic configurations, subscription tracking, schema registry, and cloud storage metadata. All brokers in the cluster share this centralized state.</p>"},{"location":"contributing/etcd_metadata_structure/#metadatastore-and-localcache-pattern","title":"MetadataStore and LocalCache Pattern","text":"<p>To balance consistency with performance, Danube implements a dual-layer caching strategy:</p> <p>MetadataStorage (ETCD)</p> <ul> <li>Source of truth for all cluster metadata</li> <li>Handles all write operations (PUT/DELETE) to ensure consistency</li> </ul> <p>LocalCache (per-broker)</p> <ul> <li>Fast read access for broker-local operations  </li> <li>Continuously synchronized from two sources:</li> <li>ETCD Watch - Real-time updates from ETCD changes</li> <li>Synchronizer Topic - Internal topic broadcasting metadata changes</li> </ul> <p>Access Pattern:</p> <pre><code>Write: Client \u2192 MetadataStore (ETCD) \u2192 Broadcast \u2192 LocalCache updates\nRead:  Client \u2192 LocalCache (fast, no ETCD query)\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#resource-hierarchy","title":"Resource Hierarchy","text":"<p>ETCD paths follow a hierarchical structure that mirrors Danube's logical architecture:</p> <pre><code>/\n\u251c\u2500\u2500 cluster/          # Cluster-wide state and broker coordination\n\u251c\u2500\u2500 namespaces/       # Namespace policies and topic lists\n\u251c\u2500\u2500 topics/           # Topic-level metadata (producers, subscriptions, policies)\n\u251c\u2500\u2500 schemas/          # Schema registry (Avro, Protobuf, JSON schemas)\n\u251c\u2500\u2500 danube-data/      # Persistent storage metadata (cloud objects, sealed state)\n\u2514\u2500\u2500 subscriptions/    # (Legacy) Subscription-level consumer tracking\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#1-cluster-resources","title":"1. Cluster Resources","text":"<p>Base Path: <code>/cluster</code> Purpose: Broker discovery, topic assignment, load balancing, and leader election</p>"},{"location":"contributing/etcd_metadata_structure/#11-cluster-name","title":"1.1 Cluster Name","text":"<p>Path: <code>/cluster/{CLUSTER_NAME}</code> Value: <code>null</code> Purpose: Marker for cluster existence</p> <p>Example:</p> <pre><code>/cluster/MY_CLUSTER\nnull\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#12-broker-registration","title":"1.2 Broker Registration","text":"<p>Path: <code>/cluster/register/{broker_id}</code> Value: JSON with broker endpoints Purpose: Broker metadata for client routing and inter-broker communication</p> <p>Example:</p> <pre><code>/cluster/register/625722408599041316\n{\n  \"admin_addr\": \"http://0.0.0.0:50051\",\n  \"advertised_addr\": \"0.0.0.0:6650\",\n  \"broker_addr\": \"http://0.0.0.0:6650\",\n  \"prom_exporter\": \"0.0.0.0:9040\"\n}\n</code></pre> <p>Fields:</p> <ul> <li><code>broker_addr</code> - gRPC endpoint for client connections</li> <li><code>admin_addr</code> - HTTP endpoint for administrative operations</li> <li><code>advertised_addr</code> - Public address advertised to clients</li> <li><code>prom_exporter</code> - Prometheus metrics endpoint</li> </ul>"},{"location":"contributing/etcd_metadata_structure/#13-broker-state","title":"1.3 Broker State","text":"<p>Path: <code>/cluster/brokers/{broker_id}/state</code> Value: JSON with operational mode Purpose: Track broker lifecycle states (active, draining, maintenance)</p> <p>Example:</p> <pre><code>/cluster/brokers/625722408599041316/state\n{\n  \"mode\": \"active\",\n  \"reason\": \"boot\"\n}\n</code></pre> <p>States:</p> <ul> <li><code>active</code> - Normal operation</li> <li><code>draining</code> - Graceful shutdown in progress</li> <li>(custom states for maintenance/upgrades)</li> </ul>"},{"location":"contributing/etcd_metadata_structure/#14-topic-assignments","title":"1.4 Topic Assignments","text":"<p>Path: <code>/cluster/brokers/{broker_id}/{namespace}/{topic}</code> Value: <code>null</code> Purpose: Maps topics to brokers; watched by brokers to load/unload topics</p> <p>Example:</p> <pre><code>/cluster/brokers/13308604176970018988/default/reliable_topic\nnull\n</code></pre> <p>Workflow:</p> <ol> <li>Load Manager assigns topic \u2192 Creates this key</li> <li>Broker watches <code>/cluster/brokers/{own_id}/</code> \u2192 Detects new topic</li> <li>Broker loads topic locally (WAL, subscriptions, etc.)</li> </ol>"},{"location":"contributing/etcd_metadata_structure/#15-unassigned-topics","title":"1.5 Unassigned Topics","text":"<p>Path: <code>/cluster/unassigned/{namespace}/{topic}</code> Value: <code>null</code> or JSON with unload reason Purpose: Queue for newly created or unloaded topics awaiting assignment</p> <p>Example (new topic):</p> <pre><code>/cluster/unassigned/default/reliable_topic\nnull\n</code></pre> <p>Example (topic unload):</p> <pre><code>/cluster/unassigned/default/reliable_topic\n{\n  \"reason\": \"unload\",\n  \"from_broker\": 12549595323552083708\n}\n</code></pre> <p>Workflow:</p> <ol> <li>Producer creates topic \u2192 Broker writes to <code>/cluster/unassigned/</code></li> <li>Load Manager watches this path</li> <li>Load Manager assigns to least-loaded broker</li> <li>Unassigned marker deleted after successful assignment</li> </ol>"},{"location":"contributing/etcd_metadata_structure/#16-load-reports","title":"1.6 Load Reports","text":"<p>Path: <code>/cluster/load/{broker_id}</code> Value: JSON with resource usage and topic list Purpose: Load Manager uses this to make assignment decisions</p> <p>Example:</p> <pre><code>/cluster/load/13308604176970018988\n{\n  \"resources_usage\": [\n    {\"resource\": \"CPU\", \"usage\": 30},\n    {\"resource\": \"Memory\", \"usage\": 30}\n  ],\n  \"topic_list\": [\"/default/reliable_topic\"],\n  \"topics_len\": 1\n}\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#17-leader-election","title":"1.7 Leader Election","text":"<p>Path: <code>/cluster/leader</code> Value: broker_id (u64) Purpose: Identifies the current cluster leader</p> <p>Example:</p> <pre><code>/cluster/leader\n625722408599041316\n</code></pre> <p>Usage:</p> <ul> <li>Load Manager runs only on the leader broker</li> <li>Schema registry operations coordinate through leader</li> </ul>"},{"location":"contributing/etcd_metadata_structure/#2-namespace-resources","title":"2. Namespace Resources","text":"<p>Base Path: <code>/namespaces</code> Purpose: Namespace-level policies and topic organization</p>"},{"location":"contributing/etcd_metadata_structure/#21-namespace-policy","title":"2.1 Namespace Policy","text":"<p>Path: <code>/namespaces/{namespace}/policy</code> Value: JSON with namespace-wide limits Purpose: Default policies for all topics in the namespace</p> <p>Example:</p> <pre><code>/namespaces/default/policy\n{\n  \"max_consumers_per_subscription\": 0,\n  \"max_consumers_per_topic\": 0,\n  \"max_message_size\": 10485760,\n  \"max_producers_per_topic\": 0,\n  \"max_publish_rate\": 0,\n  \"max_subscription_dispatch_rate\": 0,\n  \"max_subscriptions_per_topic\": 0\n}\n</code></pre> <p>Note: <code>0</code> means unlimited</p>"},{"location":"contributing/etcd_metadata_structure/#22-topic-registry","title":"2.2 Topic Registry","text":"<p>Path: <code>/namespaces/{namespace}/topics/{namespace}/{topic}</code> Value: <code>null</code> Purpose: List all topics in a namespace</p> <p>Example:</p> <pre><code>/namespaces/default/topics/default/reliable_topic\nnull\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#3-topic-resources","title":"3. Topic Resources","text":"<p>Base Path: <code>/topics</code> Purpose: Topic-specific configuration, producers, and subscriptions</p>"},{"location":"contributing/etcd_metadata_structure/#31-topic-root","title":"3.1 Topic Root","text":"<p>Path: <code>/topics/{namespace}/{topic}</code> Value: Number of partitions (usually <code>0</code> for non-partitioned) Purpose: Topic existence marker</p> <p>Example:</p> <pre><code>/topics/default/reliable_topic\n0\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#32-delivery-mode","title":"3.2 Delivery Mode","text":"<p>Path: <code>/topics/{namespace}/{topic}/delivery</code> Value: Dispatch strategy type Purpose: Determines message delivery guarantees</p> <p>Example:</p> <pre><code>/topics/default/reliable_topic/delivery\n\"Reliable\"\n</code></pre> <p>Values:</p> <ul> <li><code>\"Reliable\"</code> - At-least-once delivery with persistence</li> <li><code>\"NonReliable\"</code> - Best-effort delivery</li> </ul>"},{"location":"contributing/etcd_metadata_structure/#33-producer-registration","title":"3.3 Producer Registration","text":"<p>Path: <code>/topics/{namespace}/{topic}/producers/{producer_id}</code> Value: JSON with producer metadata Purpose: Track active producers on the topic</p> <p>Example:</p> <pre><code>/topics/default/reliable_topic/producers/13940288943180594845\n{\n  \"access_mode\": 0,\n  \"producer_id\": 13940288943180594845,\n  \"producer_name\": \"prod_json_reliable\",\n  \"status\": true,\n  \"topic_name\": \"/default/reliable_topic\"\n}\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#34-subscription-metadata","title":"3.4 Subscription Metadata","text":"<p>Path: <code>/topics/{namespace}/{topic}/subscriptions/{subscription_name}</code> Value: JSON with subscription configuration Purpose: Subscription settings and consumer tracking</p> <p>Example:</p> <pre><code>/topics/default/reliable_topic/subscriptions/subs_reliable\n{\n  \"consumer_id\": null,\n  \"consumer_name\": \"cons_reliable\",\n  \"subscription_name\": \"subs_reliable\",\n  \"subscription_type\": 0\n}\n</code></pre> <p>Subscription Types:</p> <ul> <li><code>0</code> - Exclusive (single consumer)</li> <li><code>1</code> - Shared (round-robin across consumers)</li> <li><code>2</code> - Failover (primary/backup consumers)</li> </ul>"},{"location":"contributing/etcd_metadata_structure/#35-subscription-cursor","title":"3.5 Subscription Cursor","text":"<p>Path: <code>/topics/{namespace}/{topic}/subscriptions/{subscription_name}/cursor</code> Value: Last acknowledged offset (u64) Purpose: Track consumer progress through the topic</p> <p>Example:</p> <pre><code>/topics/default/reliable_topic/subscriptions/subs_reliable/cursor\n13\n</code></pre> <p>Note: This is written periodically (every 1000 acks or 5 seconds) for performance</p>"},{"location":"contributing/etcd_metadata_structure/#4-schema-registry","title":"4. Schema Registry","text":"<p>Base Path: <code>/schemas</code> Purpose: Schema versioning, validation, and compatibility management</p>"},{"location":"contributing/etcd_metadata_structure/#41-global-schema-id-counter","title":"4.1 Global Schema ID Counter","text":"<p>Path: <code>/schemas/_global/next_schema_id</code> Value: Next available schema ID (u64) Purpose: Monotonically increasing schema ID generation</p> <p>Example:</p> <pre><code>/schemas/_global/next_schema_id\n2\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#42-schema-metadata","title":"4.2 Schema Metadata","text":"<p>Path: <code>/schemas/{subject}/metadata</code> Value: JSON with schema evolution history Purpose: Complete schema subject information</p> <p>Example:</p> <pre><code>/schemas/product-catalog/metadata\n{\n  \"compatibility_mode\": \"Backward\",\n  \"created_at\": 1768728611,\n  \"created_by\": \"danube-client\",\n  \"id\": 1,\n  \"latest_version\": 2,\n  \"subject\": \"product-catalog\",\n  \"topics_using\": [],\n  \"updated_at\": 1768728613,\n  \"versions\": [\n    {\n      \"created_at\": 1768728611,\n      \"created_by\": \"danube-client\",\n      \"version\": 1,\n      \"fingerprint\": \"sha256:270e3390...\",\n      \"schema_def\": {\"Avro\": {...}},\n      \"is_deprecated\": false,\n      \"tags\": []\n    },\n    {\n      \"created_at\": 1768728613,\n      \"version\": 2,\n      \"fingerprint\": \"sha256:d6c3da89...\",\n      \"schema_def\": {\"Avro\": {...}}\n    }\n  ]\n}\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#43-schema-version","title":"4.3 Schema Version","text":"<p>Path: <code>/schemas/{subject}/versions/{version}</code> Value: JSON with schema definition Purpose: Immutable schema version storage</p> <p>Example:</p> <pre><code>/schemas/product-catalog/versions/1\n{\n  \"created_at\": 1768728611,\n  \"created_by\": \"danube-client\",\n  \"version\": 1,\n  \"fingerprint\": \"sha256:270e3390980bc143...\",\n  \"is_deprecated\": false,\n  \"schema_def\": {\n    \"Avro\": {\n      \"fingerprint\": \"sha256:270e3390...\",\n      \"raw_schema\": \"{\\n  \\\"type\\\": \\\"record\\\",\\n  \\\"name\\\": \\\"Product\\\",\\n  ...\"\n    }\n  },\n  \"tags\": []\n}\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#44-schema-id-index","title":"4.4 Schema ID Index","text":"<p>Path: <code>/schemas/_index/by_id/{schema_id}</code> Value: JSON mapping ID to subject Purpose: Reverse lookup from schema ID to subject name</p> <p>Example:</p> <pre><code>/schemas/_index/by_id/1\n{\n  \"schema_id\": 1,\n  \"subject\": \"product-catalog\"\n}\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#5-cloud-storage-metadata","title":"5. Cloud Storage Metadata","text":"<p>Base Path: <code>/danube-data/storage</code> Purpose: Track uploaded message segments in cloud storage (S3, GCS, filesystem)</p>"},{"location":"contributing/etcd_metadata_structure/#51-object-descriptors","title":"5.1 Object Descriptors","text":"<p>Path: <code>/danube-data/storage/topics/{namespace}/{topic}/objects/{padded_start_offset}</code> Value: JSON with cloud object metadata Purpose: Map message offsets to cloud storage objects</p> <p>Example:</p> <pre><code>/danube-data/storage/topics/default/reliable_topic/objects/00000000000000000000\n{\n  \"completed\": true,\n  \"created_at\": 1768728599,\n  \"end_offset\": 29,\n  \"etag\": null,\n  \"object_id\": \"data-0-29.dnb1\",\n  \"offset_index\": [[0, 0]],\n  \"size\": 7164,\n  \"start_offset\": 0\n}\n</code></pre> <p>Fields:</p> <ul> <li><code>object_id</code> - Filename in cloud storage</li> <li><code>start_offset</code> / <code>end_offset</code> - Offset range contained in this object</li> <li><code>offset_index</code> - Sparse index for fast offset lookups</li> <li><code>size</code> - Object size in bytes</li> <li><code>completed</code> - Whether upload finished successfully</li> </ul> <p>Offset Padding: Start offsets are zero-padded to 20 digits for lexicographic sorting:</p> <ul> <li><code>0</code> \u2192 <code>00000000000000000000</code></li> <li><code>30</code> \u2192 <code>00000000000000000030</code></li> </ul>"},{"location":"contributing/etcd_metadata_structure/#52-current-object-cursor","title":"5.2 Current Object Cursor","text":"<p>Path: <code>/danube-data/storage/topics/{namespace}/{topic}/objects/cur</code> Value: JSON pointer to latest object Purpose: Quick lookup for most recent uploaded segment</p> <p>Example:</p> <pre><code>/danube-data/storage/topics/default/reliable_topic/objects/cur\n{\n  \"start\": \"00000000000000000030\"\n}\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#53-sealed-state-topic-move","title":"5.3 Sealed State (Topic Move)","text":"<p>Path: <code>/danube-data/storage/topics/{namespace}/{topic}/state</code> Value: JSON with last committed offset Purpose: Preserve offset continuity when topic moves between brokers</p> <p>Example:</p> <pre><code>/danube-data/storage/topics/default/reliable_topic/state\n{\n  \"sealed\": true,\n  \"last_committed_offset\": 21,\n  \"broker_id\": 10285063371164059634,\n  \"timestamp\": 1768625254\n}\n</code></pre> <p>When Created:</p> <ul> <li>Written during topic unload (<code>danube-admin-cli topics unload</code>)</li> <li>Read by new broker to initialize WAL at <code>last_committed_offset + 1</code></li> <li>Deleted after successful topic load on new broker</li> </ul> <p>Critical for: Preventing offset collisions during topic migrations</p>"},{"location":"contributing/etcd_metadata_structure/#6-subscriptions-resources-legacy","title":"6. Subscriptions Resources (Legacy)","text":"<p>Base Path: <code>/subscriptions</code> Purpose: Consumer metadata (largely deprecated in favor of <code>/topics/.../subscriptions/</code>)</p> <p>Path: <code>/subscriptions/{subscription_name}/{consumer_id}</code> Value: Consumer metadata JSON</p> <p>Note: This structure is mostly legacy. Current implementation stores subscription state under <code>/topics/{namespace}/{topic}/subscriptions/</code>.</p>"},{"location":"contributing/etcd_metadata_structure/#common-patterns","title":"Common Patterns","text":""},{"location":"contributing/etcd_metadata_structure/#watching-for-changes","title":"Watching for Changes","text":"<p>Brokers watch specific ETCD prefixes to react to cluster changes:</p> <pre><code>// Broker watches its own assignment path\nbroker.watch(\"/cluster/brokers/{own_broker_id}/\")\n  \u2192 Loads/unloads topics dynamically\n\n// Load Manager watches unassigned topics\nload_manager.watch(\"/cluster/unassigned/\")\n  \u2192 Assigns topics to brokers\n\n// Schema registry watches schema changes\nschema_service.watch(\"/schemas/\")\n  \u2192 Invalidates local schema cache\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#key-naming-conventions","title":"Key Naming Conventions","text":"<ul> <li>Broker IDs: 64-bit unsigned integers (e.g., <code>625722408599041316</code>)</li> <li>Topic Paths: <code>/{namespace}/{topic}</code> (e.g., <code>/default/reliable_topic</code>)</li> <li>Padded Offsets: Zero-padded to 20 digits for sorting</li> <li>Null Values: Used as existence markers when no data needed</li> </ul>"},{"location":"contributing/etcd_metadata_structure/#performance-considerations","title":"Performance Considerations","text":"<p>LocalCache First: All read-heavy operations (e.g., routing clients, looking up producers) use LocalCache to avoid ETCD query overhead.</p> <p>Batch Updates: Subscription cursors are updated in batches (every 1000 ACKs or 5s) to reduce ETCD write load.</p> <p>Watch-Based Sync: ETCD watches provide millisecond-latency updates to LocalCache, keeping reads fast and consistent.</p>"},{"location":"contributing/etcd_metadata_structure/#debugging-tips","title":"Debugging Tips","text":""},{"location":"contributing/etcd_metadata_structure/#list-all-keys","title":"List All Keys","text":"<pre><code>etcdctl get / --prefix --keys-only\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#watch-cluster-changes-live","title":"Watch Cluster Changes Live","text":"<pre><code>etcdctl watch / --prefix\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#query-specific-topic","title":"Query Specific Topic","text":"<pre><code># Topic existence\netcdctl get /topics/default/my_topic\n\n# Find which broker owns it\netcdctl get /cluster/brokers/ --prefix | grep my_topic\n\n# Check cloud storage objects\netcdctl get /danube-data/storage/topics/default/my_topic/objects/ --prefix\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#inspect-schema-evolution","title":"Inspect Schema Evolution","text":"<pre><code># Get all versions\netcdctl get /schemas/product-catalog/versions/ --prefix\n\n# Check compatibility mode\netcdctl get /schemas/product-catalog/metadata\n</code></pre>"},{"location":"contributing/etcd_metadata_structure/#summary","title":"Summary","text":"<p>Danube's ETCD structure provides:</p> <p>\u2705 Cluster coordination - Broker discovery, leader election, topic assignment \u2705 Metadata persistence - Topics, producers, subscriptions, schemas \u2705 Cloud storage tracking - Object descriptors for tiered storage \u2705 State continuity - Sealed state for seamless topic migration  </p> <p>Understanding this structure is essential for developing features that interact with cluster state, debugging production issues, or contributing to Danube's distributed systems architecture.</p>"},{"location":"danube_clis/danube_admin/brokers/","title":"Brokers Management","text":"<p>Manage and view broker information in your Danube cluster.</p>"},{"location":"danube_clis/danube_admin/brokers/#overview","title":"Overview","text":"<p>The <code>brokers</code> command provides visibility and control over the brokers in your Danube cluster. Use it to:</p> <ul> <li>List all brokers with their status</li> <li>Identify the leader broker</li> <li>View broker namespaces</li> <li>Unload topics from brokers</li> <li>Activate brokers</li> </ul>"},{"location":"danube_clis/danube_admin/brokers/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/brokers/#list-all-brokers","title":"List All Brokers","text":"<p>Display all brokers in the cluster with their details.</p> <pre><code>danube-admin-cli brokers list\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text (default) - easy to read\ndanube-admin-cli brokers list\n\n# JSON format - for scripting/automation\ndanube-admin-cli brokers list --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Broker ID    \u2502 Address             \u2502 Role     \u2502 Admin Address       \u2502 Metrics Address     \u2502 Status \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 broker-001   \u2502 127.0.0.1:6650      \u2502 leader   \u2502 127.0.0.1:50051     \u2502 127.0.0.1:9090      \u2502 active \u2502\n\u2502 broker-002   \u2502 127.0.0.1:6651      \u2502 follower \u2502 127.0.0.1:50052     \u2502 127.0.0.1:9091      \u2502 active \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Example Output (JSON):</p> <pre><code>[\n  {\n    \"broker_id\": \"broker-001\",\n    \"broker_addr\": \"127.0.0.1:6650\",\n    \"broker_role\": \"leader\",\n    \"admin_addr\": \"127.0.0.1:50051\",\n    \"metrics_addr\": \"127.0.0.1:9090\",\n    \"broker_status\": \"active\"\n  },\n  {\n    \"broker_id\": \"broker-002\",\n    \"broker_addr\": \"127.0.0.1:6651\",\n    \"broker_role\": \"follower\",\n    \"admin_addr\": \"127.0.0.1:50052\",\n    \"metrics_addr\": \"127.0.0.1:9091\",\n    \"broker_status\": \"active\"\n  }\n]\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#get-leader-broker","title":"Get Leader Broker","text":"<p>Identify which broker is currently the cluster leader.</p> <pre><code>danube-admin-cli brokers leader\n</code></pre> <p>Example Output:</p> <pre><code>Leader: broker-001\n</code></pre> <p>Why This Matters:</p> <ul> <li>The leader broker coordinates cluster operations</li> <li>Useful for debugging cluster issues</li> <li>Important for understanding cluster topology</li> </ul>"},{"location":"danube_clis/danube_admin/brokers/#list-broker-namespaces","title":"List Broker Namespaces","text":"<p>View all namespaces managed by the cluster.</p> <pre><code>danube-admin-cli brokers namespaces\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text\ndanube-admin-cli brokers namespaces\n\n# JSON format\ndanube-admin-cli brokers namespaces --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Namespaces: [\"default\", \"analytics\", \"logs\"]\n</code></pre> <p>Example Output (JSON):</p> <pre><code>[\"default\", \"analytics\", \"logs\"]\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#unload-broker-topics","title":"Unload Broker Topics","text":"<p>Gracefully unload topics from a broker (useful for maintenance or rebalancing).</p> <pre><code>danube-admin-cli brokers unload &lt;BROKER_ID&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code># Unload all topics from broker-001\ndanube-admin-cli brokers unload broker-001\n\n# Dry-run to see what would be unloaded\ndanube-admin-cli brokers unload broker-001 --dry-run\n</code></pre> <p>Advanced Options:</p> <pre><code># Unload with custom parallelism\ndanube-admin-cli brokers unload broker-001 --max-parallel 5\n\n# Unload only specific namespaces\ndanube-admin-cli brokers unload broker-001 \\\n  --namespace-include default \\\n  --namespace-include analytics\n\n# Exclude certain namespaces\ndanube-admin-cli brokers unload broker-001 \\\n  --namespace-exclude system\n\n# Set custom timeout per topic (seconds)\ndanube-admin-cli brokers unload broker-001 --timeout 30\n</code></pre> <p>Options:</p> Option Description Default <code>--dry-run</code> Preview topics to be unloaded without making changes <code>false</code> <code>--max-parallel</code> Number of topics to unload concurrently <code>1</code> <code>--namespace-include</code> Only unload topics from these namespaces (repeatable) All <code>--namespace-exclude</code> Skip topics from these namespaces (repeatable) None <code>--timeout</code> Timeout in seconds for each topic unload <code>30</code> <p>Example Output:</p> <pre><code>Unload Started: true\nTotal Topics: 45\nSucceeded: 45\nFailed: 0\nPending: 0\n</code></pre> <p>Use Cases:</p> <ul> <li>Broker Maintenance: Drain topics before shutting down a broker</li> <li>Load Rebalancing: Move topics to other brokers</li> <li>Rolling Upgrades: Safely upgrade brokers one at a time</li> </ul>"},{"location":"danube_clis/danube_admin/brokers/#activate-broker","title":"Activate Broker","text":"<p>Mark a broker as active, allowing it to receive traffic.</p> <pre><code>danube-admin-cli brokers activate &lt;BROKER_ID&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli brokers activate broker-002\n</code></pre> <p>With Audit Reason:</p> <pre><code>danube-admin-cli brokers activate broker-002 \\\n  --reason \"Maintenance completed\"\n</code></pre> <p>Example Output:</p> <pre><code>Activated: true\n</code></pre> <p>Use Cases:</p> <ul> <li>After Maintenance: Re-enable a broker after maintenance</li> <li>After Unload: Activate broker to start receiving topics again</li> <li>Cluster Expansion: Activate newly added brokers</li> </ul>"},{"location":"danube_clis/danube_admin/brokers/#common-workflows","title":"Common Workflows","text":""},{"location":"danube_clis/danube_admin/brokers/#1-health-check","title":"1. Health Check","text":"<pre><code># Check cluster health\ndanube-admin-cli brokers list\ndanube-admin-cli brokers leader\n\n# Verify all brokers are active\ndanube-admin-cli brokers list | grep -c active\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#2-broker-maintenance","title":"2. Broker Maintenance","text":"<pre><code># Step 1: Dry-run to preview unload\ndanube-admin-cli brokers unload broker-001 --dry-run\n\n# Step 2: Unload topics\ndanube-admin-cli brokers unload broker-001\n\n# Step 3: Perform maintenance (external)\n# ...\n\n# Step 4: Reactivate broker\ndanube-admin-cli brokers activate broker-001 --reason \"Maintenance completed\"\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#3-cluster-expansion","title":"3. Cluster Expansion","text":"<pre><code># List current brokers\ndanube-admin-cli brokers list\n\n# Add new broker (external process)\n# ...\n\n# Activate new broker\ndanube-admin-cli brokers activate broker-003 --reason \"New broker added\"\n\n# Verify\ndanube-admin-cli brokers list\n</code></pre>"},{"location":"danube_clis/danube_admin/brokers/#quick-reference","title":"Quick Reference","text":"<pre><code># List all brokers\ndanube-admin-cli brokers list\n\n# Get leader\ndanube-admin-cli brokers leader\n\n# List namespaces\ndanube-admin-cli brokers namespaces\n\n# Unload topics (dry-run)\ndanube-admin-cli brokers unload &lt;broker-id&gt; --dry-run\n\n# Unload topics (execute)\ndanube-admin-cli brokers unload &lt;broker-id&gt; --max-parallel 5\n\n# Activate broker\ndanube-admin-cli brokers activate &lt;broker-id&gt; --reason \"Ready\"\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/","title":"Namespaces Management","text":"<p>Organize your topics with namespaces in Danube.</p>"},{"location":"danube_clis/danube_admin/namespaces/#overview","title":"Overview","text":"<p>Namespaces provide logical isolation for topics in your Danube cluster. Use namespaces to:</p> <ul> <li>Organize topics by application, team, or environment</li> <li>Apply policies at the namespace level</li> <li>Control access and resource allocation</li> <li>Separate production, staging, and development workloads</li> </ul>"},{"location":"danube_clis/danube_admin/namespaces/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/namespaces/#list-topics-in-a-namespace","title":"List Topics in a Namespace","text":"<p>View all topics within a specific namespace.</p> <pre><code>danube-admin-cli namespaces topics &lt;NAMESPACE&gt;\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli namespaces topics default\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text (default)\ndanube-admin-cli namespaces topics default\n\n# JSON format - for automation\ndanube-admin-cli namespaces topics default --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Topics in namespace 'default':\n  /default/user-events\n  /default/payment-logs\n  /default/analytics\n</code></pre> <p>Example Output (JSON):</p> <pre><code>[\n  \"/default/user-events\",\n  \"/default/payment-logs\",\n  \"/default/analytics\"\n]\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#view-namespace-policies","title":"View Namespace Policies","text":"<p>Get the policies configured for a namespace.</p> <pre><code>danube-admin-cli namespaces policies &lt;NAMESPACE&gt;\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli namespaces policies default\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text (default) - pretty printed\ndanube-admin-cli namespaces policies default\n\n# JSON format\ndanube-admin-cli namespaces policies default --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Policies for namespace 'default':\n{\n  \"max_topics_per_namespace\": 1000,\n  \"max_producers_per_topic\": 100,\n  \"max_consumers_per_topic\": 100,\n  \"message_ttl_seconds\": 604800,\n  \"retention_policy\": \"time_based\"\n}\n</code></pre> <p>Example Output (JSON):</p> <pre><code>{\n  \"max_topics_per_namespace\": 1000,\n  \"max_producers_per_topic\": 100,\n  \"max_consumers_per_topic\": 100,\n  \"message_ttl_seconds\": 604800,\n  \"retention_policy\": \"time_based\"\n}\n</code></pre> <p>Common Policies:</p> Policy Description Typical Values <code>max_topics_per_namespace</code> Maximum number of topics <code>100</code> - <code>10000</code> <code>max_producers_per_topic</code> Maximum producers per topic <code>10</code> - <code>1000</code> <code>max_consumers_per_topic</code> Maximum consumers per topic <code>10</code> - <code>1000</code> <code>message_ttl_seconds</code> Message time-to-live <code>3600</code> (1h) - <code>604800</code> (7d) <code>retention_policy</code> How messages are retained <code>time_based</code>, <code>size_based</code>"},{"location":"danube_clis/danube_admin/namespaces/#create-a-namespace","title":"Create a Namespace","text":"<p>Create a new namespace in the cluster.</p> <pre><code>danube-admin-cli namespaces create &lt;NAMESPACE&gt;\n</code></pre> <p>Basic Usage:</p> <pre><code># Create namespace\ndanube-admin-cli namespaces create production\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Namespace created: production\n</code></pre> <p>Naming Guidelines:</p> <ul> <li>Use lowercase letters and hyphens</li> <li>Keep names descriptive: <code>production</code>, <code>staging</code>, <code>dev</code></li> <li>Avoid special characters</li> <li>Use consistent naming: <code>team-app-env</code> pattern</li> </ul> <p>Examples:</p> <pre><code># By environment\ndanube-admin-cli namespaces create production\ndanube-admin-cli namespaces create staging\ndanube-admin-cli namespaces create development\n\n# By team\ndanube-admin-cli namespaces create analytics-team\ndanube-admin-cli namespaces create platform-team\n\n# By application\ndanube-admin-cli namespaces create payment-service\ndanube-admin-cli namespaces create user-service\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#delete-a-namespace","title":"Delete a Namespace","text":"<p>Remove a namespace from the cluster.</p> <pre><code>danube-admin-cli namespaces delete &lt;NAMESPACE&gt;\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli namespaces delete old-namespace\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Namespace deleted: old-namespace\n</code></pre> <p>\u26a0\ufe0f Important Warnings:</p> <ol> <li>All Topics Deleted: Deleting a namespace removes ALL topics within it</li> <li>No Confirmation: This operation is immediate and irreversible</li> <li>Active Connections: Connected producers/consumers will be disconnected</li> <li>Data Loss: All messages in the namespace are permanently deleted</li> </ol> <p>Safety Checklist:</p> <pre><code># 1. List topics before deletion\ndanube-admin-cli namespaces topics my-namespace\n\n# 2. Verify no critical topics\ndanube-admin-cli namespaces topics my-namespace --output json | grep -i critical\n\n# 3. Check policies to understand impact\ndanube-admin-cli namespaces policies my-namespace\n\n# 4. Only then delete\ndanube-admin-cli namespaces delete my-namespace\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#common-workflows","title":"Common Workflows","text":""},{"location":"danube_clis/danube_admin/namespaces/#1-namespace-setup-for-new-application","title":"1. Namespace Setup for New Application","text":"<pre><code># Create namespace\ndanube-admin-cli namespaces create payment-service\n\n# Verify creation\ndanube-admin-cli brokers namespaces | grep payment-service\n\n# Check default policies\ndanube-admin-cli namespaces policies payment-service\n\n# Create topics in namespace\ndanube-admin-cli topics create /payment-service/transactions\ndanube-admin-cli topics create /payment-service/refunds\ndanube-admin-cli topics create /payment-service/notifications\n</code></pre>"},{"location":"danube_clis/danube_admin/namespaces/#2-multi-environment-setup","title":"2. Multi-Environment Setup","text":"<pre><code># Create environments\ndanube-admin-cli namespaces create production\ndanube-admin-cli namespaces create staging\ndanube-admin-cli namespaces create development\n\n# List all namespaces\ndanube-admin-cli brokers namespaces\n\n# Create same topics in each environment\nfor env in production staging development; do\n  danube-admin-cli topics create /$env/user-events\n  danube-admin-cli topics create /$env/order-events\ndone\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/","title":"Schema Registry Management","text":"<p>Manage schemas for data validation and evolution in Danube.</p>"},{"location":"danube_clis/danube_admin/schema_registry/#overview","title":"Overview","text":"<p>The Schema Registry provides centralized schema management for your Danube topics. It enables:</p> <ul> <li>Type Safety: Validate messages against defined schemas</li> <li>Schema Evolution: Track and manage schema versions over time</li> <li>Compatibility Checking: Ensure new schemas don't break existing consumers</li> <li>Documentation: Schemas serve as living documentation for your data</li> </ul>"},{"location":"danube_clis/danube_admin/schema_registry/#why-use-schema-registry","title":"Why Use Schema Registry?","text":"<p>Without Schema Registry:</p> <pre><code># No validation - anything goes\nproducer.send('{\"nam\": \"John\"}')  # Typo: \"nam\" instead of \"name\"\n# Message accepted \u274c - consumers break\n</code></pre> <p>With Schema Registry:</p> <pre><code># Schema enforces structure\nproducer.send('{\"nam\": \"John\"}')  # Typo detected\n# Error: Field 'name' is required \u2705\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/schema_registry/#register-a-schema","title":"Register a Schema","text":"<p>Register a new schema or create a new version of an existing schema.</p> <pre><code>danube-admin-cli schemas register &lt;SUBJECT&gt; [OPTIONS]\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#basic-schema-registration","title":"Basic Schema Registration","text":"<p>From File (Recommended):</p> <pre><code>danube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file schemas/user-events.json\n</code></pre> <p>Inline Schema:</p> <pre><code>danube-admin-cli schemas register simple-events \\\n  --schema-type json_schema \\\n  --schema '{\"type\": \"object\", \"properties\": {\"id\": {\"type\": \"string\"}}}'\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Registered new schema version\nSubject: user-events\nSchema ID: 12345\nVersion: 1\nFingerprint: sha256:abc123...\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#with-metadata","title":"With Metadata","text":"<p>Add Description and Tags:</p> <pre><code>danube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file schemas/user-events.json \\\n  --description \"User registration and login events\" \\\n  --tags users \\\n  --tags authentication \\\n  --tags analytics\n</code></pre> <p>Tags for Organization:</p> <ul> <li><code>production</code>, <code>staging</code>, <code>development</code> - Environment</li> <li><code>team-analytics</code>, <code>team-platform</code> - Ownership</li> <li><code>pii</code>, <code>sensitive</code> - Data classification</li> <li><code>v1</code>, <code>v2</code> - Version tracking</li> <li><code>deprecated</code> - Lifecycle status</li> </ul>"},{"location":"danube_clis/danube_admin/schema_registry/#schema-types","title":"Schema Types","text":"Type Description Use Cases Extension <code>json_schema</code> JSON Schema (Draft 7) Web APIs, JavaScript/TypeScript <code>.json</code> <code>avro</code> Apache Avro Big data, Kafka integration <code>.avsc</code> <code>protobuf</code> Protocol Buffers gRPC, high performance <code>.proto</code> <code>string</code> Plain string (no validation) Simple text messages <code>.txt</code> <code>bytes</code> Raw bytes (no validation) Binary data -"},{"location":"danube_clis/danube_admin/schema_registry/#json-schema-example","title":"JSON Schema Example","text":"<p>schemas/user-events.json:</p> <pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"UserEvent\",\n  \"type\": \"object\",\n  \"required\": [\"event_type\", \"user_id\", \"timestamp\"],\n  \"properties\": {\n    \"event_type\": {\n      \"type\": \"string\",\n      \"enum\": [\"login\", \"logout\", \"register\"]\n    },\n    \"user_id\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\"\n    },\n    \"timestamp\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\"\n    },\n    \"metadata\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"ip_address\": { \"type\": \"string\" },\n        \"user_agent\": { \"type\": \"string\" }\n      }\n    }\n  }\n}\n</code></pre> <p>Register:</p> <pre><code>danube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file schemas/user-events.json \\\n  --description \"User authentication events\" \\\n  --tags users authentication\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#avro-schema-example","title":"Avro Schema Example","text":"<p>schemas/payment.avsc:</p> <pre><code>{\n  \"type\": \"record\",\n  \"name\": \"Payment\",\n  \"namespace\": \"com.example.payments\",\n  \"fields\": [\n    {\"name\": \"payment_id\", \"type\": \"string\"},\n    {\"name\": \"amount\", \"type\": \"double\"},\n    {\"name\": \"currency\", \"type\": \"string\"},\n    {\"name\": \"timestamp\", \"type\": \"long\"}\n  ]\n}\n</code></pre> <p>Register:</p> <pre><code>danube-admin-cli schemas register payment-events \\\n  --schema-type avro \\\n  --file schemas/payment.avsc \\\n  --description \"Payment transaction events\"\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#get-a-schema","title":"Get a Schema","text":"<p>Retrieve schema details by subject or ID.</p> <pre><code>danube-admin-cli schemas get [OPTIONS]\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#by-subject-latest-version","title":"By Subject (Latest Version)","text":"<pre><code># Get latest version\ndanube-admin-cli schemas get --subject user-events\n</code></pre> <p>Example Output:</p> <pre><code>Schema ID: 12345\nVersion: 2\nSubject: user-events\nType: json_schema\nCompatibility Mode: BACKWARD\nDescription: User registration and login events\nTags: users, authentication, analytics\n\nSchema Definition:\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"UserEvent\",\n  \"type\": \"object\",\n  \"required\": [\"event_type\", \"user_id\", \"timestamp\"],\n  ...\n}\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#by-schema-id","title":"By Schema ID","text":"<pre><code># Get specific schema by ID\ndanube-admin-cli schemas get --id 12345\n\n# Get specific version of a schema\ndanube-admin-cli schemas get --id 12345 --version 1\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#json-output","title":"JSON Output","text":"<pre><code>danube-admin-cli schemas get --subject user-events --output json\n</code></pre> <p>Example JSON Output:</p> <pre><code>{\n  \"schema_id\": 12345,\n  \"version\": 2,\n  \"subject\": \"user-events\",\n  \"schema_type\": \"json_schema\",\n  \"schema_definition\": \"{ ... }\",\n  \"description\": \"User registration and login events\",\n  \"created_at\": 1704067200,\n  \"created_by\": \"admin\",\n  \"tags\": [\"users\", \"authentication\", \"analytics\"],\n  \"fingerprint\": \"sha256:abc123...\",\n  \"compatibility_mode\": \"BACKWARD\"\n}\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#list-schema-versions","title":"List Schema Versions","text":"<p>View all versions for a schema subject.</p> <pre><code>danube-admin-cli schemas versions &lt;SUBJECT&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli schemas versions user-events\n</code></pre> <p>Example Output:</p> <pre><code>Versions for subject 'user-events':\n  Version 1: schema_id=12344, fingerprint=sha256:old123...\n    Created by: alice\n    Description: Initial schema\n  Version 2: schema_id=12345, fingerprint=sha256:abc123...\n    Created by: bob\n    Description: Added email field\n  Version 3: schema_id=12346, fingerprint=sha256:new456...\n    Created by: charlie\n    Description: Made phone optional\n</code></pre> <p>JSON Output:</p> <pre><code>danube-admin-cli schemas versions user-events --output json\n</code></pre> <p>Example JSON:</p> <pre><code>[\n  {\n    \"version\": 1,\n    \"schema_id\": 12344,\n    \"created_at\": 1704067200,\n    \"created_by\": \"alice\",\n    \"description\": \"Initial schema\",\n    \"fingerprint\": \"sha256:old123...\"\n  },\n  {\n    \"version\": 2,\n    \"schema_id\": 12345,\n    \"created_at\": 1704153600,\n    \"created_by\": \"bob\",\n    \"description\": \"Added email field\",\n    \"fingerprint\": \"sha256:abc123...\"\n  }\n]\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#check-schema-compatibility","title":"Check Schema Compatibility","text":"<p>Verify if a new schema is compatible with existing versions.</p> <pre><code>danube-admin-cli schemas check &lt;SUBJECT&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli schemas check user-events \\\n  --file schemas/user-events-v2.json \\\n  --schema-type json_schema\n</code></pre> <p>Example Output (Compatible):</p> <pre><code>\u2705 Schema is compatible with subject 'user-events'\n</code></pre> <p>Example Output (Incompatible):</p> <pre><code>\u274c Schema is NOT compatible with subject 'user-events'\n\nCompatibility errors:\n  - Field 'user_id' was removed (breaking change)\n  - Field 'email' is now required (breaking change for existing consumers)\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#override-compatibility-mode","title":"Override Compatibility Mode","text":"<pre><code># Check with specific mode (overrides subject's default)\ndanube-admin-cli schemas check user-events \\\n  --file schemas/user-events-v2.json \\\n  --schema-type json_schema \\\n  --mode full\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#workflow-example","title":"Workflow Example","text":"<pre><code># Step 1: Create new schema version\nvim schemas/user-events-v2.json\n\n# Step 2: Check compatibility BEFORE registering\ndanube-admin-cli schemas check user-events \\\n  --file schemas/user-events-v2.json \\\n  --schema-type json_schema\n\n# Step 3: If compatible, register it\nif [ $? -eq 0 ]; then\n  danube-admin-cli schemas register user-events \\\n    --schema-type json_schema \\\n    --file schemas/user-events-v2.json \\\n    --description \"Added email field\"\nfi\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#get-compatibility-mode","title":"Get Compatibility Mode","text":"<p>Retrieve the current compatibility mode for a schema subject.</p> <pre><code>danube-admin-cli schemas get-compatibility &lt;SUBJECT&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code># Get compatibility mode for a subject\ndanube-admin-cli schemas get-compatibility user-events\n</code></pre> <p>Example Output:</p> <pre><code>Subject: user-events\nCompatibility Mode: BACKWARD\n</code></pre> <p>JSON Output:</p> <pre><code>danube-admin-cli schemas get-compatibility user-events --output json\n</code></pre> <p>Example JSON:</p> <pre><code>{\n  \"subject\": \"user-events\",\n  \"compatibility_mode\": \"BACKWARD\"\n}\n</code></pre> <p>Use Cases:</p> <ul> <li>Verify current compatibility settings before making changes</li> <li>Audit schema governance across subjects</li> <li>Automation scripts that need to check compatibility mode</li> </ul>"},{"location":"danube_clis/danube_admin/schema_registry/#set-compatibility-mode","title":"Set Compatibility Mode","text":"<p>Configure how schema evolution is enforced.</p> <pre><code>danube-admin-cli schemas set-compatibility &lt;SUBJECT&gt; --mode &lt;MODE&gt;\n</code></pre> <p>Compatibility Modes:</p> Mode Description Allows Use Case <code>none</code> No compatibility checks Any changes Development, testing <code>backward</code> New schema can read old data Add optional fields, remove fields Most common - new consumers, old producers <code>forward</code> Old schema can read new data Remove optional fields, add fields New producers, old consumers <code>full</code> Both backward and forward Add/remove optional fields only Strict compatibility <p>Examples:</p> <pre><code># Set backward compatibility (most common)\ndanube-admin-cli schemas set-compatibility user-events --mode backward\n\n# Set full compatibility (strict)\ndanube-admin-cli schemas set-compatibility payment-events --mode full\n\n# Disable compatibility (development only)\ndanube-admin-cli schemas set-compatibility test-events --mode none\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Compatibility mode set for subject 'user-events'\nMode: BACKWARD\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#backward-compatibility-recommended","title":"Backward Compatibility (Recommended)","text":"<p>Allows:</p> <ul> <li>\u2705 Adding optional fields</li> <li>\u2705 Removing fields</li> <li>\u2705 Adding enum values</li> </ul> <p>Prevents:</p> <ul> <li>\u274c Removing required fields</li> <li>\u274c Changing field types</li> <li>\u274c Making optional fields required</li> </ul> <p>Example:</p> <pre><code># Old schema\n{\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"name\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\", \"name\"]\n}\n\n# New schema (backward compatible)\n{\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"name\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\"}  // \u2705 Added optional field\n  },\n  \"required\": [\"user_id\", \"name\"]\n}\n</code></pre>"},{"location":"danube_clis/danube_admin/schema_registry/#forward-compatibility","title":"Forward Compatibility","text":"<p>Allows:</p> <ul> <li>\u2705 Removing optional fields</li> <li>\u2705 Adding fields</li> </ul> <p>Prevents:</p> <ul> <li>\u274c Adding required fields</li> <li>\u274c Changing field types</li> </ul>"},{"location":"danube_clis/danube_admin/schema_registry/#full-compatibility-strictest","title":"Full Compatibility (Strictest)","text":"<p>Allows:</p> <ul> <li>\u2705 Only changes that are both backward AND forward compatible</li> <li>\u2705 Adding optional fields with defaults</li> <li>\u2705 Removing optional fields</li> </ul> <p>Prevents:</p> <ul> <li>\u274c Most breaking changes</li> <li>\u274c Required field modifications</li> </ul>"},{"location":"danube_clis/danube_admin/schema_registry/#delete-schema-version","title":"Delete Schema Version","text":"<p>Remove a specific version of a schema.</p> <pre><code>danube-admin-cli schemas delete &lt;SUBJECT&gt; --version &lt;VERSION&gt; --confirm\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli schemas delete user-events --version 1 --confirm\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Deleted version 1 of subject 'user-events'\n</code></pre> <p>\u26a0\ufe0f Important Notes:</p> <ol> <li>Requires Confirmation: Must use <code>--confirm</code> flag to prevent accidents</li> <li>Cannot Delete Active: Cannot delete version currently used by topics</li> <li>No Undo: Deletion is permanent</li> <li>Version History: Gaps in version numbers are normal after deletion</li> </ol> <p>Safety Checks:</p> <pre><code># Step 1: List all versions\ndanube-admin-cli schemas versions user-events\n\n# Step 2: Check which version is active\ndanube-admin-cli topics describe /production/events | grep \"Version:\"\n\n# Step 3: Only delete if not active and confirmed safe\ndanube-admin-cli schemas delete user-events --version 1 --confirm\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/","title":"Topics Management","text":"<p>Create and manage topics in your Danube cluster.</p>"},{"location":"danube_clis/danube_admin/topics/#overview","title":"Overview","text":"<p>Topics are the fundamental messaging primitive in Danube. They provide:</p> <ul> <li>Named channels for publishing and subscribing to messages</li> <li>Schema enforcement via Schema Registry</li> <li>Partitioning for horizontal scaling</li> <li>Reliable or non-reliable delivery modes</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#commands","title":"Commands","text":""},{"location":"danube_clis/danube_admin/topics/#list-topics","title":"List Topics","text":"<p>View topics in a namespace or on a specific broker.</p> <pre><code>danube-admin-cli topics list [OPTIONS]\n</code></pre> <p>By Namespace:</p> <pre><code># List all topics in a namespace\ndanube-admin-cli topics list --namespace default\n\n# JSON output for automation\ndanube-admin-cli topics list --namespace default --output json\n</code></pre> <p>By Broker:</p> <pre><code># List topics on a specific broker\ndanube-admin-cli topics list --broker broker-001\n\n# JSON output\ndanube-admin-cli topics list --broker broker-001 --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Topics in namespace 'default':\n  /default/user-events\n  /default/payment-transactions\n  /default/analytics-stream\n</code></pre> <p>Example Output (JSON):</p> <pre><code>[\n  \"/default/user-events\",\n  \"/default/payment-transactions\",\n  \"/default/analytics-stream\"\n]\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#create-a-topic","title":"Create a Topic","text":"<p>Create a new topic with optional schema validation.</p> <pre><code>danube-admin-cli topics create &lt;TOPIC&gt; [OPTIONS]\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#basic-topic-creation","title":"Basic Topic Creation","text":"<p>Simple Topic (No Schema):</p> <pre><code># Create topic without schema\ndanube-admin-cli topics create /default/logs\n\n# Create with reliable delivery\ndanube-admin-cli topics create /default/events --dispatch-strategy reliable\n</code></pre> <p>Using Namespace Flag:</p> <pre><code># Specify namespace separately\ndanube-admin-cli topics create my-topic --namespace default\n\n# Equivalent to\ndanube-admin-cli topics create /default/my-topic\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#schema-validated-topics","title":"Schema-Validated Topics","text":"<p>With Schema Registry:</p> <pre><code># First, register a schema\ndanube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file user-schema.json\n\n# Create topic with schema validation\ndanube-admin-cli topics create /default/user-events \\\n  --schema-subject user-events \\\n  --dispatch-strategy reliable\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Topic created: /default/user-events\n   Schema subject: user-events\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#partitioned-topics","title":"Partitioned Topics","text":"<p>Create with Partitions:</p> <pre><code># Create partitioned topic (3 partitions)\ndanube-admin-cli topics create /default/high-throughput \\\n  --partitions 3\n\n# With schema and partitions\ndanube-admin-cli topics create /default/user-events \\\n  --partitions 5 \\\n  --schema-subject user-events \\\n  --dispatch-strategy reliable\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Partitioned topic created: /default/high-throughput\n   Schema subject: user-events\n   Partitions: 5\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#options-reference","title":"Options Reference","text":"Option Description Default Example <code>--namespace</code> Namespace (if not in topic path) - <code>--namespace default</code> <code>--partitions</code> Number of partitions 1 <code>--partitions 3</code> <code>--schema-subject</code> Schema subject from registry None <code>--schema-subject user-events</code> <code>--dispatch-strategy</code> Delivery mode <code>non_reliable</code> <code>--dispatch-strategy reliable</code> <p>Dispatch Strategies:</p> <ul> <li>non_reliable: Fast, at-most-once delivery (fire-and-forget)</li> <li>Use for: Logs, metrics, non-critical events</li> <li>Pros: Low latency, high throughput</li> <li> <p>Cons: Messages may be lost</p> </li> <li> <p>reliable: Slower, at-least-once delivery (with acknowledgments)</p> </li> <li>Use for: Transactions, orders, critical events</li> <li>Pros: Guaranteed delivery</li> <li>Cons: Higher latency</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#describe-a-topic","title":"Describe a Topic","text":"<p>View detailed information about a topic including schema and subscriptions.</p> <pre><code>danube-admin-cli topics describe &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli topics describe /default/user-events\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text (default) - human-readable\ndanube-admin-cli topics describe /default/user-events\n\n# JSON format - for automation\ndanube-admin-cli topics describe /default/user-events --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Topic: /default/user-events\nBroker ID: broker-001\nDelivery: Reliable\n\n\ud83d\udccb Schema Registry:\n  Subject: user-events\n  Schema ID: 12345\n  Version: 2\n  Type: json_schema\n  Compatibility: BACKWARD\n\nSubscriptions: [\"analytics-consumer\", \"audit-logger\"]\n</code></pre> <p>Example Output (JSON):</p> <pre><code>{\n  \"topic\": \"/default/user-events\",\n  \"broker_id\": \"broker-001\",\n  \"delivery\": \"Reliable\",\n  \"schema_subject\": \"user-events\",\n  \"schema_id\": 12345,\n  \"schema_version\": 2,\n  \"schema_type\": \"json_schema\",\n  \"compatibility_mode\": \"BACKWARD\",\n  \"subscriptions\": [\n    \"analytics-consumer\",\n    \"audit-logger\"\n  ]\n}\n</code></pre> <p>Without Schema:</p> <pre><code>Topic: /default/logs\nBroker ID: broker-002\nDelivery: NonReliable\n\n\ud83d\udccb Schema: None\n\nSubscriptions: []\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#list-subscriptions","title":"List Subscriptions","text":"<p>View all active subscriptions for a topic.</p> <pre><code>danube-admin-cli topics subscriptions &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli topics subscriptions /default/user-events\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text\ndanube-admin-cli topics subscriptions /default/user-events\n\n# JSON format\ndanube-admin-cli topics subscriptions /default/user-events --output json\n</code></pre> <p>Example Output:</p> <pre><code>Subscriptions: [\"consumer-1\", \"consumer-2\", \"analytics-team\"]\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#delete-a-topic","title":"Delete a Topic","text":"<p>Permanently remove a topic and all its messages.</p> <pre><code>danube-admin-cli topics delete &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli topics delete /default/old-topic\n</code></pre> <p>With Namespace:</p> <pre><code>danube-admin-cli topics delete old-topic --namespace default\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Topic deleted: /default/old-topic\n</code></pre> <p>\u26a0\ufe0f Important Warnings:</p> <ol> <li>Data Loss: All messages in the topic are permanently deleted</li> <li>No Confirmation: Operation is immediate and irreversible</li> <li>Active Subscriptions: All consumers will be disconnected</li> <li>Schema Intact: The schema in the registry is NOT deleted</li> </ol> <p>Safety Checklist:</p> <pre><code># 1. Check subscriptions\ndanube-admin-cli topics subscriptions /default/my-topic\n\n# 2. Verify topic details\ndanube-admin-cli topics describe /default/my-topic\n\n# 3. Backup if needed (application-level)\n\n# 4. Delete topic\ndanube-admin-cli topics delete /default/my-topic\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#unsubscribe","title":"Unsubscribe","text":"<p>Remove a specific subscription from a topic.</p> <pre><code>danube-admin-cli topics unsubscribe &lt;TOPIC&gt; --subscription &lt;NAME&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli topics unsubscribe /default/user-events \\\n  --subscription old-consumer\n</code></pre> <p>With Namespace:</p> <pre><code>danube-admin-cli topics unsubscribe my-topic \\\n  --namespace default \\\n  --subscription old-consumer\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Unsubscribed: true\n</code></pre> <p>Use Cases:</p> <ul> <li>Remove inactive consumers</li> <li>Clean up test subscriptions</li> <li>Force consumer reconnection</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#unload-a-topic","title":"Unload a Topic","text":"<p>Gracefully unload a topic from its current broker.</p> <pre><code>danube-admin-cli topics unload &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code>danube-admin-cli topics unload /default/user-events\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Topic unloaded: /default/user-events\n</code></pre> <p>Use Cases:</p> <ul> <li>Rebalance topics across brokers</li> <li>Prepare for broker maintenance</li> <li>Move topic to different broker</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#schema-configuration-commands-admin-only","title":"Schema Configuration Commands (Admin-Only)","text":""},{"location":"danube_clis/danube_admin/topics/#configure-topic-schema","title":"Configure Topic Schema","text":"<p>Configure complete schema settings for a topic including schema subject, validation policy, and payload validation (admin-only operation).</p> <pre><code>danube-admin-cli topics configure-schema &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code># Configure topic with schema and strict validation\ndanube-admin-cli topics configure-schema /default/user-events \\\n  --subject user-events \\\n  --validation-policy enforce \\\n  --enable-payload-validation\n</code></pre> <p>Options:</p> Option Required Values Description <code>--subject</code> Yes String Schema subject name from registry <code>--validation-policy</code> No <code>none</code>, <code>warn</code>, <code>enforce</code> Validation strictness (default: <code>none</code>) <code>--enable-payload-validation</code> No Flag Enable deep payload validation <code>--namespace</code> No String Namespace if not in topic path <p>Validation Policies:</p> Policy Behavior Use Case <code>none</code> No validation Development topics, unstructured data <code>warn</code> Validate and log errors, accept messages Monitoring/debugging production <code>enforce</code> Reject invalid messages Production requiring strict data quality <p>Examples:</p> <pre><code># Production: Strict validation\ndanube-admin-cli topics configure-schema /production/orders \\\n  --subject order-events \\\n  --validation-policy enforce \\\n  --enable-payload-validation\n\n# Staging: Warn on validation errors\ndanube-admin-cli topics configure-schema /staging/orders \\\n  --subject order-events \\\n  --validation-policy warn \\\n  --enable-payload-validation\n\n# Development: No validation\ndanube-admin-cli topics configure-schema /dev/orders \\\n  --subject order-events \\\n  --validation-policy none\n\n# Using namespace flag\ndanube-admin-cli topics configure-schema my-topic \\\n  --namespace production \\\n  --subject events \\\n  --validation-policy enforce\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Schema configuration set for topic '/production/orders'\n   Schema Subject: order-events\n   Validation Policy: ENFORCE\n   Payload Validation: ENABLED\n</code></pre> <p>\u26a0\ufe0f Important Notes:</p> <ol> <li>Admin-Only: Only administrators can change topic schema configuration</li> <li>Schema Must Exist: The schema subject must be registered before configuring</li> <li>Overrides First Producer: This overrides the schema set by first producer</li> <li>Affects All Producers: All producers must use the configured schema subject</li> </ol>"},{"location":"danube_clis/danube_admin/topics/#set-validation-policy","title":"Set Validation Policy","text":"<p>Update the validation policy for a topic without changing its schema subject (admin-only operation).</p> <pre><code>danube-admin-cli topics set-validation-policy &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code># Change validation policy to warn\ndanube-admin-cli topics set-validation-policy /production/events \\\n  --policy warn \\\n  --enable-payload-validation\n</code></pre> <p>Options:</p> Option Required Values Description <code>--policy</code> Yes <code>none</code>, <code>warn</code>, <code>enforce</code> Validation policy to set <code>--enable-payload-validation</code> No Flag Enable/disable payload validation <code>--namespace</code> No String Namespace if not in topic path <p>Examples:</p> <pre><code># Enable strict validation for production\ndanube-admin-cli topics set-validation-policy /production/events \\\n  --policy enforce \\\n  --enable-payload-validation\n\n# Switch to warn mode for debugging\ndanube-admin-cli topics set-validation-policy /production/events \\\n  --policy warn \\\n  --enable-payload-validation\n\n# Disable validation temporarily\ndanube-admin-cli topics set-validation-policy /production/events \\\n  --policy none\n</code></pre> <p>Example Output:</p> <pre><code>\u2705 Validation policy updated for topic '/production/events'\n   Policy: WARN\n   Payload Validation: ENABLED\n</code></pre> <p>Use Cases:</p> <ul> <li>Production Issue: Temporarily switch to <code>warn</code> mode to allow messages through while debugging</li> <li>Gradual Rollout: Start with <code>warn</code>, monitor errors, then switch to <code>enforce</code></li> <li>Performance Tuning: Disable payload validation if schema ID matching is sufficient</li> <li>Emergency Override: Switch to <code>none</code> if validation is blocking critical messages</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#get-schema-configuration","title":"Get Schema Configuration","text":"<p>Retrieve the current schema configuration for a topic.</p> <pre><code>danube-admin-cli topics get-schema-config &lt;TOPIC&gt; [OPTIONS]\n</code></pre> <p>Basic Usage:</p> <pre><code># Get schema configuration\ndanube-admin-cli topics get-schema-config /production/events\n</code></pre> <p>Output Formats:</p> <pre><code># Plain text (default)\ndanube-admin-cli topics get-schema-config /production/events\n\n# JSON format for automation\ndanube-admin-cli topics get-schema-config /production/events --output json\n</code></pre> <p>Example Output (Plain Text):</p> <pre><code>Topic: /production/events\nSchema Subject: user-events\nValidation Policy: ENFORCE\nPayload Validation: ENABLED\nCached Schema ID: 12345\n</code></pre> <p>Example Output (No Schema Configured):</p> <pre><code>Topic: /production/logs\nNo schema configured for this topic\n</code></pre> <p>Example Output (JSON):</p> <pre><code>{\n  \"topic\": \"/production/events\",\n  \"schema_subject\": \"user-events\",\n  \"validation_policy\": \"ENFORCE\",\n  \"enable_payload_validation\": true,\n  \"schema_id\": 12345\n}\n</code></pre> <p>Use Cases:</p> <ul> <li>Verify schema configuration after changes</li> <li>Audit which topics have schema validation enabled</li> <li>Automation scripts that need to query topic settings</li> <li>Troubleshoot validation issues by checking current configuration</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#common-workflows","title":"Common Workflows","text":""},{"location":"danube_clis/danube_admin/topics/#1-create-topic-with-schema-validation","title":"1. Create Topic with Schema Validation","text":"<p>Step-by-step:</p> <pre><code># Step 1: Register schema\ndanube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file schemas/user-events.json \\\n  --description \"User event schema\" \\\n  --tags users analytics\n\n# Step 2: Verify schema\ndanube-admin-cli schemas get --subject user-events\n\n# Step 3: Create topic\ndanube-admin-cli topics create /production/user-events \\\n  --schema-subject user-events \\\n  --dispatch-strategy reliable \\\n  --partitions 5\n\n# Step 4: Verify topic\ndanube-admin-cli topics describe /production/user-events\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#2-schema-evolution","title":"2. Schema Evolution","text":"<p>Update schema for existing topic:</p> <pre><code># Step 1: Check current compatibility mode\ndanube-admin-cli schemas get --subject user-events\n\n# Step 2: Test new schema compatibility\ndanube-admin-cli schemas check user-events \\\n  --file schemas/user-events-v2.json \\\n  --schema-type json_schema\n\n# Step 3: If compatible, register new version\ndanube-admin-cli schemas register user-events \\\n  --schema-type json_schema \\\n  --file schemas/user-events-v2.json \\\n  --description \"Added email field\"\n\n# Step 4: Verify topic picked up new version\ndanube-admin-cli topics describe /production/user-events\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#3-multi-environment-deployment","title":"3. Multi-Environment Deployment","text":"<p>Create same topics across environments:</p> <pre><code># Production\ndanube-admin-cli topics create /production/user-events \\\n  --schema-subject user-events \\\n  --dispatch-strategy reliable \\\n  --partitions 10\n\n# Staging\ndanube-admin-cli topics create /staging/user-events \\\n  --schema-subject user-events \\\n  --dispatch-strategy reliable \\\n  --partitions 3\n\n# Development\ndanube-admin-cli topics create /development/user-events \\\n  --schema-subject user-events-dev \\\n  --partitions 1\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#4-topic-migration","title":"4. Topic Migration","text":"<p>Move topic to new namespace:</p> <pre><code># Step 1: Create new namespace\ndanube-admin-cli namespaces create new-namespace\n\n# Step 2: Get old topic schema\nOLD_SCHEMA=$(danube-admin-cli topics describe /old/topic --output json | jq -r '.schema_subject')\n\n# Step 3: Create new topic with same schema\ndanube-admin-cli topics create /new-namespace/topic \\\n  --schema-subject $OLD_SCHEMA \\\n  --dispatch-strategy reliable\n\n# Step 4: Migrate consumers (application-level)\n\n# Step 5: Delete old topic\ndanube-admin-cli topics delete /old/topic\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#5-admin-schema-configuration-production","title":"5. Admin Schema Configuration (Production)","text":"<p>Configure validation policies for existing topics:</p> <pre><code># Step 1: Register schema with full compatibility\ndanube-admin-cli schemas register order-events \\\n  --schema-type json_schema \\\n  --file schemas/orders.json \\\n  --description \"Order transaction schema\"\n\ndanube-admin-cli schemas set-compatibility order-events --mode full\n\n# Step 2: Create topic (no schema initially)\ndanube-admin-cli topics create /production/orders \\\n  --dispatch-strategy reliable \\\n  --partitions 5\n\n# Step 3: Configure schema with strict validation (admin-only)\ndanube-admin-cli topics configure-schema /production/orders \\\n  --subject order-events \\\n  --validation-policy enforce \\\n  --enable-payload-validation\n\n# Step 4: Verify configuration\ndanube-admin-cli topics get-schema-config /production/orders\n\n# Step 5: Monitor for issues, adjust if needed\n# If issues found, temporarily switch to warn mode\ndanube-admin-cli topics set-validation-policy /production/orders \\\n  --policy warn \\\n  --enable-payload-validation\n\n# Step 6: After fixes, re-enable strict validation\ndanube-admin-cli topics set-validation-policy /production/orders \\\n  --policy enforce \\\n  --enable-payload-validation\n</code></pre>"},{"location":"danube_clis/danube_admin/topics/#related-commands","title":"Related Commands","text":""},{"location":"danube_clis/danube_admin/topics/#schema-management","title":"Schema Management","text":"<ul> <li><code>danube-admin-cli schemas register</code> - Register schemas for validation</li> <li><code>danube-admin-cli schemas get</code> - View schema details</li> <li><code>danube-admin-cli schemas get-compatibility</code> - Get compatibility mode</li> <li><code>danube-admin-cli schemas set-compatibility</code> - Set compatibility mode</li> <li><code>danube-admin-cli schemas delete</code> - Delete schema versions</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#topic-schema-configuration-admin","title":"Topic Schema Configuration (Admin)","text":"<ul> <li><code>danube-admin-cli topics configure-schema</code> - Configure topic schema settings</li> <li><code>danube-admin-cli topics set-validation-policy</code> - Update validation policy</li> <li><code>danube-admin-cli topics get-schema-config</code> - Get topic schema configuration</li> </ul>"},{"location":"danube_clis/danube_admin/topics/#cluster-management","title":"Cluster Management","text":"<ul> <li><code>danube-admin-cli namespaces create</code> - Create namespaces for topics</li> <li><code>danube-admin-cli brokers list</code> - View broker topology</li> </ul>"},{"location":"danube_clis/danube_cli/consumer/","title":"Consumer Guide","text":"<p>Learn how to consume messages from Danube topics. \ud83d\udce5</p>"},{"location":"danube_clis/danube_cli/consumer/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic Usage</li> <li>Subscription Types</li> <li>Schema-Based Consumption</li> <li>Advanced Patterns</li> <li>Practical Examples</li> </ul>"},{"location":"danube_clis/danube_cli/consumer/#basic-usage","title":"Basic Usage","text":""},{"location":"danube_clis/danube_cli/consumer/#simple-consumption","title":"Simple Consumption","text":"<pre><code>danube-cli consume \\\n  --service-addr http://localhost:6650 \\\n  --subscription my-subscription\n</code></pre> <p>Consumes from the default topic <code>/default/test_topic</code> with a shared subscription.</p>"},{"location":"danube_clis/danube_cli/consumer/#custom-topic","title":"Custom Topic","text":"<pre><code>danube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  -m order-processors\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#using-short-flags","title":"Using Short Flags","text":"<pre><code>danube-cli consume -s http://localhost:6650 -t /default/events -m event-sub\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#custom-consumer-name","title":"Custom Consumer Name","text":"<pre><code>danube-cli consume \\\n  -s http://localhost:6650 \\\n  -n order-consumer-1 \\\n  -m order-subscription\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#subscription-types","title":"Subscription Types","text":""},{"location":"danube_clis/danube_cli/consumer/#shared-default","title":"Shared (Default)","text":"<p>Multiple consumers share message processing. Messages are distributed across consumers.</p> <pre><code># Consumer 1\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  -m shared-sub \\\n  --sub-type shared\n\n# Consumer 2 (run in parallel)\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  -m shared-sub \\\n  --sub-type shared\n</code></pre> <p>Use case: Load balancing, parallel processing</p>"},{"location":"danube_clis/danube_cli/consumer/#exclusive","title":"Exclusive","text":"<p>Only one consumer can be active at a time. Ensures ordered processing.</p> <pre><code>danube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  -m exclusive-sub \\\n  --sub-type exclusive\n</code></pre> <p>Use case: Ordered message processing, single consumer workflows</p>"},{"location":"danube_clis/danube_cli/consumer/#failover","title":"Failover","text":"<p>Multiple consumers but only one is active. Others act as standby.</p> <pre><code># Primary consumer\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/critical \\\n  -m ha-sub \\\n  --sub-type fail-over\n\n# Standby consumer (automatically takes over if primary fails)\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/critical \\\n  -m ha-sub \\\n  --sub-type fail-over\n</code></pre> <p>Use case: High availability, ordered processing with failover</p>"},{"location":"danube_clis/danube_cli/consumer/#schema-based-consumption","title":"Schema-Based Consumption","text":""},{"location":"danube_clis/danube_cli/consumer/#auto-detection","title":"Auto-Detection","text":"<p>The consumer automatically detects and validates against the topic's schema:</p> <pre><code>danube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  -m order-sub\n</code></pre> <p>Output shows schema validation:</p> <pre><code>\ud83d\udd0d Checking for schema associated with topic...\n\u2705 Topic has schema: orders (json_schema, version 1)\n\ud83d\udce5 Consuming with schema validation...\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#without-schema","title":"Without Schema","text":"<p>If the topic has no schema:</p> <pre><code>danube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/raw-data \\\n  -m raw-sub\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd0d Checking for schema associated with topic...\n\u2139\ufe0f  Topic has no schema - consuming raw bytes\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#schema-evolution","title":"Schema Evolution","text":"<p>Consumers automatically handle schema evolution:</p> <pre><code># Producer sends message with v1 schema\ndanube-cli produce \\\n  -t /default/users \\\n  --schema-subject users \\\n  -m '{\"user_id\":\"123\",\"name\":\"Alice\"}'\n\n# Schema evolves to v2 (adds optional \"email\" field)\ndanube-cli schema register users \\\n  --schema-type json_schema \\\n  --file users-v2.json\n\n# Consumer automatically uses latest schema\ndanube-cli consume \\\n  -t /default/users \\\n  -m user-processors\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"danube_clis/danube_cli/consumer/#fan-out-pattern","title":"Fan-Out Pattern","text":"<p>Multiple subscriptions on the same topic for different purposes:</p> <pre><code># Subscription 1: Process orders\ndanube-cli consume -t /default/orders -m order-processing &amp;\n\n# Subscription 2: Analytics\ndanube-cli consume -t /default/orders -m order-analytics &amp;\n\n# Subscription 3: Notifications\ndanube-cli consume -t /default/orders -m order-notifications &amp;\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#worker-pool-pattern","title":"Worker Pool Pattern","text":"<p>Multiple consumers in a shared subscription for parallel processing:</p> <pre><code># Start 4 workers\nfor i in {1..4}; do\n  danube-cli consume \\\n    -s http://localhost:6650 \\\n    -t /default/tasks \\\n    -n \"worker-$i\" \\\n    -m task-workers \\\n    --sub-type shared &amp;\ndone\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#multi-stage-pipeline","title":"Multi-Stage Pipeline","text":"<p>Process messages through multiple stages:</p> <pre><code># Stage 1: Consume from source\ndanube-cli consume -t /pipeline/raw-events -m stage1 &amp;\n\n# Stage 2: Consume from enriched\ndanube-cli consume -t /pipeline/enriched-events -m stage2 &amp;\n\n# Stage 3: Consume from processed\ndanube-cli consume -t /pipeline/processed-events -m stage3 &amp;\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#practical-examples","title":"Practical Examples","text":""},{"location":"danube_clis/danube_cli/consumer/#e-commerce-order-processing","title":"E-Commerce Order Processing","text":"<pre><code># Multiple workers processing orders\nfor i in {1..3}; do\n  danube-cli consume \\\n    -s http://localhost:6650 \\\n    -t /default/orders \\\n    -n \"order-worker-$i\" \\\n    -m order-processors \\\n    --sub-type shared &amp;\ndone\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#real-time-analytics","title":"Real-Time Analytics","text":"<pre><code># Exclusive consumer for ordered analytics\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /analytics/events \\\n  -m analytics-processor \\\n  --sub-type exclusive\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#event-driven-microservices","title":"Event-Driven Microservices","text":"<pre><code># Service 1: User service\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/user-events \\\n  -m user-service &amp;\n\n# Service 2: Email service\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/user-events \\\n  -m email-service &amp;\n\n# Service 3: Analytics service\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/user-events \\\n  -m analytics-service &amp;\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#iot-data-collection","title":"IoT Data Collection","text":"<pre><code># Consume sensor data with high availability\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /iot/sensors \\\n  -m sensor-processor \\\n  --sub-type fail-over\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#log-aggregation","title":"Log Aggregation","text":"<pre><code># Consume logs from multiple sources\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /logs/application \\\n  -m log-aggregator \\\n  --sub-type shared\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#notification-service","title":"Notification Service","text":"<pre><code># Process notifications with worker pool\nfor i in {1..5}; do\n  danube-cli consume \\\n    -s http://localhost:6650 \\\n    -t /notifications/queue \\\n    -n \"notification-worker-$i\" \\\n    -m notification-processors \\\n    --sub-type shared &amp;\ndone\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#message-output-format","title":"Message Output Format","text":""},{"location":"danube_clis/danube_cli/consumer/#text-messages","title":"Text Messages","text":"<pre><code>Received message: Hello, World!\nSize: 13 bytes, Total received: 13 bytes\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#json-messages","title":"JSON Messages","text":"<pre><code>Received message: {\"user_id\":\"123\",\"action\":\"login\"}\nSize: 35 bytes, Total received: 35 bytes\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#binary-data","title":"Binary Data","text":"<pre><code>Received message: [binary data - 1024 bytes]\nSize: 1024 bytes, Total received: 1024 bytes\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#with-schema-validation","title":"With Schema Validation","text":"<pre><code>\u2705 Message validated against schema 'orders' (version 1)\nReceived message: {\"order_id\":\"ord_123\",\"amount\":99.99}\nSize: 42 bytes, Total received: 42 bytes\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#command-reference","title":"Command Reference","text":""},{"location":"danube_clis/danube_cli/consumer/#all-consumer-flags","title":"All Consumer Flags","text":"Flag Short Description Default <code>--service-addr</code> <code>-s</code> Broker URL Required <code>--topic</code> <code>-t</code> Topic name <code>/default/test_topic</code> <code>--subscription</code> <code>-m</code> Subscription name Required <code>--consumer-name</code> <code>-n</code> Consumer name <code>consumer_pubsub</code> <code>--sub-type</code> - Subscription type <code>shared</code>"},{"location":"danube_clis/danube_cli/consumer/#subscription-types_1","title":"Subscription Types","text":"Type Description Use Case <code>shared</code> Load balanced across consumers Parallel processing <code>exclusive</code> Single active consumer Ordered processing <code>fail-over</code> Active/standby with failover HA ordered processing"},{"location":"danube_clis/danube_cli/consumer/#scripting-with-consumers","title":"Scripting with Consumers","text":""},{"location":"danube_clis/danube_cli/consumer/#basic-shell-script","title":"Basic Shell Script","text":"<pre><code>#!/bin/bash\n\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  -m event-processor | \\\nwhile IFS= read -r line; do\n  echo \"Processing: $line\"\n  # Your processing logic here\ndone\n</code></pre>"},{"location":"danube_clis/danube_cli/consumer/#filter-messages","title":"Filter Messages","text":"<pre><code>#!/bin/bash\n\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  -m event-filter | \\\nwhile IFS= read -r line; do\n  # Process only specific messages\n  if echo \"$line\" | grep -q \"error\"; then\n    echo \"Error detected: $line\"\n    # Send alert\n  fi\ndone\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/","title":"Danube CLI Documentation","text":"<p>Welcome to the Danube CLI - command-line companion for interacting with Danube messaging system! \ud83d\ude80</p>"},{"location":"danube_clis/danube_cli/getting_started/#what-is-danube-cli","title":"What is Danube CLI?","text":"<p>Danube CLI is a powerful, easy-to-use command-line tool that lets you:</p> <ul> <li>\ud83d\udce4 Produce messages to topics with schema validation</li> <li>\ud83d\udce5 Consume messages from topics with automatic schema detection</li> <li>\ud83d\udccb Manage schemas in the schema registry</li> <li>\ud83d\udd04 Test your Danube deployment end-to-end</li> <li>\ud83d\udee0\ufe0f Develop and debug messaging workflows</li> </ul>"},{"location":"danube_clis/danube_cli/getting_started/#core-concepts","title":"Core Concepts","text":""},{"location":"danube_clis/danube_cli/getting_started/#topics","title":"Topics","text":"<p>Topics are logical channels where messages are published and consumed. Topic names follow a hierarchical structure:</p> <pre><code>/namespace/topic-name\n</code></pre> <p>Example: <code>/default/user-events</code>, <code>/production/orders</code></p>"},{"location":"danube_clis/danube_cli/getting_started/#producers","title":"Producers","text":"<p>Producers send messages to topics. They can:</p> <ul> <li>Send messages with or without schemas</li> <li>Configure partitioning for scalability</li> <li>Enable reliable delivery for critical messages</li> </ul>"},{"location":"danube_clis/danube_cli/getting_started/#consumers","title":"Consumers","text":"<p>Consumers receive messages from topics via subscriptions. They support:</p> <ul> <li>Multiple subscription types (Exclusive, Shared, Failover)</li> <li>Automatic schema validation</li> <li>Message acknowledgment</li> </ul>"},{"location":"danube_clis/danube_cli/getting_started/#schema-registry","title":"Schema Registry","text":"<p>The schema registry provides:</p> <ul> <li>Centralized schema management</li> <li>Schema evolution with compatibility checking</li> <li>Automatic validation for producers and consumers</li> </ul> <p>Whether you're testing a new deployment, debugging message flows, or building automation scripts, Danube CLI has you covered!</p>"},{"location":"danube_clis/danube_cli/getting_started/#quick-start","title":"Quick Start","text":"<p>Download the latest release for your system from Danube Releases:</p> <pre><code># Linux\nwget https://github.com/danube-messaging/danube/releases/download/v0.6.0/danube-cli-linux\nchmod +x danube-cli-linux\n\n# macOS (Apple Silicon)\nwget https://github.com/danube-messaging/danube/releases/download/v0.6.0/danube-cli-macos\nchmod +x danube-cli-macos\n\n# Windows\n# Download danube-cli-windows.exe from the releases page\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#add-to-path-optional","title":"Add to PATH (Optional)","text":"<p>For easier access, add the binary to your PATH:</p> <pre><code># Option 1: Copy to a directory in your PATH\nsudo cp danube-cli-linux /usr/local/bin/danube-cli\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#verify-installation","title":"Verify Installation","text":"<pre><code>danube-cli --version\ndanube-cli --help\n</code></pre> <p>You should see the CLI version and help information!</p> <p>Let's send and receive a simple message!</p>"},{"location":"danube_clis/danube_cli/getting_started/#your-first-message","title":"Your First Message","text":""},{"location":"danube_clis/danube_cli/getting_started/#step-1-produce-messages","title":"Step 1: Produce Messages","text":"<p>First, let's produce some messages (this also creates the topic):</p> <pre><code>danube-cli produce \\\n  --service-addr http://localhost:6650 \\\n  --topic /default/getting-started \\\n  --message \"Hello from Danube CLI!\" \\\n  --count 5\n</code></pre> <p>You should see:</p> <pre><code>\u2705 Producer 'test_producer' created successfully\n\ud83d\udce4 Message 1/5 sent successfully (ID: ...)\n\ud83d\udce4 Message 2/5 sent successfully (ID: ...)\n\ud83d\udce4 Message 3/5 sent successfully (ID: ...)\n\ud83d\udce4 Message 4/5 sent successfully (ID: ...)\n\ud83d\udce4 Message 5/5 sent successfully (ID: ...)\n\ud83d\udcca Summary:\n   \u2705 Success: 5\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#step-2-start-a-consumer","title":"Step 2: Start a Consumer","text":"<p>Now open a new terminal and start consuming the messages:</p> <pre><code>danube-cli consume \\\n  --service-addr http://localhost:6650 \\\n  --topic /default/getting-started \\\n  --subscription my-first-subscription\n</code></pre> <p>You should see:</p> <pre><code>\ud83d\udd0d Checking for schema associated with topic...\n\u2139\ufe0f  Topic has no schema - consuming raw bytes\nReceived message: Hello from Danube CLI!\nSize: 24 bytes, Total received: 24 bytes\nReceived message: Hello from Danube CLI!\nSize: 24 bytes, Total received: 48 bytes\n...\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#example-workflows","title":"Example Workflows","text":""},{"location":"danube_clis/danube_cli/getting_started/#add-a-schema","title":"Add a Schema","text":"<p>Schemas ensure your messages have the right structure:</p> <pre><code># 1. Create a simple schema file\ncat &gt; /tmp/user-schema.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"action\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\", \"action\"]\n}\nEOF\n\n# 2. Register the schema\ndanube-cli schema register user-events \\\n  --schema-type json_schema \\\n  --file /tmp/user-schema.json\n\n# 3. Produce with schema validation\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/user-events \\\n  --schema-subject user-events \\\n  -m '{\"user_id\":\"user_123\",\"action\":\"login\"}' \\\n  --count 5\n\n# 4. Consume with automatic validation\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/user-events \\\n  -m user-subscription\n</code></pre> <p>The consumer will automatically validate messages against the schema!</p>"},{"location":"danube_clis/danube_cli/getting_started/#send-multiple-messages","title":"Send Multiple Messages","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/test \\\n  -m \"Message\" \\\n  --count 20 \\\n  --interval 500\n</code></pre> <p>This sends 20 messages with a 500ms delay between each.</p>"},{"location":"danube_clis/danube_cli/getting_started/#use-different-subscription-types","title":"Use Different Subscription Types","text":"<pre><code># Exclusive: Only one consumer at a time\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/test \\\n  -m my-exclusive \\\n  --sub-type exclusive\n\n# Shared: Multiple consumers share messages\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /default/test \\\n  -m my-shared \\\n  --sub-type shared\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#common-patterns","title":"Common Patterns","text":""},{"location":"danube_clis/danube_cli/getting_started/#pattern-1-quick-test-message","title":"Pattern 1: Quick Test Message","text":"<pre><code># Shortest way to send a message\ndanube-cli produce -s http://localhost:6650 -m \"test\"\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#pattern-2-binary-files","title":"Pattern 2: Binary Files","text":"<pre><code># Send a file as a message\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  --file /path/to/data.bin\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#pattern-3-messages-with-metadata","title":"Pattern 3: Messages with Metadata","text":"<pre><code># Add attributes for routing/filtering\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Alert!\" \\\n  --attributes \"priority:high,region:us-west\"\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#pattern-4-partitioned-topics","title":"Pattern 4: Partitioned Topics","text":"<pre><code># Create a topic with partitions\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  --partitions 4 \\\n  -m \"Partitioned message\"\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#pattern-5-reliable-delivery","title":"Pattern 5: Reliable Delivery","text":"<pre><code># Guarantee message delivery\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Important message\" \\\n  --reliable\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#tips-for-success","title":"Tips for Success","text":""},{"location":"danube_clis/danube_cli/getting_started/#1-check-examples-in-help","title":"1. Check Examples in Help","text":"<p>Every command has examples built-in:</p> <pre><code>danube-cli produce --help     # See producer examples\ndanube-cli consume --help     # See consumer examples\ndanube-cli schema --help      # See schema examples\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#2-json-output-for-scripting","title":"2. JSON Output for Scripting","text":"<p>Use <code>--output json</code> for programmatic parsing:</p> <pre><code>danube-cli schema get user-events --output json | jq .\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#3-descriptive-names","title":"3. Descriptive Names","text":"<p>Use meaningful names for easier debugging:</p> <pre><code>danube-cli produce \\\n  --producer-name order-service-producer \\\n  --topic /production/orders \\\n  -m '{\"order_id\":\"123\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/getting_started/#whats-next","title":"What's Next?","text":"<p>You can explore more advanced features:</p> <ol> <li>\ud83d\udce4 Producer Guide - Message production</li> <li>\ud83d\udce5 Consumer Guide - Message consumption</li> <li>\ud83d\udccb Schema Registry - Schema management</li> </ol>"},{"location":"danube_clis/danube_cli/producer/","title":"Producer Guide","text":"<p>Learn how to produce messages to Danube topics. \ud83d\udce4</p>"},{"location":"danube_clis/danube_cli/producer/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic Usage</li> <li>Message Content Types</li> <li>Multiple Messages</li> <li>Message Attributes</li> <li>Schema-Based Production</li> <li>Partitioned Topics</li> <li>Reliable Delivery</li> </ul>"},{"location":"danube_clis/danube_cli/producer/#basic-usage","title":"Basic Usage","text":""},{"location":"danube_clis/danube_cli/producer/#simple-message","title":"Simple Message","text":"<pre><code>danube-cli produce \\\n  --service-addr http://localhost:6650 \\\n  --message \"Hello, World!\"\n</code></pre> <p>Sends one message to the default topic <code>/default/test_topic</code>.</p>"},{"location":"danube_clis/danube_cli/producer/#custom-topic","title":"Custom Topic","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  -m \"Order received\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#using-short-flags","title":"Using Short Flags","text":"<pre><code>danube-cli produce -s http://localhost:6650 -t /default/events -m \"Quick message\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#message-content-types","title":"Message Content Types","text":""},{"location":"danube_clis/danube_cli/producer/#text-messages","title":"Text Messages","text":"<pre><code>danube-cli produce -s http://localhost:6650 -m \"Simple text message\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#json-messages","title":"JSON Messages","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -m '{\"user_id\":\"123\",\"action\":\"login\",\"timestamp\":\"2024-01-15T10:30:00Z\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#binary-files","title":"Binary Files","text":"<pre><code># Send an image\ndanube-cli produce -s http://localhost:6650 --file image.png\n\n# Send any binary file\ndanube-cli produce -s http://localhost:6650 --file data.bin\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#multiple-messages","title":"Multiple Messages","text":""},{"location":"danube_clis/danube_cli/producer/#send-multiple-times","title":"Send Multiple Times","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Repeated message\" \\\n  --count 10\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#with-interval","title":"With Interval","text":"<pre><code># Send 100 messages with 500ms delay between each\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Message\" \\\n  --count 100 \\\n  --interval 500\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#rapid-fire","title":"Rapid Fire","text":"<pre><code># Minimum interval is 100ms\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Fast message\" \\\n  --count 1000 \\\n  --interval 100\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#message-attributes","title":"Message Attributes","text":""},{"location":"danube_clis/danube_cli/producer/#single-attribute","title":"Single Attribute","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Alert!\" \\\n  --attributes \"priority:high\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#multiple-attributes","title":"Multiple Attributes","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"User action\" \\\n  --attributes \"user_id:123,region:us-west,priority:high\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#schema-based-production","title":"Schema-Based Production","text":""},{"location":"danube_clis/danube_cli/producer/#with-pre-registered-schema-latest-version","title":"With Pre-Registered Schema (Latest Version)","text":"<pre><code># First, register the schema (one time)\ndanube-cli schema register orders \\\n  --schema-type json_schema \\\n  --file order-schema.json\n\n# Produce with schema validation (uses latest version)\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  --schema-subject orders \\\n  -m '{\"order_id\":\"ord_123\",\"amount\":99.99,\"currency\":\"USD\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#pin-to-specific-schema-version","title":"Pin to Specific Schema Version","text":"<p>Use a specific version instead of latest (useful for compatibility testing or controlled rollouts):</p> <pre><code># Use version 2 of the schema\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  --schema-subject orders \\\n  --schema-version 2 \\\n  -m '{\"order_id\":\"ord_456\",\"amount\":149.99,\"currency\":\"USD\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#use-minimum-schema-version","title":"Use Minimum Schema Version","text":"<p>Require a minimum schema version or newer:</p> <pre><code># Use version 3 or newer\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  --schema-subject orders \\\n  --schema-min-version 3 \\\n  -m '{\"order_id\":\"ord_789\",\"amount\":199.99,\"currency\":\"USD\"}'\n</code></pre> <p>Version Control Notes: - <code>--schema-subject</code> alone uses the latest version - <code>--schema-version</code> pins to a specific version - <code>--schema-min-version</code> uses the specified version or newer - Cannot use both <code>--schema-version</code> and <code>--schema-min-version</code> together</p>"},{"location":"danube_clis/danube_cli/producer/#auto-register-schema","title":"Auto-Register Schema","text":"<pre><code># Schema will be registered automatically if it doesn't exist\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  --schema-file event-schema.json \\\n  --schema-type json_schema \\\n  -m '{\"event\":\"user_signup\",\"user_id\":\"user_456\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#schema-types","title":"Schema Types","text":"<p>JSON Schema:</p> <pre><code>danube-cli produce \\\n  --schema-subject my-schema \\\n  --schema-type json_schema \\\n  -m '{\"key\":\"value\"}'\n</code></pre> <p>Avro:</p> <pre><code>danube-cli produce \\\n  --schema-subject my-avro-schema \\\n  --schema-type avro \\\n  --file message.avro\n</code></pre> <p>Protobuf:</p> <pre><code>danube-cli produce \\\n  --schema-subject my-proto-schema \\\n  --schema-type protobuf \\\n  --file message.pb\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#partitioned-topics","title":"Partitioned Topics","text":""},{"location":"danube_clis/danube_cli/producer/#create-partitioned-topic","title":"Create Partitioned Topic","text":"<pre><code># Specify number of partitions\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/events \\\n  --partitions 8 \\\n  -m \"Partitioned message\"\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#high-throughput-example","title":"High Throughput Example","text":"<pre><code># 16 partitions for parallel processing\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/high-volume \\\n  --partitions 16 \\\n  --schema-subject events \\\n  -m '{\"event\":\"data\"}' \\\n  --count 10000 \\\n  --interval 100\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#reliable-delivery","title":"Reliable Delivery","text":""},{"location":"danube_clis/danube_cli/producer/#enable-reliable-delivery","title":"Enable Reliable Delivery","text":"<pre><code># Messages are persisted to disk before acknowledgment\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -m \"Critical message\" \\\n  --reliable\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#reliable-partitioned","title":"Reliable + Partitioned","text":"<pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/transactions \\\n  --partitions 4 \\\n  --reliable \\\n  --schema-subject transactions \\\n  -m '{\"tx_id\":\"tx_789\",\"amount\":1000.00}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#practical-examples","title":"Practical Examples","text":""},{"location":"danube_clis/danube_cli/producer/#e-commerce-orders","title":"E-Commerce Orders","text":"<pre><code># Register order schema\ndanube-cli schema register orders \\\n  --schema-type json_schema \\\n  --file order-schema.json\n\n# Send order\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /default/orders \\\n  --schema-subject orders \\\n  --reliable \\\n  -m '{\n    \"order_id\":\"ord_123\",\n    \"customer_id\":\"cust_456\",\n    \"items\":[{\"sku\":\"ITEM1\",\"qty\":2,\"price\":29.99}],\n    \"total\":59.98\n  }'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#event-streaming","title":"Event Streaming","text":"<pre><code># High-volume event stream\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /analytics/events \\\n  --partitions 16 \\\n  --schema-subject user-events \\\n  -m '{\"event\":\"page_view\",\"user_id\":\"user_789\",\"page\":\"/home\"}' \\\n  --count 100000 \\\n  --interval 100\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#iot-sensor-data","title":"IoT Sensor Data","text":"<pre><code># Send sensor readings\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /iot/sensors \\\n  --attributes \"sensor_id:temp_001,location:warehouse-a\" \\\n  -m '{\"temperature\":22.5,\"humidity\":45,\"timestamp\":\"2024-01-15T10:30:00Z\"}' \\\n  --count 1440 \\\n  --interval 60000  # Every minute\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#log-aggregation","title":"Log Aggregation","text":"<pre><code># Send application logs\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /logs/application \\\n  --attributes \"app:api-server,env:production,level:error\" \\\n  -m '{\"timestamp\":\"2024-01-15T10:30:00Z\",\"message\":\"Database connection failed\",\"stack_trace\":\"...\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/producer/#command-reference","title":"Command Reference","text":""},{"location":"danube_clis/danube_cli/producer/#all-producer-flags","title":"All Producer Flags","text":"Flag Short Description Default <code>--service-addr</code> <code>-s</code> Broker URL <code>http://127.0.0.1:6650</code> <code>--topic</code> <code>-t</code> Topic name <code>/default/test_topic</code> <code>--message</code> <code>-m</code> Message content Required* <code>--file</code> <code>-f</code> Binary file path - <code>--producer-name</code> <code>-n</code> Producer name <code>test_producer</code> <code>--schema-subject</code> - Schema subject (latest version) - <code>--schema-version</code> - Pin to specific schema version - <code>--schema-min-version</code> - Use minimum version or newer - <code>--schema-file</code> - Schema file (auto-register) - <code>--schema-type</code> - Schema type - <code>--count</code> <code>-c</code> Number of messages <code>1</code> <code>--interval</code> <code>-i</code> Interval in ms <code>500</code> <code>--partitions</code> <code>-p</code> Number of partitions - <code>--attributes</code> <code>-a</code> Message attributes - <code>--reliable</code> - Reliable delivery <code>false</code> <p>*Required unless <code>--file</code> is provided</p>"},{"location":"danube_clis/danube_cli/schema_registry/","title":"Schema Registry Guide","text":"<p>Master schema management for reliable, validated messaging! \ud83d\udccb</p>"},{"location":"danube_clis/danube_cli/schema_registry/#table-of-contents","title":"Table of Contents","text":"<ul> <li>What is a Schema Registry?</li> <li>Schema Management</li> <li>Schema Types</li> <li>Schema Evolution</li> <li>Compatibility Modes</li> <li>Complete Workflows</li> </ul>"},{"location":"danube_clis/danube_cli/schema_registry/#what-is-a-schema-registry","title":"What is a Schema Registry?","text":"<p>The Schema Registry is a centralized repository that stores and manages schemas for your messages.</p>"},{"location":"danube_clis/danube_cli/schema_registry/#why-use-schemas","title":"Why Use Schemas?","text":"<p>Without Schemas:</p> <pre><code># Producer sends anything\ndanube-cli produce -s http://localhost:6650 -m '{\"user\":123}'  # number\ndanube-cli produce -s http://localhost:6650 -m '{\"user\":\"abc\"}' # string\ndanube-cli produce -s http://localhost:6650 -m '{\"usr\":\"xyz\"}'  # typo!\n\n# Consumer has no idea what to expect! \u274c\n</code></pre> <p>With Schemas:</p> <pre><code># Schema defines the contract\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\", \"email\"]\n}\n\n# Only valid messages are accepted \u2705\n# Consumers know exactly what to expect \u2705\n# Breaking changes are prevented \u2705\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#benefits","title":"Benefits","text":"Benefit Description Type Safety Prevent invalid data at the source Documentation Schema serves as living documentation Evolution Safe schema updates with compatibility checking Validation Automatic validation for producers and consumers Versioning Track schema changes over time"},{"location":"danube_clis/danube_cli/schema_registry/#schema-management","title":"Schema Management","text":""},{"location":"danube_clis/danube_cli/schema_registry/#register-a-schema","title":"Register a Schema","text":"<pre><code>danube-cli schema register &lt;subject&gt; \\\n  --schema-type &lt;schema-type&gt; \\\n  --file &lt;schema-file&gt;\n</code></pre> <p>Example:</p> <pre><code># Create a JSON schema file\ncat &gt; user-schema.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\", \"format\": \"email\"},\n    \"age\": {\"type\": \"integer\", \"minimum\": 0}\n  },\n  \"required\": [\"user_id\", \"email\"]\n}\nEOF\n\n# Register the schema\ndanube-cli schema register user-events \\\n  --schema-type json_schema \\\n  --file user-schema.json\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udce4 Registering schema 'user-events' (type: JsonSchema)...\n\u2705 Schema registered successfully!\n   Subject: user-events\n   Schema ID: 1\n   Version: 1\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#get-schema-details","title":"Get Schema Details","text":"<pre><code>danube-cli schema get &lt;subject&gt;\n</code></pre> <p>Example:</p> <pre><code>danube-cli schema get user-events\n</code></pre> <p>Output:</p> <pre><code>\u2705 Schema Details\n==================================================\nSubject:       user-events\nVersion:       1\nSchema ID:     1\nType:          json_schema\n==================================================\nSchema Definition:\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\", \"format\": \"email\"},\n    \"age\": {\"type\": \"integer\", \"minimum\": 0}\n  },\n  \"required\": [\"user_id\", \"email\"]\n}\n==================================================\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#list-schema-versions","title":"List Schema Versions","text":"<pre><code>danube-cli schema versions &lt;subject&gt;\n</code></pre> <p>Example:</p> <pre><code>danube-cli schema versions user-events\n</code></pre> <p>Output:</p> <pre><code>\u2705 Schema Versions for 'user-events'\n==================================================\nVersion 1 (ID: 1) - Current\nVersion 2 (ID: 2)\nVersion 3 (ID: 3) - Latest\n==================================================\nTotal versions: 3\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#check-compatibility","title":"Check Compatibility","text":"<p>Before registering a new version, check compatibility:</p> <pre><code>danube-cli schema check &lt;subject&gt; \\\n  --schema-type &lt;schema-type&gt; \\\n  --file &lt;new-schema-file&gt;\n</code></pre> <p>Example:</p> <pre><code># Create updated schema (v2)\ncat &gt; user-schema-v2.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\", \"format\": \"email\"},\n    \"age\": {\"type\": \"integer\", \"minimum\": 0},\n    \"name\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\", \"email\"]\n}\nEOF\n\n# Check compatibility\ndanube-cli schema check user-events \\\n  --schema-type json_schema \\\n  --file user-schema-v2.json\n</code></pre> <p>Output (Compatible):</p> <pre><code>\u2705 Schema is compatible!\n   Subject: user-events\n   Compatibility Mode: backward\n\nSchema can be safely registered.\n</code></pre> <p>Output (Incompatible):</p> <pre><code>\u274c Schema is NOT compatible!\n   Subject: user-events\n   Compatibility Mode: backward\n\nCompatibility errors:\n- Required field 'name' added (breaks backward compatibility)\n\nCannot register this schema version.\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#schema-types","title":"Schema Types","text":""},{"location":"danube_clis/danube_cli/schema_registry/#json-schema","title":"JSON Schema","text":"<p>Most common for JSON messages.</p> <p>Create Schema:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"event_type\": {\"type\": \"string\"},\n    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"},\n    \"user_id\": {\"type\": \"string\"}\n  },\n  \"required\": [\"event_type\", \"timestamp\"]\n}\n</code></pre> <p>Register:</p> <pre><code>danube-cli schema register events \\\n  --schema-type json_schema \\\n  --file events-schema.json\n</code></pre> <p>Use:</p> <pre><code># Produce with validation\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  --schema-subject events \\\n  -m '{\"event_type\":\"login\",\"timestamp\":\"2024-01-01T10:00:00Z\",\"user_id\":\"u123\"}'\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#avro-schema","title":"Avro Schema","text":"<p>For compact binary serialization.</p> <p>Create Schema:</p> <pre><code>{\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"namespace\": \"com.example\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"string\"},\n    {\"name\": \"email\", \"type\": \"string\"},\n    {\"name\": \"age\", \"type\": [\"null\", \"int\"], \"default\": null}\n  ]\n}\n</code></pre> <p>Register:</p> <pre><code>danube-cli schema register users \\\n  --schema-type avro \\\n  --file user-schema.avsc\n</code></pre> <p>Use:</p> <pre><code>danube-cli produce \\\n  -s http://localhost:6650 \\\n  --schema-subject users \\\n  -m '{\"id\":\"u123\",\"email\":\"user@example.com\",\"age\":25}'\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#protobuf-schema","title":"Protobuf Schema","text":"<p>For Google Protocol Buffers.</p> <p>Create Schema (message.proto):</p> <pre><code>syntax = \"proto3\";\n\nmessage User {\n  string id = 1;\n  string email = 2;\n  int32 age = 3;\n}\n</code></pre> <p>Register:</p> <pre><code>danube-cli schema register users \\\n  --schema-type protobuf \\\n  --file message.proto\n</code></pre> <p>Use:</p> <pre><code># Send compiled protobuf binary\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  --schema-subject users \\\n  --file compiled-message.bin\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#schema-evolution","title":"Schema Evolution","text":""},{"location":"danube_clis/danube_cli/schema_registry/#evolution-scenarios","title":"Evolution Scenarios","text":""},{"location":"danube_clis/danube_cli/schema_registry/#adding-optional-fields-safe","title":"Adding Optional Fields (Safe)","text":"<p>V1:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\"]\n}\n</code></pre> <p>V2 (Add optional field):</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\"]\n}\n</code></pre> <p>\u2705 Backward compatible - Old consumers can read new messages \u2705 Forward compatible - New consumers can read old messages</p>"},{"location":"danube_clis/danube_cli/schema_registry/#removing-optional-fields-safe","title":"Removing Optional Fields (Safe)","text":"<p>V1:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"temp_field\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\"]\n}\n</code></pre> <p>V2 (Remove optional field):</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\"]\n}\n</code></pre> <p>\u2705 Backward compatible - Old consumers still work</p>"},{"location":"danube_clis/danube_cli/schema_registry/#adding-required-fields-unsafe","title":"Adding Required Fields (Unsafe)","text":"<p>V1:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\"]\n}\n</code></pre> <p>V2 (Add required field):</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"email\": {\"type\": \"string\"}\n  },\n  \"required\": [\"user_id\", \"email\"]\n}\n</code></pre> <p>\u274c NOT backward compatible - Old producers can't provide required field</p>"},{"location":"danube_clis/danube_cli/schema_registry/#safe-evolution-workflow","title":"Safe Evolution Workflow","text":"<pre><code># Step 1: Check current schema\ndanube-cli schema get orders\n\n# Step 2: Create new schema version\ncat &gt; orders-v2.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"order_id\": {\"type\": \"string\"},\n    \"amount\": {\"type\": \"number\"},\n    \"currency\": {\"type\": \"string\", \"default\": \"USD\"}\n  },\n  \"required\": [\"order_id\", \"amount\"]\n}\nEOF\n\n# Step 3: Check compatibility\ndanube-cli schema check orders \\\n  --schema-type json_schema \\\n  --file orders-v2.json\n\n# Step 4: If compatible, register\ndanube-cli schema register orders \\\n  --schema-type json_schema \\\n  --file orders-v2.json\n\n# Step 5: Verify versions\ndanube-cli schema versions orders\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#compatibility-modes","title":"Compatibility Modes","text":"<p>Compatibility modes control how schemas can evolve.</p> <p>\u26a0\ufe0f Note: Setting compatibility mode is an admin-only operation using <code>danube-admin-cli</code>. Clients can only check compatibility, not set it.</p>"},{"location":"danube_clis/danube_cli/schema_registry/#backward-default","title":"Backward (Default)","text":"<p>New schema can read data written with old schema.</p> <p>Use when: Consumers are upgraded before producers</p> <pre><code># Check backward compatibility (client operation)\ndanube-cli schema check orders \\\n  --schema-type json_schema \\\n  --file orders-v2.json\n\n# Set compatibility mode (admin-only - use danube-admin-cli)\n# danube-admin-cli schemas set-compatibility orders --mode backward\n</code></pre> <p>Allowed changes:</p> <ul> <li>\u2705 Add optional fields</li> <li>\u2705 Remove required fields</li> </ul> <p>Forbidden changes:</p> <ul> <li>\u274c Add required fields</li> <li>\u274c Remove optional fields</li> </ul>"},{"location":"danube_clis/danube_cli/schema_registry/#forward","title":"Forward","text":"<p>Old schema can read data written with new schema.</p> <p>Use when: Producers are upgraded before consumers</p> <p>Allowed changes:</p> <ul> <li>\u2705 Remove optional fields</li> <li>\u2705 Add required fields</li> </ul> <p>Forbidden changes:</p> <ul> <li>\u274c Add optional fields</li> <li>\u274c Remove required fields</li> </ul>"},{"location":"danube_clis/danube_cli/schema_registry/#full","title":"Full","text":"<p>Both backward and forward compatible.</p> <p>Use when: Consumers and producers upgrade independently</p> <p>Allowed changes:</p> <ul> <li>\u2705 Add optional fields with defaults</li> <li>\u2705 Remove optional fields</li> </ul> <p>Forbidden changes:</p> <ul> <li>\u274c Add required fields</li> <li>\u274c Remove required fields</li> <li>\u274c Change field types</li> </ul>"},{"location":"danube_clis/danube_cli/schema_registry/#none","title":"None","text":"<p>No compatibility checking.</p> <p>Use when: Breaking changes are acceptable</p> <p>\u26a0\ufe0f Warning: Can break consumers!</p>"},{"location":"danube_clis/danube_cli/schema_registry/#complete-workflows","title":"Complete Workflows","text":""},{"location":"danube_clis/danube_cli/schema_registry/#workflow-1-new-schema-from-scratch","title":"Workflow 1: New Schema from Scratch","text":"<pre><code># Step 1: Create schema file\ncat &gt; payment-events.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"payment_id\": {\"type\": \"string\"},\n    \"amount\": {\"type\": \"number\", \"minimum\": 0},\n    \"currency\": {\"type\": \"string\"},\n    \"status\": {\"type\": \"string\", \"enum\": [\"pending\", \"completed\", \"failed\"]}\n  },\n  \"required\": [\"payment_id\", \"amount\", \"currency\", \"status\"]\n}\nEOF\n\n# Step 2: Register schema\ndanube-cli schema register payment-events \\\n  --schema-type json_schema \\\n  --file payment-events.json\n\n# Step 3: Verify registration\ndanube-cli schema get payment-events\n\n# Step 4: Start producer with schema\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  -t /production/payments \\\n  --schema-subject payment-events \\\n  -m '{\"payment_id\":\"pay_123\",\"amount\":99.99,\"currency\":\"USD\",\"status\":\"completed\"}'\n\n# Step 5: Start consumer (automatic schema fetching and validation)\ndanube-cli consume \\\n  -s http://localhost:6650 \\\n  -t /production/payments \\\n  -m payment-processor\n# Consumer automatically fetches schema using schema_id from message metadata\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#workflow-2-schema-evolution","title":"Workflow 2: Schema Evolution","text":"<pre><code># Step 1: Check current schema\ndanube-cli schema get user-events\ndanube-cli schema versions user-events\n\n# Step 2: Create new schema version\ncat &gt; user-events-v2.json &lt;&lt; 'EOF'\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_id\": {\"type\": \"string\"},\n    \"event\": {\"type\": \"string\"},\n    \"timestamp\": {\"type\": \"string\"},\n    \"metadata\": {\"type\": \"object\"}\n  },\n  \"required\": [\"user_id\", \"event\", \"timestamp\"]\n}\nEOF\n\n# Step 3: Check compatibility\ndanube-cli schema check user-events \\\n  --schema-type json_schema \\\n  --file user-events-v2.json\n\n# Step 4: Register if compatible\ndanube-cli schema register user-events \\\n  --schema-type json_schema \\\n  --file user-events-v2.json\n\n# Step 5: Verify new version\ndanube-cli schema versions user-events\n\n# Step 6: Test with new schema\ndanube-cli produce \\\n  -s http://localhost:6650 \\\n  --schema-subject user-events \\\n  -m '{\"user_id\":\"u123\",\"event\":\"login\",\"timestamp\":\"2024-01-01T10:00:00Z\",\"metadata\":{\"ip\":\"127.0.0.1\"}}'\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#troubleshooting","title":"Troubleshooting","text":""},{"location":"danube_clis/danube_cli/schema_registry/#schema-not-found","title":"Schema Not Found","text":"<pre><code># Check if schema is registered\ndanube-cli schema get my-subject\n\n# If not found, register it\ndanube-cli schema register my-subject --schema-type json_schema --file schema.json\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#validation-failures","title":"Validation Failures","text":"<pre><code># Get current schema\ndanube-cli schema get my-subject --output json\n\n# Verify your message matches the schema\n# Check required fields, types, formats\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#compatibility-issues","title":"Compatibility Issues","text":"<pre><code># Check what compatibility mode is set\ndanube-cli schema get my-subject\n\n# Check compatibility\ndanube-cli schema check my-subject \\\n  --schema-type json_schema \\\n  --file new-schema.json\n</code></pre>"},{"location":"danube_clis/danube_cli/schema_registry/#client-vs-admin-operations","title":"Client vs Admin Operations","text":""},{"location":"danube_clis/danube_cli/schema_registry/#what-clients-can-do-danube-cli","title":"What Clients Can Do (danube-cli)","text":"<p>\u2705 Register schemas - Add new schemas or versions \u2705 Get schema details - Fetch schema information \u2705 List versions - View version history \u2705 Check compatibility - Validate before registering \u2705 Choose schema version - Producers can pin to specific versions \u2705 Auto-register schemas - Register during production</p>"},{"location":"danube_clis/danube_cli/schema_registry/#what-requires-admin-danube-admin-cli","title":"What Requires Admin (danube-admin-cli)","text":"<p>\u274c Set compatibility mode - Governance control (use <code>danube-admin-cli schemas set-compatibility</code>) \u274c Configure topic schemas - Topic-level validation policies (use <code>danube-admin-cli topics configure-schema</code>) \u274c Delete schemas - Dangerous operation (use <code>danube-admin-cli schemas delete</code>)</p> <p>See Also:</p> <ul> <li>Admin Schema Registry Guide - For admin-only operations</li> <li>Admin Topics Guide - For topic schema configuration</li> </ul>"},{"location":"danube_clis/danube_cli/schema_registry/#consumer-schema-fetching","title":"Consumer Schema Fetching","text":"<p>Consumers automatically fetch and validate schemas:</p> <pre><code>danube-cli consume -s http://localhost:6650 -t /default/events -m my-sub\n</code></pre> <p>How it works:</p> <ol> <li>Consumer receives message with <code>schema_id</code> in metadata</li> <li>Automatically fetches schema from registry using <code>schema_id</code></li> <li>Caches schema for performance</li> <li>Validates JSON messages against schema (if JSON Schema type)</li> <li>Pretty-prints validated JSON messages</li> </ol> <p>Benefits:</p> <ul> <li>No manual schema configuration needed</li> <li>Always uses the exact schema the producer used</li> <li>Handles schema evolution automatically</li> <li>Efficient caching reduces registry calls</li> </ul>"},{"location":"danube_clis/danube_cli/schema_registry/#json-output-for-automation","title":"JSON Output for Automation","text":"<p>All schema commands support JSON output:</p> <pre><code># Get schema as JSON\ndanube-cli schema get user-events --output json | jq .\n\n# List versions as JSON\ndanube-cli schema versions user-events --output json | jq .\n\n# Check compatibility with JSON output\ndanube-cli schema check user-events \\\n  --schema-type json_schema \\\n  --file new-schema.json \\\n  --output json | jq .\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/","title":"Run Danube with Docker Compose","text":"<p>This guide provides instructions on how to run Danube Messaging using Docker and Docker Compose. It sets up ETCD for metadata storage and MinIO for topic persistence storage.</p>"},{"location":"getting_started/Danube_docker_compose/#architecture-overview","title":"Architecture Overview","text":"<p>The setup includes:</p> <ul> <li>2 Danube Brokers: High-availability message brokers with load balancing</li> <li>ETCD: Distributed metadata store for cluster coordination</li> <li>MinIO: S3-compatible object storage for persistent message storage</li> <li>MinIO Client (MC): Automatic bucket creation and configuration</li> </ul> <p></p>"},{"location":"getting_started/Danube_docker_compose/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine 20.10+</li> <li>Docker Compose 2.0+</li> <li>At least 4GB RAM available for containers</li> <li>Ports 2379, 2380, 6650-6651, 9000-9001, 9040-9041, 50051-50052 available</li> </ul>"},{"location":"getting_started/Danube_docker_compose/#quick-start","title":"Quick Start","text":""},{"location":"getting_started/Danube_docker_compose/#step-1-setup-choose-one-option","title":"Step 1: Setup (Choose One Option)","text":"<p>Option 1: Download Docker Compose Files (Recommended for running the broker)</p> <p>Create a directory and download the required files:</p> <pre><code>mkdir danube-docker &amp;&amp; cd danube-docker\n</code></pre> <p>Download the docker-compose file:</p> <pre><code>curl -O https://raw.githubusercontent.com/danube-messaging/danube/main/docker/docker-compose.yml\n</code></pre> <p>Download the broker configuration file:</p> <pre><code>curl -O https://raw.githubusercontent.com/danube-messaging/danube/main/docker/danube_broker.yml\n</code></pre> <p>Option 2: Clone Repository (Recommended for development and building from source)</p> <pre><code>git clone https://github.com/danube-messaging/danube.git\ncd danube/docker\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#step-2-start-the-cluster","title":"Step 2: Start the Cluster","text":"<p>Start the entire cluster:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#step-3-verify-all-services-are-healthy","title":"Step 3: Verify all services are healthy","text":"<p>Verify all services are running:</p> <pre><code>docker-compose ps\n</code></pre> <p>Expected output:</p> <pre><code>\u2717 docker-compose ps\nNAME        IMAGE        COMMAND       SERVICE         CREATED        STATUS       PORTS\n\ndanube-broker1   docker-broker1                             \"/usr/local/bin/danu\u2026\"   broker1      About a minute ago   Up 6 seconds (health: starting)   0.0.0.0:6650-&gt;6650/tcp, [::]:6650-&gt;6650/tcp, 0.0.0.0:9040-&gt;9040/tcp, [::]:9040-&gt;9040/tcp, 0.0.0.0:50051-&gt;50051/tcp, [::]:50051-&gt;50051/tcp\n\ndanube-broker2   docker-broker2                             \"/usr/local/bin/danu\u2026\"   broker2      About a minute ago   Up 6 seconds (health: starting)   0.0.0.0:6651-&gt;6650/tcp, [::]:6651-&gt;6650/tcp, 0.0.0.0:9041-&gt;9040/tcp, [::]:9041-&gt;9040/tcp, 0.0.0.0:50052-&gt;50051/tcp, [::]:50052-&gt;50051/tcp\n\ndanube-cli       docker-danube-cli                          \"sleep infinity\"         danube-cli   About a minute ago   Up 6 seconds                      \n\ndanube-etcd      quay.io/coreos/etcd:v3.5.9                 \"/usr/local/bin/etcd\"    etcd         About a minute ago   Up 12 seconds (healthy)           0.0.0.0:2379-2380-&gt;2379-2380/tcp, [::]:2379-2380-&gt;2379-2380/tcp\n\ndanube-mc        minio/mc:RELEASE.2024-09-16T17-43-14Z      \"/bin/sh -c ' echo '\u2026\"   mc           About a minute ago   Up About a minute                 \n\ndanube-minio     minio/minio:RELEASE.2025-07-23T15-54-02Z   \"/usr/bin/docker-ent\u2026\"   minio        About a minute ago   Up About a minute (healthy)       0.0.0.0:9000-9001-&gt;9000-9001/tcp, [::]:9000-9001-&gt;9000-9001/tcp\n</code></pre> <p>Check logs (optional):</p> <pre><code># View all logs\ndocker-compose logs -f\n\n# View specific service logs\ndocker-compose logs -f broker1\ndocker-compose logs -f broker2\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#service-endpoints","title":"Service Endpoints","text":"Service Endpoint Purpose Danube Broker 1 <code>localhost:6650</code> gRPC messaging Danube Broker 2 <code>localhost:6651</code> gRPC messaging Admin API 1 <code>localhost:50051</code> Broker administration Admin API 2 <code>localhost:50052</code> Broker administration Prometheus 1 <code>localhost:9040</code> Metrics and monitoring Prometheus 2 <code>localhost:9041</code> Metrics and monitoring MinIO API <code>localhost:9000</code> S3-compatible storage MinIO Console <code>localhost:9001</code> Web UI (minioadmin/minioadmin123) ETCD <code>localhost:2379</code> Metadata store"},{"location":"getting_started/Danube_docker_compose/#testing-with-danube-cli","title":"Testing with Danube CLI","text":""},{"location":"getting_started/Danube_docker_compose/#using-the-cli-container","title":"Using the CLI Container","text":"<p>The Docker Compose setup includes a <code>danube-cli</code> container with both <code>danube-cli</code> and <code>danube-admin-cli</code> tools pre-installed. This eliminates the need to build or install Rust locally.</p> <p>No local installation required - use the containerized CLI tools directly.</p>"},{"location":"getting_started/Danube_docker_compose/#reliable-messaging-with-s3-storage","title":"Reliable Messaging with S3 Storage","text":"<p>Test the cloud-ready persistent storage capabilities:</p> <p>Produce with reliable delivery and S3 persistence:</p> <pre><code>docker exec -it danube-cli danube-cli produce \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/persistent-topic\" \\\n  --count 1000 \\\n  --message \"Persistent message\" \\\n  --reliable\n</code></pre> <p>Consume persistent messages:</p> <pre><code>docker exec -it danube-cli danube-cli consume \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/persistent-topic\" \\\n  --subscription \"persistent-sub\" \\\n  --sub-type exclusive\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#non-reliable-message-flow-testing","title":"Non-Reliable Message Flow Testing","text":""},{"location":"getting_started/Danube_docker_compose/#basic-string-messages","title":"Basic string messages","text":"<p>Produce basic string messages:</p> <pre><code>docker exec -it danube-cli danube-cli produce \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/test-topic\" \\\n  --count 100 \\\n  --message \"Hello from Danube Docker!\"\n</code></pre> <p>Consume from shared subscription:</p> <pre><code>docker exec -it danube-cli danube-cli consume \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/test-topic\" \\\n  --subscription \"shared-sub\" \\\n  --consumer \"docker-consumer\"\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#json-schema-messages","title":"JSON schema messages","text":"<p>Produce JSON messages with schema:</p> <pre><code>docker exec -it danube-cli danube-cli produce \\\n  --service-addr http://broker1:6650 \\\n  --topic \"/default/json-topic\" \\\n  --count 100 \\\n  --schema json \\\n  --json-schema '{\"type\":\"object\",\"properties\":{\"message\":{\"type\":\"string\"},\"timestamp\":{\"type\":\"number\"}}}' \\\n  --message '{\"message\":\"Hello JSON\",\"timestamp\":1640995200}'\n</code></pre> <p>Consume JSON messages:</p> <pre><code>docker exec -it danube-cli danube-cli consume \\\n  --service-addr http://broker2:6650 \\\n  --topic \"/default/json-topic\" \\\n  --subscription \"json-sub\" \\\n  --consumer \"json-consumer\"\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#admin-cli-operations","title":"Admin CLI Operations","text":"<p>Use danube-admin-cli for cluster management:</p> <pre><code># List active brokers\ndocker exec -it danube-cli danube-admin-cli brokers list\n\n# List namespaces in cluster\ndocker exec -it danube-cli danube-admin-cli brokers namespaces\n\n# List topics in a namespace\ndocker exec -it danube-cli danube-admin-cli topics list default\n\n# List subscriptions on a topic\ndocker exec -it danube-cli danube-admin-cli topics subscriptions /default/test-topic\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"getting_started/Danube_docker_compose/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>Access broker metrics for monitoring:</p> <pre><code># Broker 1 metrics\ncurl http://localhost:9040/metrics\n\n# Broker 2 metrics  \ncurl http://localhost:9041/metrics\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#minio-console","title":"MinIO Console","text":"<ol> <li>Open http://localhost:9001 in your browser</li> <li>Login with credentials: <code>minioadmin</code> / <code>minioadmin123</code></li> <li>Navigate to \"Buckets\" to see:</li> <li><code>danube-messages</code>: Persistent message storage</li> <li><code>danube-wal</code>: Write-ahead log storage</li> </ol>"},{"location":"getting_started/Danube_docker_compose/#etcd-inspection","title":"ETCD Inspection","text":"<pre><code># List all keys in ETCD\ndocker exec danube-etcd etcdctl --endpoints=http://127.0.0.1:2379 get --prefix \"\"\n\n# Watch for changes\ndocker exec danube-etcd etcdctl --endpoints=http://127.0.0.1:2379 watch --prefix \"\"\n\n# Check broker registrations\ndocker exec danube-etcd etcdctl --endpoints=http://127.0.0.1:2379 get --prefix \"/cluster/register\"\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#configuration","title":"Configuration","text":""},{"location":"getting_started/Danube_docker_compose/#broker-configuration","title":"Broker Configuration","text":"<p>The <code>danube_broker.yml</code> file is optimized for:</p> <ul> <li>S3 Storage: MinIO integration with automatic credential management</li> <li>High Performance: Optimized WAL rotation and batch sizes</li> <li>Development: Relaxed security and unlimited resource policies</li> <li>Monitoring: Prometheus metrics enabled on all brokers</li> </ul>"},{"location":"getting_started/Danube_docker_compose/#environment-variables","title":"Environment Variables","text":"<p>Key environment variables used:</p> <ul> <li><code>AWS_ACCESS_KEY_ID=minioadmin</code></li> <li><code>AWS_SECRET_ACCESS_KEY=minioadmin123</code></li> <li><code>AWS_REGION=us-east-1</code></li> <li><code>RUST_LOG=danube_broker=info,danube_core=info</code></li> </ul>"},{"location":"getting_started/Danube_docker_compose/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting_started/Danube_docker_compose/#common-issues","title":"Common Issues","text":"<ol> <li>Port conflicts: Ensure all required ports are available</li> <li>Memory issues: Increase Docker memory allocation if containers fail to start</li> <li>Storage issues: Check MinIO bucket creation in logs: <code>docker-compose logs mc</code></li> </ol>"},{"location":"getting_started/Danube_docker_compose/#reset-environment","title":"Reset Environment","text":"<pre><code># Stop and remove all containers, networks, and volumes\ndocker-compose down -v\n\n# Remove all Danube-related Docker resources\ndocker volume prune -f\ndocker network prune -f\n\n# Restart fresh\ndocker-compose up -d\n</code></pre>"},{"location":"getting_started/Danube_docker_compose/#production-considerations","title":"Production Considerations","text":"<p>This setup demonstrates Danube's cloud-ready capabilities. For production deployment:</p> <ol> <li>Replace MinIO with AWS S3, Google Cloud Storage, or Azure Blob Storage</li> <li>Enable TLS/SSL authentication in broker configuration</li> <li>Configure resource limits and health checks appropriately</li> <li>Set up monitoring with Prometheus and Grafana</li> <li>Implement backup strategies for ETCD and persistent storage</li> <li>Use container orchestration like Kubernetes for scaling</li> </ol>"},{"location":"getting_started/Danube_docker_compose/#aws-s3-migration","title":"AWS S3 Migration","text":"<p>To migrate from MinIO to AWS S3, update <code>danube_broker.yml</code>:</p> <pre><code>wal_cloud:\n  cloud:\n    backend: \"s3\"\n    root: \"s3://your-production-bucket/danube-cluster\"\n    region: \"us-west-2\"\n    # Remove endpoint for AWS S3\n    # endpoint: \"http://minio:9000\"  \n    # Use IAM roles or environment variables for credentials\n</code></pre> <p>This Docker Compose setup showcases Danube's architecture with cloud-native storage.</p>"},{"location":"getting_started/Danube_kubernetes/","title":"Run Danube messaging on Kubernetes","text":"<p>This documentation covers the instalation of the Danube cluster on the kubernetes. The Helm chart deploys the Danube Cluster with ETCD as metadata storage in the same namespace.</p> <p>The documentation assumes that you have a Kubernetes cluster running and that you have installed the Helm package manager. For local testing you can use kind.</p>"},{"location":"getting_started/Danube_kubernetes/#install-the-ngnix-ingress-controller","title":"Install the Ngnix Ingress controller","text":"<p>Using the Official NGINX Ingress Helm Chart. This is required in order to route traffic to each broker service in the cluster. The Broker configuration is provisioned in the danube_helm, you can tweak the values.yaml per your needs.</p> <p>The Danube messaging has no dependency on ngnix, can work with any ingress controller of your choice.</p> <p>Install the NGINX Ingress Controller using Helm:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#run-the-nginx-ingress-controller-on-your-local-kubernetes-cluster","title":"Run the NGINX Ingress Controller on your local kubernetes cluster","text":"<p>For local testing the NGINX Ingress Controller can be exposed using a NodePort service so that the traffic from the local machine (outside the cluster) can reach the Ingress controller.</p> <pre><code>helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.service.type=NodePort\n</code></pre> <p>You can find out which port is assigned by running</p> <pre><code>kubectl get svc\n\nNAME                                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nkubernetes                                         ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                      4m17s\nnginx-ingress-ingress-nginx-controller             NodePort    10.96.245.118   &lt;none&gt;        80:30115/TCP,443:30294/TCP   2m58s\nnginx-ingress-ingress-nginx-controller-admission   ClusterIP   10.96.169.82    &lt;none&gt;        443/TCP                      2m58s\n</code></pre> <p>If ngnix is running as NodePort (usually for testing), you need local port in this case 30115, in order to provide to danube_helm installation.</p>"},{"location":"getting_started/Danube_kubernetes/#run-the-nginx-ingress-controller-in-a-remote-cluster-cloud","title":"Run the NGINX Ingress Controller in a remote cluster (cloud)","text":"<pre><code>helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.publishService.enabled=true\n</code></pre> <ul> <li>The publishService feature enables the Ingress controller to publish information about itself (such as its external IP or hostname) in a Kubernetes Service resource.</li> <li>This is particularly useful when you are running the Ingress controller in a cloud environment (like AWS, GCP, or Azure) and need it to publish its external IP address to handle incoming traffic</li> </ul>"},{"location":"getting_started/Danube_kubernetes/#install-danube-messaging-brokers","title":"Install Danube Messaging Brokers","text":"<p>First, add the repository to your Helm client:</p> <pre><code>helm repo add danube https://danube-messaging.github.io/danube_helm\nhelm repo update\n</code></pre> <p>You can install the chart with the release name <code>my-danube-cluster</code> using the below command. This will deploy the Danube Broker and an ETCD instance with the default configuration.</p>"},{"location":"getting_started/Danube_kubernetes/#running-on-the-local-kubernetes-cluster","title":"Running on the local kubernetes cluster","text":"<pre><code>helm install my-danube-cluster danube/danube-helm-chart --set broker.service.advertisedPort=30115\n</code></pre> <p>The advertisedPort is used to allow the client to reach the brokers, through the ingress NodePort.</p>"},{"location":"getting_started/Danube_kubernetes/#running-on-the-remote-cluster-cloud","title":"Running on the remote cluster (cloud)","text":"<p>The Danube cluster configuration from the values.yaml file has to be adjusted for your needs.</p> <p>You can override the default values by providing a custom <code>values.yaml</code> file:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart -f custom-values.yaml\n</code></pre> <p>Alternatively, you can specify individual values using the <code>--set</code> flag:</p> <pre><code>helm install my-danube-cluster danube/danube-helm-chart --set broker.service.type=\"ClusterIP\"\n</code></pre> <p>You can further customize the installation, check the readme file. The default configuration is running 3 Danube Brokers in cluster.</p>"},{"location":"getting_started/Danube_kubernetes/#check-the-install","title":"Check the install","text":"<p>Make sure that the brokers, etcd and the ngnix ingress are running properly in the cluster.</p>"},{"location":"getting_started/Danube_kubernetes/#example-running-on-local-kubernetes-cluster","title":"Example running on local kubernetes cluster","text":"<pre><code>kubectl get all\n\nNAME                                                          READY   STATUS    RESTARTS   AGE\npod/my-danube-cluster-danube-broker1-766665d6f4-qdbf6         1/1     Running   0          12s\npod/my-danube-cluster-danube-broker2-5774ff4dd6-dvx66         1/1     Running   0          12s\npod/my-danube-cluster-danube-broker3-6db6b5fccd-dkr2k         1/1     Running   0          12s\npod/my-danube-cluster-etcd-867f5b85f8-g4m9m                   1/1     Running   0          12s\npod/nginx-ingress-ingress-nginx-controller-7bc7c7776d-wqc5g   1/1     Running   0          47m\n\nNAME                                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                       AGE\nservice/kubernetes                                         ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                       48m\nservice/my-danube-cluster-danube-broker1                   ClusterIP   10.96.40.244    &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-danube-broker2                   ClusterIP   10.96.204.21    &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-danube-broker3                   ClusterIP   10.96.46.5      &lt;none&gt;        6650/TCP,50051/TCP,9040/TCP   12s\nservice/my-danube-cluster-etcd                             ClusterIP   10.96.232.70    &lt;none&gt;        2379/TCP                      12s\nservice/nginx-ingress-ingress-nginx-controller             NodePort    10.96.245.118   &lt;none&gt;        80:30115/TCP,443:30294/TCP    47m\nservice/nginx-ingress-ingress-nginx-controller-admission   ClusterIP   10.96.169.82    &lt;none&gt;        443/TCP                       47m\n\nNAME                                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/my-danube-cluster-danube-broker1         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-danube-broker2         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-danube-broker3         1/1     1            1           12s\ndeployment.apps/my-danube-cluster-etcd                   1/1     1            1           12s\ndeployment.apps/nginx-ingress-ingress-nginx-controller   1/1     1            1           47m\n\nNAME                                                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/my-danube-cluster-danube-broker1-766665d6f4         1         1         1       12s\nreplicaset.apps/my-danube-cluster-danube-broker2-5774ff4dd6         1         1         1       12s\nreplicaset.apps/my-danube-cluster-danube-broker3-6db6b5fccd         1         1         1       12s\nreplicaset.apps/my-danube-cluster-etcd-867f5b85f8                   1         1         1       12s\nreplicaset.apps/nginx-ingress-ingress-nginx-controller-7bc7c7776d   1         1         1       47m\n</code></pre> <p>Validate that the brokers have started correctly:</p> <pre><code>kubectl logs pod/my-danube-cluster-danube-broker1-766665d6f4-qdbf6\n\ninitializing metrics exporter\n2024-08-28T04:30:22.969462Z  INFO danube_broker: Use ETCD storage as metadata persistent store\n2024-08-28T04:30:22.969598Z  INFO danube_broker: Start the Danube Service\n2024-08-28T04:30:22.969612Z  INFO danube_broker::danube_service: Setting up the cluster MY_CLUSTER\n2024-08-28T04:30:22.971978Z  INFO danube_broker::danube_service::local_cache: Initial cache populated\n2024-08-28T04:30:22.972013Z  INFO danube_broker::danube_service: Started the Local Cache service.\n2024-08-28T04:30:22.990763Z  INFO danube_broker::danube_service::broker_register: Broker 14150019297734190044 registered in the cluster\n2024-08-28T04:30:22.991620Z  INFO danube_broker::danube_service: Namespace default already exists.\n2024-08-28T04:30:22.991926Z  INFO danube_broker::danube_service: Namespace system already exists.\n2024-08-28T04:30:22.992480Z  INFO danube_broker::danube_service: Namespace default already exists.\n2024-08-28T04:30:22.992490Z  INFO danube_broker::danube_service: cluster metadata setup completed\n2024-08-28T04:30:22.992551Z  INFO danube_broker::danube_service:  Started the Broker GRPC server\n2024-08-28T04:30:22.992563Z  INFO danube_broker::broker_server: Server is listening on address: 0.0.0.0:6650\n2024-08-28T04:30:22.992605Z  INFO danube_broker::danube_service: Started the Leader Election service\n2024-08-28T04:30:22.993050Z  INFO danube_broker::danube_service: Started the Load Manager service.\n2024-08-28T04:30:22.993143Z  INFO danube_broker::danube_service:  Started the Danube Admin GRPC server\n2024-08-28T04:30:22.993274Z  INFO danube_broker::admin: Admin is listening on address: 0.0.0.0:50051\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#setup-in-order-to-communicate-with-cluster-danube-brokers","title":"Setup in order to communicate with cluster danube brokers","text":"<p>If you would like to communicate to the messaging sytem by using the danube-cli tool, or your own danube clients running locally, you can do the following:</p> <pre><code>kubectl get nodes -o wide\nNAME                 STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION       CONTAINER-RUNTIME\nkind-control-plane   Ready    control-plane   53m   v1.30.0   172.20.0.2    &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   5.15.0-118-generic   containerd://1.7.15\n</code></pre> <p>Use the INTERNAL-IP to route the traffic to broker hosts. Add the following in the hosts file, but make sure you match the number and the name of the brokers from the helm values.yaml file.</p> <pre><code>cat /etc/hosts\n172.20.0.2 broker1.example.com broker2.example.com broker3.example.com\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#inspect-the-etcd-instance","title":"Inspect the etcd instance","text":"<p>If you want to connect from your local machine, use kubectl port-forward to forward the etcd port to your local machine:</p> <p>Port Forward etcd Service:</p> <pre><code>kubectl port-forward service/my-danube-cluster-etcd 2379:2379\n</code></pre> <p>Once port forwarding is set up, you can run etcdctl commands from your local machine:</p> <pre><code>etcdctl --endpoints=http://localhost:2379 watch --prefix /\n</code></pre>"},{"location":"getting_started/Danube_kubernetes/#cleanup","title":"Cleanup","text":"<p>To uninstall the <code>my-danube-cluster</code> release:</p> <pre><code>helm uninstall my-danube-cluster\n</code></pre> <p>This command removes all the Kubernetes components associated with the chart and deletes the release.</p>"},{"location":"getting_started/Danube_local/","title":"Run Danube Broker on your local machine","text":""},{"location":"getting_started/Danube_local/#start-metadata-storage-etcd","title":"Start Metadata Storage (ETCD)","text":"<p>Danube uses ETCD for metadata storage to provide high availability and scalability. Run ETCD using Docker:</p> <pre><code>docker run -d --name etcd-danube -p 2379:2379 quay.io/coreos/etcd:latest etcd --advertise-client-urls http://0.0.0.0:2379 --listen-client-urls http://0.0.0.0:2379\n</code></pre> <p>Verify ETCD is running:</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS          PORTS                                                 NAMES\n27792bce6077   quay.io/coreos/etcd:latest   \"etcd --advertise-cl\u2026\"   35 seconds ago   Up 34 seconds   0.0.0.0:2379-&gt;2379/tcp, :::2379-&gt;2379/tcp, 2380/tcp   etcd-danube\n</code></pre>"},{"location":"getting_started/Danube_local/#configure-and-run-danube-broker","title":"Configure and Run Danube Broker","text":""},{"location":"getting_started/Danube_local/#create-and-configure-broker-config","title":"Create and configure broker config","text":"<p>Create a local config file, use the sample config file as a reference.</p> <pre><code>touch danube_broker.yml\n</code></pre>"},{"location":"getting_started/Danube_local/#download-and-run-the-danube-broker","title":"Download and run the Danube Broker","text":"<p>Download the latest binary from the releases page.</p> <p>If you would like to run Danube Brokers in cluster, you need to upload the binary to each machine and use the same cluster configuration name.</p> <p>Run the Danube Broker:</p> <pre><code>touch broker.log\n</code></pre> <pre><code>RUST_LOG=info ./danube-broker-linux --config-file danube_broker.yml --broker-addr \"0.0.0.0:6650\" --admin-addr \"0.0.0.0:50051\" &gt; broker.log 2&gt;&amp;1 &amp;\n</code></pre> <p>Check the logs:</p> <pre><code>tail -n 100 -f broker.log\n</code></pre> <pre><code>2025-01-12T06:15:53.705416Z  INFO danube_broker: Use ETCD storage as metadata persistent store\n2025-01-12T06:15:53.705665Z  INFO danube_broker: Start the Danube Service\n2025-01-12T06:15:53.705679Z  INFO danube_broker::danube_service: Setting up the cluster MY_CLUSTER\n2025-01-12T06:15:53.707988Z  INFO danube_broker::danube_service::local_cache: Initial cache populated\n2025-01-12T06:15:53.709521Z  INFO danube_broker::danube_service: Started the Local Cache service.\n2025-01-12T06:15:53.713329Z  INFO danube_broker::danube_service::broker_register: Broker 15139934490483381581 registered in the cluster\n2025-01-12T06:15:53.714977Z  INFO danube_broker::danube_service: Namespace default already exists.\n2025-01-12T06:15:53.716405Z  INFO danube_broker::danube_service: Namespace system already exists.\n2025-01-12T06:15:53.717979Z  INFO danube_broker::danube_service: Namespace default already exists.\n2025-01-12T06:15:53.718012Z  INFO danube_broker::danube_service: cluster metadata setup completed\n2025-01-12T06:15:53.718092Z  INFO danube_broker::danube_service:  Started the Broker GRPC server\n2025-01-12T06:15:53.718116Z  INFO danube_broker::broker_server: Server is listening on address: 0.0.0.0:6650\n2025-01-12T06:15:53.718191Z  INFO danube_broker::danube_service: Started the Leader Election service\n2025-01-12T06:15:53.722454Z  INFO danube_broker::danube_service: Started the Load Manager service.\n2025-01-12T06:15:53.724727Z  INFO danube_broker::danube_service:  Started the Danube Admin GRPC server\n2025-01-12T06:15:53.724727Z  INFO danube_broker::admin: Admin is listening on address: 0.0.0.0:50051\n</code></pre>"},{"location":"getting_started/Danube_local/#use-danube-cli-to-publish-and-consume-messages","title":"Use Danube CLI to Publish and Consume Messages","text":"<p>Download the latest Danube CLI binary from the releases page and run it:</p> <pre><code>./danube-cli-linux produce -s http://127.0.0.1:6650 -t /default/demo_topic -c 1000 -m \"Hello, Danube!\"\n</code></pre> <pre><code>Message sent successfully with ID: 9\nMessage sent successfully with ID: 10\nMessage sent successfully with ID: 11\nMessage sent successfully with ID: 12\n</code></pre> <p>Open a new terminal and run the below command to consume the messages:</p> <pre><code>./danube-cli-linux consume -s http://127.0.0.1:6650 -t /default/demo_topic -m my_subscription\n</code></pre> <pre><code>Received bytes message: 9, with payload: Hello, Danube!\nReceived bytes message: 10, with payload: Hello, Danube!\nReceived bytes message: 11, with payload: Hello, Danube!\nReceived bytes message: 12, with payload: Hello, Danube!\n</code></pre>"},{"location":"getting_started/Danube_local/#validate","title":"Validate","text":"<p>Ensure ETCD is running and accessible. You can check its status by accessing <code>http://&lt;ETCD_SERVER_IP&gt;:2379</code> from a browser or using <code>curl</code>:</p> <pre><code>curl http://&lt;ETCD_SERVER_IP&gt;:2379/v3/version\n</code></pre> <p>Ensure each broker instance is running and listening on the specified port. You can check this with <code>netstat</code> or <code>ss</code>:</p> <pre><code>netstat -tuln | grep 6650\n</code></pre> <p>For debugging, check the logs of each Danube broker instance.</p>"},{"location":"getting_started/Danube_local/#cleanup","title":"Cleanup","text":"<p>Stop the Danube Broker:</p> <pre><code>pkill danube-broker\n</code></pre> <p>Stop and remove ETCD container</p> <pre><code>docker stop etcd-danube\n</code></pre> <pre><code>docker rm -f etcd-danube\n</code></pre> <p>Verify cleanup</p> <pre><code>ps aux | grep danube-broker\ndocker ps | grep etcd-danube\n</code></pre>"},{"location":"integrations/danube_connect_architecture/","title":"Danube Connect Architecture","text":"<p>High-level design and core concepts</p>"},{"location":"integrations/danube_connect_architecture/#design-philosophy","title":"Design Philosophy","text":""},{"location":"integrations/danube_connect_architecture/#isolation-safety","title":"Isolation &amp; Safety","text":"<p>The Danube broker remains pure Rust with zero knowledge of external systems. Connectors run as separate processes, ensuring that a failing connector never crashes the broker.</p> <p>Key principle: Process-level boundaries guarantee fault isolation, a misconfigured PostgreSQL connector cannot impact message delivery for other topics or tenants.</p>"},{"location":"integrations/danube_connect_architecture/#shared-core-library","title":"Shared Core Library","text":"<p>The <code>danube-connect-core</code> SDK eliminates boilerplate by handling all Danube communication, lifecycle management, retries, and observability.</p> <p>Connector developers implement simple traits:</p> <ul> <li><code>SourceConnector</code> - Import data into Danube</li> <li><code>SinkConnector</code> - Export data from Danube</li> </ul> <p>Core SDK handles:</p> <ul> <li>Danube client connection management</li> <li>Message processing loops</li> <li>Automatic retry with exponential backoff</li> <li>Prometheus metrics and health checks</li> <li>Signal handling and graceful shutdown</li> </ul>"},{"location":"integrations/danube_connect_architecture/#architecture-layers","title":"Architecture Layers","text":"<pre><code>External Systems (MQTT, databases, APIs)\n          \u2193\nYour Connector Logic (implement trait)\n          \u2193\ndanube-connect-core (SDK framework)\n          \u2193\ndanube-client (gRPC communication)\n          \u2193\nDanube Broker Cluster\n</code></pre> <p>Separation of concerns:</p> <ul> <li>Connector responsibility: External system integration and data transformation</li> <li>SDK responsibility: Danube communication, lifecycle, retries, metrics</li> </ul>"},{"location":"integrations/danube_connect_architecture/#connector-types","title":"Connector Types","text":""},{"location":"integrations/danube_connect_architecture/#source-connectors-external-danube","title":"Source Connectors (External \u2192 Danube)","text":"<p>Poll or subscribe to external systems and publish messages to Danube topics.</p> <p>Key responsibilities:</p> <ul> <li>Connect to external system</li> <li>Poll for new data or listen for events</li> <li>Transform external format to Danube messages</li> <li>Route to appropriate Danube topics</li> <li>Manage offsets/checkpoints</li> </ul>"},{"location":"integrations/danube_connect_architecture/#sink-connectors-danube-external","title":"Sink Connectors (Danube \u2192 External)","text":"<p>Consume messages from Danube topics and write to external systems.</p> <p>Key responsibilities:</p> <ul> <li>Subscribe to Danube topics</li> <li>Transform Danube messages to external format</li> <li>Write to external system (with batching for efficiency)</li> <li>Handle acknowledgments</li> </ul>"},{"location":"integrations/danube_connect_architecture/#the-danube-connect-core-sdk","title":"The danube-connect-core SDK","text":""},{"location":"integrations/danube_connect_architecture/#core-traits","title":"Core Traits","text":"<p>Connectors implements the danube-connect-core traits:</p>"},{"location":"integrations/danube_connect_architecture/#sourceconnector","title":"SourceConnector","text":"<pre><code>#[async_trait]\npub trait SourceConnector {\n    // Initialize external system connection\n    async fn initialize(&amp;mut self, config: ConnectorConfig) -&gt; Result&lt;()&gt;;\n\n    // Define destination Danube topics\n    async fn producer_configs(&amp;self) -&gt; Result&lt;Vec&lt;ProducerConfig&gt;&gt;;\n\n    // Poll external system for data (non-blocking)\n    async fn poll(&amp;mut self) -&gt; Result&lt;Vec&lt;SourceRecord&gt;&gt;;\n\n    // Optional: commit offsets after successful publish\n    async fn commit(&amp;mut self, offsets: Vec&lt;Offset&gt;) -&gt; Result&lt;()&gt;;\n\n    // Optional: cleanup on shutdown\n    async fn shutdown(&amp;mut self) -&gt; Result&lt;()&gt;;\n}\n</code></pre>"},{"location":"integrations/danube_connect_architecture/#sinkconnector","title":"SinkConnector","text":"<pre><code>#[async_trait]\npub trait SinkConnector {\n    // Initialize external system connection\n    async fn initialize(&amp;mut self, config: ConnectorConfig) -&gt; Result&lt;()&gt;;\n\n    // Define which Danube topics to consume from\n    async fn consumer_configs(&amp;self) -&gt; Result&lt;Vec&lt;ConsumerConfig&gt;&gt;;\n\n    // Process a single message\n    async fn process(&amp;mut self, record: SinkRecord) -&gt; Result&lt;()&gt;;\n\n    // Optional: batch processing (better performance)\n    async fn process_batch(&amp;mut self, records: Vec&lt;SinkRecord&gt;) -&gt; Result&lt;()&gt;;\n\n    // Optional: cleanup on shutdown\n    async fn shutdown(&amp;mut self) -&gt; Result&lt;()&gt;;\n}\n</code></pre>"},{"location":"integrations/danube_connect_architecture/#message-types","title":"Message Types","text":"<p>SinkRecord - Message consumed from Danube (Danube \u2192 External System)</p> <p>The runtime handles schema-aware deserialization automatically. Your connector receives typed data as <code>serde_json::Value</code>, already deserialized based on the message's schema.</p> <p>API:</p> <ul> <li><code>payload()</code> - Returns <code>&amp;serde_json::Value</code> (typed data, not raw bytes)</li> <li><code>as_type&lt;T&gt;()</code> - Deserialize payload to specific Rust type</li> <li><code>schema()</code> - Get schema information (subject, version, type) if message has schema</li> <li><code>attributes()</code> - Metadata key-value pairs from producer</li> <li><code>topic()</code>, <code>offset()</code>, <code>publish_time()</code>, <code>producer_name()</code> - Message metadata</li> <li><code>message_id()</code> - Formatted ID for logging/debugging</li> </ul> <p>Example:</p> <pre><code>async fn process(&amp;mut self, record: SinkRecord) -&gt; ConnectorResult&lt;()&gt; {\n    // Access as typed data (already deserialized by runtime)\n    let payload = record.payload();  // &amp;serde_json::Value\n    let user_id = payload[\"user_id\"].as_str().unwrap();\n\n    // Or deserialize to struct\n    let event: MyEvent = record.as_type()?;\n\n    // Check schema info\n    if let Some(schema) = record.schema() {\n        println!(\"Schema: {} v{}\", schema.subject, schema.version);\n    }\n\n    Ok(())\n}\n</code></pre> <p>SourceRecord - Message to publish to Danube (External System \u2192 Danube)</p> <p>You provide typed data as <code>serde_json::Value</code>. The runtime handles schema-aware serialization before sending to Danube.</p> <p>API:</p> <ul> <li><code>new(topic, payload)</code> - Create from <code>serde_json::Value</code></li> <li><code>from_json(topic, data)</code> - Create from any JSON-serializable type</li> <li><code>from_string(topic, text)</code> - Create from string</li> <li><code>with_attribute(key, value)</code> - Add metadata attribute</li> <li><code>with_attributes(map)</code> - Add multiple attributes</li> <li><code>with_key(key)</code> - Set routing key for partitioning</li> <li><code>payload()</code> - Get payload reference</li> </ul> <p>Example:</p> <pre><code>use serde_json::json;\n\n// From JSON value\nlet record = SourceRecord::new(\"/events/users\", json!({\n    \"user_id\": \"123\",\n    \"action\": \"login\"\n})).with_attribute(\"source\", \"api\");\n\n// From struct (automatically serialized to JSON)\n#[derive(Serialize)]\nstruct Event { user_id: String, action: String }\n\nlet event = Event { user_id: \"123\".into(), action: \"login\".into() };\nlet record = SourceRecord::from_json(\"/events/users\", &amp;event)?\n    .with_attribute(\"source\", \"api\")\n    .with_key(&amp;event.user_id);  // For partitioning\n</code></pre>"},{"location":"integrations/danube_connect_architecture/#runtime-management","title":"Runtime Management","text":"<p>The SDK provides <code>SourceRuntime</code> and <code>SinkRuntime</code> that handle:</p> <ol> <li>Lifecycle - Initialize connector, create Danube clients, start message loops</li> <li>Message processing - Poll/consume messages, call your trait methods</li> <li>Error handling - Automatic retry with exponential backoff</li> <li>Acknowledgments - Ack successfully processed messages to Danube</li> <li>Shutdown - Graceful cleanup on SIGTERM/SIGINT</li> <li>Observability - Expose Prometheus metrics and health endpoints</li> </ol>"},{"location":"integrations/danube_connect_architecture/#multi-topic-support","title":"Multi-Topic Support","text":"<p>A single connector can handle multiple topics with different configurations.</p> <p>Source connector example:</p> <ul> <li>One MQTT connector routes <code>sensors/#</code> \u2192 <code>/iot/sensors</code> (8 partitions, reliable)</li> <li>Same connector routes <code>debug/#</code> \u2192 <code>/iot/debug</code> (1 partition, non-reliable)</li> </ul> <p>Sink connector example:</p> <ul> <li>One ClickHouse connector consumes from <code>/analytics/events</code> and <code>/analytics/metrics</code></li> <li>Writes both to different tables in the same database</li> </ul>"},{"location":"integrations/danube_connect_architecture/#schema-registry-integration","title":"Schema Registry Integration","text":"<p>Critical feature: The runtime automatically handles schema-aware serialization and deserialization, eliminating the need for connectors to manage schema logic.</p>"},{"location":"integrations/danube_connect_architecture/#how-it-works","title":"How It Works","text":"<p>For Sink Connectors (Consuming):</p> <ol> <li>Message arrives with schema ID</li> <li>Runtime fetches schema from registry (cached for performance)</li> <li>Runtime deserializes payload based on schema type (JSON, String, Bytes, Avro, Protobuf)</li> <li>Your connector receives <code>SinkRecord</code> with typed <code>serde_json::Value</code> payload</li> <li>You process the typed data without worrying about raw bytes or schemas</li> </ol> <p>For Source Connectors (Publishing):</p> <ol> <li>You create <code>SourceRecord</code> with <code>serde_json::Value</code> payload</li> <li>Runtime serializes payload based on configured schema type</li> <li>Runtime registers/validates schema with registry</li> <li>Message is sent to Danube with schema ID attached</li> <li>Consumers automatically deserialize using the schema</li> </ol>"},{"location":"integrations/danube_connect_architecture/#schema-configuration","title":"Schema Configuration","text":"<p>Define schemas per topic in your connector configuration:</p> <pre><code># Core Danube settings\ndanube_service_url = \"http://danube-broker:6650\"\nconnector_name = \"my-connector\"\n\n# Schema mappings for topics\n[[schemas]]\ntopic = \"/events/users\"\nsubject = \"user-events\"\nschema_type = \"json_schema\"\nschema_file = \"schemas/user-event.json\"\nversion_strategy = \"latest\"  # or \"pinned\" or \"minimum\"\n\n[[schemas]]\ntopic = \"/iot/sensors\"\nsubject = \"sensor-data\"\nschema_type = \"json_schema\"\nschema_file = \"schemas/sensor.json\"\nversion_strategy = { pinned = 2 }  # Pin to version 2\n\n[[schemas]]\ntopic = \"/raw/telemetry\"\nsubject = \"telemetry\"\nschema_type = \"bytes\"  # Binary data\n</code></pre>"},{"location":"integrations/danube_connect_architecture/#supported-schema-types","title":"Supported Schema Types","text":"Type Description Serialization Use Case <code>json_schema</code> JSON Schema JSON Structured events, APIs <code>string</code> UTF-8 text String bytes Logs, text messages <code>bytes</code> Raw binary Base64 in JSON Binary data, images <code>number</code> Numeric data JSON number Metrics, counters <code>avro</code> Apache Avro Avro binary High-performance schemas <code>protobuf</code> Protocol Buffers Protobuf binary gRPC, microservices <p>Note: Avro and Protobuf support coming soon. Currently JSON, String, Bytes, and Number are fully supported.</p>"},{"location":"integrations/danube_connect_architecture/#version-strategies","title":"Version Strategies","text":"<p>Control schema evolution with version strategies:</p> <pre><code># Latest version (default)\nversion_strategy = \"latest\"\n\n# Pin to specific version\nversion_strategy = { pinned = 3 }\n\n# Minimum version (use &gt;= version)\nversion_strategy = { minimum = 2 }\n</code></pre>"},{"location":"integrations/danube_connect_architecture/#connector-benefits","title":"Connector Benefits","text":"<p>What you DON'T need to do:</p> <ul> <li>\u274c Fetch schemas from registry</li> <li>\u274c Deserialize/serialize payloads manually</li> <li>\u274c Handle schema versions</li> <li>\u274c Manage schema caching</li> <li>\u274c Deal with raw bytes</li> </ul> <p>What you DO:</p> <ul> <li>\u2705 Work with typed <code>serde_json::Value</code> data</li> <li>\u2705 Focus on business logic</li> <li>\u2705 Let the runtime handle all schema operations</li> </ul>"},{"location":"integrations/danube_connect_architecture/#example-schema-aware-sink","title":"Example: Schema-Aware Sink","text":"<pre><code>async fn process(&amp;mut self, record: SinkRecord) -&gt; ConnectorResult&lt;()&gt; {\n    // Payload is already deserialized based on schema\n    let payload = record.payload();\n\n    // Check what schema was used\n    if let Some(schema) = record.schema() {\n        info!(\"Processing message with schema: {} v{}\", \n              schema.subject, schema.version);\n    }\n\n    // Access typed data directly\n    let user_id = payload[\"user_id\"].as_str()\n        .ok_or_else(|| ConnectorError::invalid_data(\"Missing user_id\", vec![]))?;\n\n    // Or deserialize to struct\n    #[derive(Deserialize)]\n    struct UserEvent {\n        user_id: String,\n        action: String,\n        timestamp: u64,\n    }\n\n    let event: UserEvent = record.as_type()?;\n\n    // Write to external system with typed data\n    self.database.insert_user_event(event).await?;\n\n    Ok(())\n}\n</code></pre>"},{"location":"integrations/danube_connect_architecture/#example-schema-aware-source","title":"Example: Schema-Aware Source","text":"<pre><code>async fn poll(&amp;mut self) -&gt; ConnectorResult&lt;Vec&lt;SourceRecord&gt;&gt; {\n    let external_data = self.client.fetch_events().await?;\n\n    let records: Vec&lt;SourceRecord&gt; = external_data\n        .into_iter()\n        .map(|event| {\n            // Create record with typed JSON data\n            // Runtime will serialize based on configured schema\n            SourceRecord::from_json(\"/events/users\", &amp;event)\n                .unwrap()\n                .with_attribute(\"source\", \"external-api\")\n        })\n        .collect();\n\n    Ok(records)\n}\n</code></pre> <p>The runtime automatically:</p> <ul> <li>Serializes each record's payload based on <code>/events/users</code> topic's configured schema</li> <li>Registers schema with registry if needed</li> <li>Attaches schema ID to messages</li> <li>Handles schema validation errors</li> </ul>"},{"location":"integrations/danube_connect_architecture/#configuration-pattern","title":"Configuration Pattern","text":"<p>Connectors use a single configuration file combining core Danube settings with connector-specific settings:</p> <pre><code># Core Danube settings (provided by danube-connect-core)\ndanube_service_url = \"http://danube-broker:6650\"\nconnector_name = \"mqtt-bridge\"\n\n# Connector-specific settings\n[mqtt]\nbroker_host = \"mosquitto\"\nbroker_port = 1883\n\n[[mqtt.topic_mappings]]\nmqtt_topic = \"sensors/#\"\ndanube_topic = \"/iot/sensors\"\npartitions = 8\n</code></pre> <p>Environment variable overrides:</p> <ul> <li>Mandatory fields: <code>DANUBE_SERVICE_URL</code>, <code>CONNECTOR_NAME</code></li> <li>Secrets: <code>MQTT_PASSWORD</code>, <code>API_KEY</code>, etc.</li> <li>Connector-specific overrides as needed</li> </ul> <p>See: Configuration Guide for complete details</p>"},{"location":"integrations/danube_connect_architecture/#deployment-architecture","title":"Deployment Architecture","text":"<p>Connectors run as standalone Docker containers alongside your Danube cluster:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  External Systems           \u2502\n\u2502  (MQTT, DBs, APIs)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Connector Layer             \u2502\n\u2502  (Docker Containers)         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 MQTT   \u2502  \u2502 Delta  \u2502 ... \u2502\n\u2502  \u2502 Source \u2502  \u2502 Sink   \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2193          \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Danube Broker Cluster        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Scaling:</p> <ul> <li>Connectors scale independently from brokers</li> <li>Run multiple instances for high availability</li> <li>No impact on broker resources</li> </ul>"},{"location":"integrations/danube_connect_architecture/#error-handling","title":"Error Handling","text":"<p>The SDK categorizes errors into three types:</p> <ol> <li>Retryable - Temporary failures (network issues, rate limits) \u2192 automatic retry with exponential backoff</li> <li>Invalid Data - Malformed messages that can never be processed \u2192 log and skip</li> <li>Fatal - Unrecoverable errors \u2192 graceful shutdown, let orchestrator restart</li> </ol> <p>Your connector: Return the appropriate error type, the SDK handles the rest</p>"},{"location":"integrations/danube_connect_architecture/#observability","title":"Observability","text":"<p>Every connector automatically exposes:</p> <p>Prometheus metrics (<code>http://connector:9090/metrics</code>):</p> <ul> <li><code>connector_messages_received</code> / <code>connector_messages_sent</code></li> <li><code>connector_errors_total</code></li> <li><code>connector_processing_duration_seconds</code></li> </ul> <p>Health checks (<code>http://connector:9090/health</code>):</p> <ul> <li><code>healthy</code>, <code>degraded</code>, or <code>unhealthy</code></li> </ul> <p>Structured logging:</p> <ul> <li>JSON-formatted logs with trace IDs</li> <li>Configurable log levels via <code>LOG_LEVEL</code> env var</li> </ul>"},{"location":"integrations/danube_connect_architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Build Source Connector - How to build a source connector</li> <li>Build Sink Connector - How to build a sink connector</li> <li>GitHub Repository - Source code and examples</li> <li>danube-connect-core API - SDK documentation</li> </ul>"},{"location":"integrations/danube_connect_overview/","title":"Danube Connect","text":"<p>Connector ecosystem for seamless integration with external systems</p>"},{"location":"integrations/danube_connect_overview/#what-is-danube-connect","title":"What is Danube Connect?","text":"<p>Danube Connect is a plug-and-play connector framework that enables Danube to integrate with external systems: databases, message queues, IoT protocols, analytics platforms, and more\u2014without compromising the broker's safety, stability, or performance.</p> <p>Instead of embedding integrations directly into the Danube broker (monolithic approach), connectors run as standalone processes that communicate with Danube via gRPC. This architecture ensures:</p> <ul> <li>\ud83d\udee1\ufe0f Isolation - Connector failures never crash the broker</li> <li>\ud83d\udcc8 Scalability - Scale connectors independently from brokers</li> <li>\ud83d\udd0c Modularity - Add or remove integrations without touching core Danube</li> <li>\ud83e\udd80 Memory Safety - Pure Rust implementation with zero FFI in the broker</li> </ul>"},{"location":"integrations/danube_connect_overview/#architecture","title":"Architecture","text":"<pre><code>External Systems \u2194 Connectors \u2194 danube-connect-core \u2194 danube-client \u2194 Danube Broker\n</code></pre> <p>Connectors are standalone binaries that:</p> <ol> <li>Connect to external systems (MQTT, databases, HTTP APIs, etc.)</li> <li>Use danube-connect-core SDK for Danube communication</li> <li>Transform data between external formats and Danube messages</li> <li>Run independently with their own lifecycle and resources</li> </ol> <p>Key principle: The Danube broker remains \"dumb\" and pure Rust\u2014it knows nothing about external systems.</p>"},{"location":"integrations/danube_connect_overview/#connector-types","title":"Connector Types","text":""},{"location":"integrations/danube_connect_overview/#source-connectors-external-danube","title":"Source Connectors (External \u2192 Danube)","text":"<p>Import data into Danube from external systems.</p> <p>Examples:</p> <ul> <li>MQTT Source - Bridge IoT devices to Danube topics</li> <li>HTTP Webhook Source - Ingest webhooks from SaaS platforms</li> <li>PostgreSQL CDC - Stream database changes to Danube</li> <li>Kafka Source - Migrate from Kafka to Danube</li> </ul> <p>Use cases: IoT data ingestion, event streaming, change data capture, system integration</p>"},{"location":"integrations/danube_connect_overview/#sink-connectors-danube-external","title":"Sink Connectors (Danube \u2192 External)","text":"<p>Export data from Danube to external systems.</p> <p>Examples:</p> <ul> <li>Qdrant Sink - Stream vectors to RAG/AI pipelines</li> <li>Delta Lake Sink - Archive messages to data lakes (S3, Azure, GCS)</li> <li>SurrealDB Sink - Store events in multi-model databases</li> <li>ClickHouse Sink - Real-time analytics and feature stores</li> </ul> <p>Use cases: Data archival, analytics, machine learning, system integration</p>"},{"location":"integrations/danube_connect_overview/#available-connectors","title":"Available Connectors","text":""},{"location":"integrations/danube_connect_overview/#already-available","title":"Already Available","text":"Connector Type Description MQTT Source IoT device integration (MQTT 3.1.1) HTTP Webhook Source Universal webhook ingestion Qdrant Sink Vector embeddings for RAG/AI SurrealDB Sink Multi-model database storage Delta Lake Sink ACID data lake ingestion"},{"location":"integrations/danube_connect_overview/#coming-soon","title":"Coming Soon","text":"<ul> <li>OpenTelemetry Source (traces, metrics, logs)</li> <li>PostgreSQL CDC Source</li> <li>LanceDB Sink (vector search)</li> <li>ClickHouse Sink (analytics)</li> <li>GreptimeDB Sink (observability)</li> </ul>"},{"location":"integrations/danube_connect_overview/#quick-start","title":"Quick Start","text":""},{"location":"integrations/danube_connect_overview/#running-a-connector","title":"Running a Connector","text":"<p>Deploy connectors using Docker:</p> <pre><code>docker run -d \\\n  -e DANUBE_SERVICE_URL=http://danube-broker:6650 \\\n  -e CONNECTOR_NAME=mqtt-bridge \\\n  -v $(pwd)/config.toml:/config.toml \\\n  danube-connect/source-mqtt:latest\n</code></pre> <p>That's it! The connector handles:</p> <ul> <li>Connection management to Danube</li> <li>Message transformation and routing</li> <li>Retry logic and error handling</li> <li>Metrics and health checks</li> <li>Graceful shutdown</li> </ul>"},{"location":"integrations/danube_connect_overview/#why-danube-connect","title":"Why Danube Connect?","text":""},{"location":"integrations/danube_connect_overview/#versus-embedding-in-broker","title":"Versus Embedding in Broker","text":"Embedded Integrations Danube Connect \u274c Broker crashes if integration fails \u2705 Isolated processes \u274c Tight coupling, hard to maintain \u2705 Clean separation \u274c Bloated broker binary \u2705 Lightweight core \u274c All-or-nothing scaling \u2705 Independent scaling"},{"location":"integrations/danube_connect_overview/#versus-custom-scripts","title":"Versus Custom Scripts","text":"DIY Integration Scripts Danube Connect \u274c Manual retry logic \u2705 Built-in exponential backoff \u274c No observability \u2705 Prometheus metrics + health checks \u274c Ad-hoc error handling \u2705 Standardized error types \u274c Reinvent the wheel \u2705 Reusable SDK framework"},{"location":"integrations/danube_connect_overview/#key-features","title":"Key Features","text":""},{"location":"integrations/danube_connect_overview/#bidirectional-data-flow","title":"\ud83d\udd04 Bidirectional Data Flow","text":"<p>Both source (import) and sink (export) connectors supported</p>"},{"location":"integrations/danube_connect_overview/#schema-registry-integration","title":"\ud83d\udccb Schema Registry Integration","text":"<p>Automatic schema-aware serialization/deserialization</p> <ul> <li>Connectors work with typed <code>serde_json::Value</code> data, not raw bytes</li> <li>Runtime handles all schema operations (fetch, cache, validate, serialize)</li> <li>Support for JSON Schema, String, Bytes, Number (Avro &amp; Protobuf coming soon)</li> <li>Schema evolution with version strategies (latest, pinned, minimum)</li> <li>Zero schema boilerplate in your connector code</li> </ul>"},{"location":"integrations/danube_connect_overview/#modular-architecture","title":"\ud83d\udce6 Modular Architecture","text":"<p>Clean separation between connector framework and implementations</p>"},{"location":"integrations/danube_connect_overview/#cloud-native","title":"\ud83d\ude80 Cloud Native","text":"<p>Docker-first with Kubernetes support, horizontal scaling</p>"},{"location":"integrations/danube_connect_overview/#observable","title":"\ud83d\udcca Observable","text":"<p>Prometheus metrics, structured logging, health endpoints</p>"},{"location":"integrations/danube_connect_overview/#high-performance","title":"\u26a1 High Performance","text":"<p>Async I/O, batching, connection pooling, parallel processing, schema caching</p>"},{"location":"integrations/danube_connect_overview/#pure-rust","title":"\ud83e\udd80 Pure Rust","text":"<p>Memory-safe, high-performance, zero-cost abstractions</p>"},{"location":"integrations/danube_connect_overview/#learn-more","title":"Learn More","text":"<ul> <li>Connector Architecture - Deep dive into design and concepts</li> <li>Buil Source Connector - Create your own source connector</li> <li>Build Sink Connector - Create your own sink connector</li> <li>Github Connector Core - Connector SDK source code</li> <li>GitHub Connectors Repo - Connectors source code and full examples</li> </ul>"},{"location":"integrations/danube_connect_overview/#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Issues: Report bugs or request connectors</li> <li>Source Code: danube-messaging/danube-connect</li> <li>Examples: Complete connector examples</li> </ul>"},{"location":"integrations/sink_connector_development/","title":"Sink Connector Development Guide","text":"<p>Build sink connectors to export data from Danube to any external system</p>"},{"location":"integrations/sink_connector_development/#overview","title":"Overview","text":"<p>A sink connector exports data from Danube topics to external systems. The connector consumes messages, transforms data, and writes to the target system with batching and error handling. The runtime handles consumption, schema deserialization, and delivery guarantees.</p> <p>Examples: Danube\u2192Delta Lake, Danube\u2192ClickHouse, Danube\u2192Vector DB, Danube\u2192HTTP API</p>"},{"location":"integrations/sink_connector_development/#core-concepts","title":"Core Concepts","text":""},{"location":"integrations/sink_connector_development/#division-of-responsibilities","title":"Division of Responsibilities","text":"You Handle Runtime Handles Connect to external system Connect to Danube broker Transform <code>SinkRecord</code> data Consume from Danube topics Write to target system Schema deserialization &amp; validation Batch operations Message buffering &amp; delivery Error handling per system Lifecycle &amp; health monitoring Flush buffered data Metrics &amp; observability <p>Key insight: You receive typed <code>serde_json::Value</code> data already deserialized by the runtime. No manual schema operations needed.</p>"},{"location":"integrations/sink_connector_development/#sinkconnector-trait","title":"SinkConnector Trait","text":"<pre><code>#[async_trait]\npub trait SinkConnector: Send + Sync {\n    async fn initialize(&amp;mut self, config: ConnectorConfig) -&gt; ConnectorResult&lt;()&gt;;\n    async fn consumer_configs(&amp;self) -&gt; ConnectorResult&lt;Vec&lt;ConsumerConfig&gt;&gt;;\n    async fn process(&amp;mut self, record: SinkRecord) -&gt; ConnectorResult&lt;()&gt;;\n\n    // Optional methods with defaults\n    async fn process_batch(&amp;mut self, records: Vec&lt;SinkRecord&gt;) -&gt; ConnectorResult&lt;()&gt;;\n    async fn shutdown(&amp;mut self) -&gt; ConnectorResult&lt;()&gt; { Ok(()) }\n    async fn health_check(&amp;self) -&gt; ConnectorResult&lt;()&gt; { Ok(()) }\n}\n</code></pre> <p>Required: <code>initialize</code>, <code>consumer_configs</code>, <code>process</code> Optional but recommended: <code>process_batch</code> (for performance) Optional: <code>shutdown</code>, <code>health_check</code></p>"},{"location":"integrations/sink_connector_development/#configuration-architecture","title":"Configuration Architecture","text":""},{"location":"integrations/sink_connector_development/#unified-configuration-pattern","title":"Unified Configuration Pattern","text":"<p>Use a single configuration struct combining core Danube settings with connector-specific settings:</p> <pre><code>#[derive(Deserialize)]\npub struct MySinkConfig {\n    #[serde(flatten)]\n    pub core: ConnectorConfig,  // Danube settings (no schemas for sinks!)\n\n    pub my_sink: MySettings,    // Connector-specific\n}\n</code></pre> <p>Critical for Sink Connectors:</p> <ul> <li>\u2705 Sink connectors do not use <code>config.core.schemas</code></li> <li>\u2705 Schema validation happens on the producer side (source connectors)</li> <li>\u2705 You receive already-deserialized, validated data</li> <li>\u2705 Schema info available via <code>record.schema()</code> if message has schema metadata</li> </ul>"},{"location":"integrations/sink_connector_development/#toml-structure","title":"TOML Structure","text":"<pre><code># At root level (flattened from ConnectorConfig)\ndanube_service_url = \"http://danube-broker:6650\"\nconnector_name = \"my-sink\"\n\n# No [[schemas]] section - sinks consume, don't validate\n\n# Connector-specific section\n[my_sink]\ntarget_url = \"http://database.example.com\"\nconnection_pool_size = 10\n\n# Topic mappings with subscription details\n[[my_sink.topic_mappings]]\ntopic = \"/events/users\"\nsubscription = \"sink-group-1\"\nsubscription_type = \"Shared\"\ntarget_table = \"users\"\nexpected_schema_subject = \"user-events-v1\"  # Optional: verify schema\nbatch_size = 100\nflush_interval_ms = 1000\n\n[[my_sink.topic_mappings]]\ntopic = \"/events/orders\"\nsubscription = \"sink-group-1\"\nsubscription_type = \"Shared\"\ntarget_table = \"orders\"\nbatch_size = 500\n</code></pre>"},{"location":"integrations/sink_connector_development/#configuration-loading","title":"Configuration Loading","text":"<p>Mechanism:</p> <ol> <li>Load TOML file from <code>CONNECTOR_CONFIG_PATH</code> environment variable</li> <li>Apply environment variable overrides (secrets only)</li> <li>Validate configuration</li> <li>Pass to connector and runtime</li> </ol> <p>Environment overrides:</p> <ul> <li>Core: <code>DANUBE_SERVICE_URL</code>, <code>CONNECTOR_NAME</code></li> <li>Secrets: Database passwords, API keys, credentials</li> </ul> <p>Not overridable: Topic mappings, subscription settings (must be in TOML)</p>"},{"location":"integrations/sink_connector_development/#implementation-steps","title":"Implementation Steps","text":""},{"location":"integrations/sink_connector_development/#1-initialize-connection","title":"1. Initialize Connection","text":"<p><code>async fn initialize(&amp;mut self, config: ConnectorConfig)</code></p> <p>Purpose: Establish connection to external system and prepare for writing.</p> <p>What to do:</p> <ul> <li>Create client/connection to target system</li> <li>Configure authentication (credentials, API tokens, certificates)</li> <li>Set up connection pools for parallel writes</li> <li>Validate write permissions with test write/query</li> <li>Create tables/collections/indexes if needed</li> <li>Store client in struct for later use in <code>process()</code></li> </ul> <p>Error handling:</p> <ul> <li>Return <code>ConnectorError::Initialization</code> for connection failures</li> <li>Log detailed error context</li> <li>Fail fast - don't retry in <code>initialize()</code></li> </ul> <p>Examples:</p> <ul> <li>Database: Create connection pool, validate table schema, test insert</li> <li>HTTP API: Set up client with auth headers, test endpoint availability</li> <li>Object Storage: Configure S3/GCS client, verify bucket access</li> <li>Search Engine: Connect, create index if needed, validate mappings</li> </ul>"},{"location":"integrations/sink_connector_development/#2-define-consumer-configs","title":"2. Define Consumer Configs","text":"<p><code>async fn consumer_configs(&amp;self) -&gt; Vec&lt;ConsumerConfig&gt;</code></p> <p>Purpose: Tell the runtime which Danube topics to consume from and how.</p> <p>What to do:</p> <ul> <li>Map your topic configuration to Danube consumer configs</li> <li>For each source topic, create a <code>ConsumerConfig</code></li> <li>Specify subscription type (Shared, Exclusive, Failover)</li> <li>Set consumer name and subscription name</li> <li>Optionally specify expected schema subject for validation</li> </ul> <p>Consumer config structure:</p> <pre><code>ConsumerConfig {\n    topic: \"/events/users\",\n    consumer_name: \"my-sink-1\",\n    subscription: \"my-sink-group\",\n    subscription_type: SubscriptionType::Shared,\n    expected_schema_subject: Some(\"user-events-v1\"),  // Optional\n}\n</code></pre> <p>Subscription types:</p> <ul> <li>Shared: Load balancing across multiple consumers (most common)</li> <li>Exclusive: Single consumer, ordered processing</li> <li>Failover: Active-passive, automatic failover</li> </ul> <p>Called once at startup after <code>initialize()</code>. Runtime creates consumers based on these configs.</p>"},{"location":"integrations/sink_connector_development/#3-process-records","title":"3. Process Records","text":"<p><code>async fn process(&amp;mut self, record: SinkRecord)</code></p> <p>Purpose: Transform a single message and write to external system.</p> <p>Called: For each message consumed from Danube (or use <code>process_batch</code> instead)</p> <p>What to do:</p> <ol> <li>Extract data - Already deserialized by runtime</li> <li>Access metadata - Topic, offset, timestamp, attributes</li> <li>Transform - Convert to target system format</li> <li>Write - Send to external system</li> <li>Handle errors - Return appropriate error type</li> </ol> <p>SinkRecord API:</p> <pre><code>let payload = record.payload();           // &amp;serde_json::Value\nlet topic = record.topic();               // &amp;str\nlet offset = record.offset();             // u64\nlet timestamp = record.publish_time();    // u64 (microseconds)\nlet attributes = record.attributes();     // &amp;HashMap&lt;String, String&gt;\n\n// Check schema info if message has schema\nif let Some(schema) = record.schema() {\n    // schema.subject, schema.version, schema.schema_type\n}\n\n// Deserialize to struct (type-safe)\nlet event: MyStruct = record.as_type()?;\n</code></pre> <p>Runtime provides:</p> <ul> <li>Pre-deserialized JSON data</li> <li>Schema validation already done</li> <li>Type-safe access methods</li> <li>Metadata extraction</li> </ul>"},{"location":"integrations/sink_connector_development/#4-batch-processing-recommended","title":"4. Batch Processing (Recommended)","text":"<p><code>async fn process_batch(&amp;mut self, records: Vec&lt;SinkRecord&gt;)</code></p> <p>Purpose: Process multiple records together for better throughput.</p> <p>Why batching:</p> <ul> <li>10-100x better performance for bulk operations</li> <li>Reduced network round trips</li> <li>More efficient resource usage</li> <li>Lower latency at scale</li> </ul> <p>What to do:</p> <ol> <li>Group by target - If multi-topic, group by destination table/collection</li> <li>Transform batch - Convert all records to target format</li> <li>Bulk write - Single operation for entire batch</li> <li>Handle partial failures - Some systems support partial success</li> </ol> <p>Batch triggers:</p> <ul> <li>Size: <code>batch_size</code> records buffered (e.g., 100)</li> <li>Time: <code>flush_interval_ms</code> elapsed (e.g., 1000ms)</li> <li>Shutdown: Flush remaining records</li> </ul> <p>Performance guidance:</p> <ul> <li>High throughput: 500-1000 records per batch</li> <li>Low latency: 10-50 records per batch</li> <li>Balanced: 100 records per batch</li> </ul> <p>Per-topic configuration: Different topics can have different batch sizes based on workload characteristics.</p>"},{"location":"integrations/sink_connector_development/#5-error-handling","title":"5. Error Handling","text":"<p>Error types guide runtime behavior:</p> Error Type When to Use Runtime Action <code>Retryable</code> Temporary failures (rate limit, connection) Retry with backoff <code>Fatal</code> Permanent failures (auth, bad config) Shutdown connector <code>Ok(())</code> Success or skippable errors Acknowledge message <p>Mechanism:</p> <ul> <li>Temporary errors: Rate limits, timeouts, transient failures \u2192 <code>Retryable</code></li> <li>Invalid data: Log warning, skip record \u2192 <code>Ok()</code> (acknowledge anyway)</li> <li>Fatal errors: Auth failure, connection lost \u2192 <code>Fatal</code> (stops connector)</li> </ul> <p>The runtime:</p> <ul> <li>Retries <code>Retryable</code> errors with exponential backoff</li> <li>Stops connector on <code>Fatal</code> errors</li> <li>Acknowledges and continues on <code>Ok(())</code></li> </ul> <p>Partial batch failures: For systems that support it, process successful records and retry only failed ones.</p>"},{"location":"integrations/sink_connector_development/#6-graceful-shutdown","title":"6. Graceful Shutdown","text":"<p><code>async fn shutdown(&amp;mut self)</code></p> <p>Purpose: Clean up resources and flush pending data before stopping.</p> <p>What to do:</p> <ul> <li>Flush any buffered records (if using internal batching)</li> <li>Complete any in-flight writes</li> <li>Close connections to external system</li> <li>Save any state if needed</li> <li>Log shutdown completion</li> </ul> <p>Runtime guarantees:</p> <ul> <li><code>shutdown()</code> called on SIGTERM/SIGINT</li> <li>No new messages delivered after shutdown starts</li> <li>Time to complete graceful shutdown</li> </ul>"},{"location":"integrations/sink_connector_development/#schema-handling-in-sink-connectors","title":"Schema Handling in Sink Connectors","text":""},{"location":"integrations/sink_connector_development/#how-schemas-work","title":"How Schemas Work","text":"<p>For sink connectors, schemas are handled differently than sources:</p> <ol> <li>Producer side (source connector or app) validates and attaches schema</li> <li>Message stored in Danube with schema ID</li> <li>Runtime fetches schema from registry when consuming</li> <li>Runtime deserializes payload based on schema type</li> <li>You receive validated, typed <code>serde_json::Value</code></li> </ol> <p>You never:</p> <ul> <li>\u274c Validate schemas (producer already did)</li> <li>\u274c Deserialize raw bytes (runtime already did)</li> <li>\u274c Fetch schemas from registry (runtime caches them)</li> <li>\u274c Configure schemas in <code>ConnectorConfig</code> (no <code>[[schemas]]</code> section)</li> </ul> <p>You can:</p> <ul> <li>\u2705 Access schema metadata via <code>record.schema()</code></li> <li>\u2705 Specify <code>expected_schema_subject</code> for validation</li> <li>\u2705 Use different logic based on schema version</li> <li>\u2705 Access pre-validated typed data</li> </ul>"},{"location":"integrations/sink_connector_development/#expected-schema-subject","title":"Expected Schema Subject","text":"<p>Optional verification that messages have expected schema:</p> <pre><code>[[my_sink.topic_mappings]]\ntopic = \"/events/users\"\nexpected_schema_subject = \"user-events-v1\"  # Verify schema subject\n</code></pre> <p>What happens:</p> <ul> <li>Runtime checks if message schema subject matches</li> <li>Mismatch results in error (configurable behavior)</li> <li>Useful for ensuring data contracts</li> </ul> <p>When to use:</p> <ul> <li>\u2705 Strict data contracts required</li> <li>\u2705 Multiple producers on same topic</li> <li>\u2705 Critical data pipelines</li> </ul> <p>When to skip:</p> <ul> <li>Schema evolution in progress</li> <li>Flexible data formats</li> <li>Mixed content topics</li> </ul>"},{"location":"integrations/sink_connector_development/#subscription-types","title":"Subscription Types","text":""},{"location":"integrations/sink_connector_development/#shared-most-common","title":"Shared (Most Common)","text":"<p>Behavior: Messages load-balanced across all active consumers</p> <p>Use cases:</p> <ul> <li>High-volume parallel processing</li> <li>Horizontal scaling</li> <li>Order doesn't matter within topic</li> </ul> <p>Example: Processing events where each event is independent</p>"},{"location":"integrations/sink_connector_development/#exclusive","title":"Exclusive","text":"<p>Behavior: Only one consumer receives messages, strictly ordered</p> <p>Use cases:</p> <ul> <li>Order-dependent processing</li> <li>Sequential operations</li> <li>Single-threaded requirements</li> </ul> <p>Example: Bank transactions that must be processed in order</p>"},{"location":"integrations/sink_connector_development/#failover","title":"Failover","text":"<p>Behavior: Active-passive setup with automatic failover</p> <p>Use cases:</p> <ul> <li>Ordered processing with high availability</li> <li>Standby consumer for reliability</li> <li>Graceful failover</li> </ul> <p>Example: Critical pipeline with backup consumer</p>"},{"location":"integrations/sink_connector_development/#multi-topic-routing","title":"Multi-Topic Routing","text":""},{"location":"integrations/sink_connector_development/#pattern","title":"Pattern","text":"<p>Each topic mapping is independent:</p> <ul> <li>Different target entities (tables, collections, indexes)</li> <li>Different batch sizes and flush intervals</li> <li>Different transformations</li> <li>Different schema expectations</li> </ul>"},{"location":"integrations/sink_connector_development/#mechanism","title":"Mechanism","text":"<ol> <li>Store topic \u2192 config mapping in connector</li> <li>Lookup configuration for each record's topic</li> <li>Apply topic-specific logic for transformation</li> <li>Route to target entity (table, collection, etc.)</li> </ol>"},{"location":"integrations/sink_connector_development/#configuration-example","title":"Configuration Example","text":"<pre><code>[[my_sink.topic_mappings]]\ntopic = \"/events/clicks\"\ntarget_table = \"clicks\"\nbatch_size = 1000  # High volume\n\n[[my_sink.topic_mappings]]\ntopic = \"/events/orders\"\ntarget_table = \"orders\"\nbatch_size = 100   # Lower latency\n</code></pre>"},{"location":"integrations/sink_connector_development/#main-entry-point","title":"Main Entry Point","text":"<pre><code>#[tokio::main]\nasync fn main() -&gt; ConnectorResult&lt;()&gt; {\n    // 1. Initialize logging\n    tracing_subscriber::fmt::init();\n\n    // 2. Load configuration\n    let config = MySinkConfig::load()?;\n\n    // 3. Create connector with settings\n    let connector = MySinkConnector::new(config.my_sink)?;\n\n    // 4. Create and run runtime (handles everything else)\n    let runtime = SinkRuntime::new(connector, config.core).await?;\n    runtime.run().await\n}\n</code></pre> <p>Runtime handles:</p> <ul> <li>Danube connection</li> <li>Consumer creation with subscription configs</li> <li>Message consumption and buffering</li> <li>Schema fetching and deserialization</li> <li>Calling <code>process()</code> or <code>process_batch()</code></li> <li>Retries and error handling</li> <li>Metrics and health monitoring</li> <li>Graceful shutdown on signals</li> </ul>"},{"location":"integrations/sink_connector_development/#best-practices","title":"Best Practices","text":""},{"location":"integrations/sink_connector_development/#configuration","title":"Configuration","text":"<ul> <li>\u2705 Use flattened <code>ConnectorConfig</code> - no schemas field needed</li> <li>\u2705 Keep secrets in environment variables, not TOML</li> <li>\u2705 Validate configuration at startup</li> <li>\u2705 Provide per-topic batch configuration</li> </ul>"},{"location":"integrations/sink_connector_development/#data-processing","title":"Data Processing","text":"<ul> <li>\u2705 Use <code>process_batch()</code> for performance (10-100x faster)</li> <li>\u2705 Group records by target entity in multi-topic scenarios</li> <li>\u2705 Make metadata addition optional (configurable)</li> <li>\u2705 Handle missing fields gracefully</li> </ul>"},{"location":"integrations/sink_connector_development/#batching","title":"Batching","text":"<ul> <li>\u2705 Configure both size and time triggers</li> <li>\u2705 Adjust per-topic for different workloads</li> <li>\u2705 Flush on shutdown to avoid data loss</li> <li>\u2705 Document batch size recommendations</li> </ul>"},{"location":"integrations/sink_connector_development/#error-handling","title":"Error Handling","text":"<ul> <li>\u2705 <code>Retryable</code> for temporary failures (rate limits, network)</li> <li>\u2705 <code>Fatal</code> for permanent failures (auth, config)</li> <li>\u2705 <code>Ok(())</code> for success or skippable data issues</li> <li>\u2705 Log detailed error context</li> </ul>"},{"location":"integrations/sink_connector_development/#subscription-types_1","title":"Subscription Types","text":"<ul> <li>\u2705 Use <code>Shared</code> for scalable parallel processing (default)</li> <li>\u2705 Use <code>Exclusive</code> only when ordering is critical</li> <li>\u2705 Use <code>Failover</code> for ordered + high availability</li> <li>\u2705 Document ordering requirements</li> </ul>"},{"location":"integrations/sink_connector_development/#performance","title":"Performance","text":"<ul> <li>\u2705 Use connection pooling for concurrent writes</li> <li>\u2705 Batch operations whenever possible</li> <li>\u2705 Configure appropriate timeouts</li> <li>\u2705 Monitor backpressure and lag</li> </ul>"},{"location":"integrations/sink_connector_development/#common-patterns","title":"Common Patterns","text":""},{"location":"integrations/sink_connector_development/#direct-write","title":"Direct Write","text":"<p>Receive data, transform, write directly. Simplest approach for low-volume or low-latency requirements.</p>"},{"location":"integrations/sink_connector_development/#buffered-batch-write","title":"Buffered Batch Write","text":"<p>Buffer records until batch size/timeout reached, then bulk write. Best for throughput.</p>"},{"location":"integrations/sink_connector_development/#grouped-multi-topic-write","title":"Grouped Multi-Topic Write","text":"<p>Group records by target entity, write each group as batch. Efficient for multi-topic routing.</p>"},{"location":"integrations/sink_connector_development/#enriched-write","title":"Enriched Write","text":"<p>Add metadata from Danube (topic, offset, timestamp) to records before writing. Useful for auditing.</p>"},{"location":"integrations/sink_connector_development/#upsert-pattern","title":"Upsert Pattern","text":"<p>Use record attributes or payload fields to determine insert vs. update. Common for CDC scenarios.</p>"},{"location":"integrations/sink_connector_development/#testing","title":"Testing","text":""},{"location":"integrations/sink_connector_development/#local-development","title":"Local Development","text":"<ol> <li>Start Danube: <code>docker-compose up -d</code></li> <li>Set config: <code>export CONNECTOR_CONFIG_PATH=./config/connector.toml</code></li> <li>Run: <code>cargo run</code></li> <li>Produce: <code>danube-cli produce --topic /events/test --message '{\"test\":\"data\"}'</code></li> <li>Verify: Check target system for data</li> </ol>"},{"location":"integrations/sink_connector_development/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test transformation logic with sample <code>SinkRecord</code>s</li> <li>Test topic routing and configuration lookup</li> <li>Mock external system for isolated testing</li> <li>Validate error handling paths</li> </ul>"},{"location":"integrations/sink_connector_development/#integration-tests","title":"Integration Tests","text":"<ul> <li>Full stack: Danube + Connector + Target system</li> <li>Produce messages with schemas</li> <li>Verify data in target system</li> <li>Test batching behavior</li> <li>Test error scenarios</li> </ul>"},{"location":"integrations/sink_connector_development/#summary","title":"Summary","text":"<p>You implement:</p> <ul> <li><code>SinkConnector</code> trait (3 required, 2-3 optional methods)</li> <li>Configuration with flattened <code>ConnectorConfig</code> (no schemas!)</li> <li>External system integration (connection, writes, batching)</li> <li>Message transformation from <code>SinkRecord</code> to target format</li> <li>Topic routing and multi-topic handling</li> </ul> <p>Runtime handles:</p> <ul> <li>Danube connection &amp; consumption</li> <li>Schema fetching &amp; deserialization</li> <li>Message buffering &amp; delivery</li> <li>Calling your <code>process</code> methods</li> <li>Retries &amp; error handling</li> <li>Lifecycle &amp; monitoring</li> <li>Metrics &amp; health</li> </ul> <p>Remember:</p> <ul> <li>Configuration uses flattened <code>ConnectorConfig</code> without schemas field</li> <li>Schemas already validated by producers - you receive typed data</li> <li><code>expected_schema_subject</code> is optional verification, not validation</li> <li>Use <code>process_batch()</code> for best performance</li> <li>Focus on external system integration and data transformation</li> </ul>"},{"location":"integrations/source_connector_development/","title":"Source Connector Development Guide","text":"<p>Build source connectors to import data into Danube from any external system</p>"},{"location":"integrations/source_connector_development/#overview","title":"Overview","text":"<p>A source connector imports data from external systems into Danube topics. The connector polls or listens to the external system, transforms data into <code>SourceRecord</code>s, and the runtime handles publishing to Danube with schema validation and delivery guarantees.</p> <p>Examples: MQTT\u2192Danube, PostgreSQL CDC\u2192Danube, HTTP webhooks\u2192Danube, Kafka\u2192Danube</p>"},{"location":"integrations/source_connector_development/#core-concepts","title":"Core Concepts","text":""},{"location":"integrations/source_connector_development/#division-of-responsibilities","title":"Division of Responsibilities","text":"You Handle Runtime Handles Connect to external system Connect to Danube broker Poll/listen for data Publish to Danube topics Transform to <code>SourceRecord</code> Schema serialization &amp; validation Topic routing logic Message delivery &amp; retries Error handling per system Lifecycle &amp; health monitoring Offset tracking (optional) Metrics &amp; observability <p>Key insight: You work with typed <code>serde_json::Value</code> data. The runtime handles schema-based serialization and raw byte formats.</p>"},{"location":"integrations/source_connector_development/#sourceconnector-trait","title":"SourceConnector Trait","text":"<pre><code>#[async_trait]\npub trait SourceConnector: Send + Sync {\n    async fn initialize(&amp;mut self, config: ConnectorConfig) -&gt; ConnectorResult&lt;()&gt;;\n    async fn producer_configs(&amp;self) -&gt; ConnectorResult&lt;Vec&lt;ProducerConfig&gt;&gt;;\n    async fn poll(&amp;mut self) -&gt; ConnectorResult&lt;Vec&lt;SourceRecord&gt;&gt;;\n\n    // Optional methods with defaults\n    async fn commit(&amp;mut self, offsets: Vec&lt;Offset&gt;) -&gt; ConnectorResult&lt;()&gt; { Ok(()) }\n    async fn shutdown(&amp;mut self) -&gt; ConnectorResult&lt;()&gt; { Ok(()) }\n    async fn health_check(&amp;self) -&gt; ConnectorResult&lt;()&gt; { Ok(()) }\n}\n</code></pre> <p>Required: <code>initialize</code>, <code>producer_configs</code>, <code>poll</code> Optional: <code>commit</code>, <code>shutdown</code>, <code>health_check</code></p>"},{"location":"integrations/source_connector_development/#configuration-architecture","title":"Configuration Architecture","text":""},{"location":"integrations/source_connector_development/#unified-configuration-pattern","title":"Unified Configuration Pattern","text":"<p>Use a single configuration struct that combines core Danube settings with connector-specific settings:</p> <pre><code>#[derive(Deserialize)]\npub struct MySourceConfig {\n    #[serde(flatten)]\n    pub core: ConnectorConfig,  // Danube settings + schemas\n\n    pub my_source: MySettings,  // Connector-specific\n}\n</code></pre> <p>Critical: <code>ConnectorConfig</code> is flattened to the root level. This means:</p> <ul> <li>\u2705 Core fields appear at TOML root: <code>danube_service_url</code>, <code>connector_name</code></li> <li>\u2705 <code>[[schemas]]</code> sections at root level (not nested)</li> <li>\u2705 Access schemas via <code>config.core.schemas</code></li> <li>\u274c Never duplicate schemas field in your config struct</li> </ul>"},{"location":"integrations/source_connector_development/#toml-structure","title":"TOML Structure","text":"<pre><code># At root level (flattened from ConnectorConfig)\ndanube_service_url = \"http://danube-broker:6650\"\nconnector_name = \"my-source\"\n\n[[schemas]]  # \u2190 From ConnectorConfig (flattened)\ntopic = \"/data/events\"\nsubject = \"events-v1\"\nschema_type = \"json_schema\"\nschema_file = \"schemas/events.json\"\nauto_register = true\nversion_strategy = \"latest\"\n\n# Connector-specific section\n[my_source]\nexternal_host = \"system.example.com\"\npoll_interval_ms = 100\n\n[[my_source.topic_mappings]]\nsource_pattern = \"input/*\"\ndanube_topic = \"/data/events\"\npartitions = 4\n</code></pre>"},{"location":"integrations/source_connector_development/#configuration-loading","title":"Configuration Loading","text":"<p>Mechanism:</p> <ol> <li>Load TOML file from <code>CONNECTOR_CONFIG_PATH</code> environment variable</li> <li>Apply environment variable overrides (secrets only)</li> <li>Validate configuration</li> <li>Pass to connector and runtime</li> </ol> <p>Environment overrides:</p> <ul> <li>Core: <code>DANUBE_SERVICE_URL</code>, <code>CONNECTOR_NAME</code></li> <li>Secrets: System-specific credentials (e.g., <code>API_KEY</code>, <code>PASSWORD</code>)</li> </ul> <p>Not overridable: Topic mappings, schemas, processing settings (must be in TOML)</p>"},{"location":"integrations/source_connector_development/#implementation-steps","title":"Implementation Steps","text":""},{"location":"integrations/source_connector_development/#1-initialize-connection","title":"1. Initialize Connection","text":"<p><code>async fn initialize(&amp;mut self, config: ConnectorConfig)</code></p> <p>Purpose: Establish connection to external system and prepare for data collection.</p> <p>What to do:</p> <ul> <li>Create client/connection to external system</li> <li>Configure authentication (API keys, certificates, tokens)</li> <li>Set up connection pools if needed</li> <li>Subscribe to data sources (topics, streams, tables)</li> <li>Validate connectivity with test query/ping</li> <li>Store client in struct for later use in <code>poll()</code></li> </ul> <p>Error handling:</p> <ul> <li>Return <code>ConnectorError::Initialization</code> for connection failures</li> <li>Log detailed error context for debugging</li> <li>Fail fast - don't retry in <code>initialize()</code></li> </ul> <p>Examples:</p> <ul> <li>MQTT: Connect to broker, authenticate, subscribe to topic patterns</li> <li>Database: Create connection pool, validate table access</li> <li>HTTP: Set up client with auth headers, test endpoint</li> <li>Kafka: Configure consumer group, set deserializers</li> </ul>"},{"location":"integrations/source_connector_development/#2-define-producer-configs","title":"2. Define Producer Configs","text":"<p><code>async fn producer_configs(&amp;self) -&gt; Vec&lt;ProducerConfig&gt;</code></p> <p>Purpose: Tell the runtime which Danube topics you'll publish to and their configurations.</p> <p>What to do:</p> <ul> <li>Map your topic routing logic to Danube topics</li> <li>For each destination topic, create a <code>ProducerConfig</code></li> <li>Attach schema configuration if defined for that topic</li> <li>Specify partitions and reliability settings</li> </ul> <p>Schema mapping:</p> <pre><code>// Find schema for this Danube topic from config.core.schemas\nlet schema_config = self.schemas.iter()\n    .find(|s| s.topic == danube_topic)\n    .map(|s| convert_to_schema_config(s));\n\nProducerConfig {\n    topic: danube_topic,\n    partitions: 4,\n    reliable_dispatch: true,\n    schema_config,  // Runtime uses this for serialization\n}\n</code></pre> <p>Called once at startup after <code>initialize()</code>. Runtime creates producers based on these configs.</p>"},{"location":"integrations/source_connector_development/#3-poll-for-data","title":"3. Poll for Data","text":"<p><code>async fn poll(&amp;mut self) -&gt; Vec&lt;SourceRecord&gt;</code></p> <p>Purpose: Fetch new data from external system and transform to <code>SourceRecord</code>s.</p> <p>Called: Repeatedly at configured interval (default: 100ms)</p> <p>What to do:</p> <ol> <li>Fetch data from external system (query, consume, poll buffer)</li> <li>Check for data - if none, return empty <code>Vec</code> (non-blocking)</li> <li>Route messages - determine destination Danube topic for each message</li> <li>Transform - convert to <code>SourceRecord</code> with typed data</li> <li>Return batch of records</li> </ol> <p>Transformation mechanism:</p> <pre><code>// From JSON-serializable struct\nSourceRecord::from_json(topic, &amp;struct_data)?\n\n// From JSON value\nSourceRecord::new(topic, serde_json::Value)\n\n// From string\nSourceRecord::from_string(topic, &amp;text)\n\n// From number\nSourceRecord::from_number(topic, value)\n</code></pre> <p>Runtime handles:</p> <ul> <li>Schema validation (if configured)</li> <li>Serialization based on schema type</li> <li>Publishing to Danube</li> <li>Retries and delivery guarantees</li> </ul> <p>Polling patterns:</p> <ul> <li>Pull: Query with cursor/offset, track last position</li> <li>Push: Drain in-memory buffer filled by subscriptions/callbacks</li> <li>Stream: Read batch from continuous stream</li> </ul> <p>Return empty <code>Vec</code> when:</p> <ul> <li>No data available (normal, not an error)</li> <li>External system returns empty result</li> <li>Rate limited (after logging warning)</li> </ul>"},{"location":"integrations/source_connector_development/#4-error-handling","title":"4. Error Handling","text":"<p>Error types guide runtime behavior:</p> Error Type When to Use Runtime Action <code>Retryable</code> Temporary failures (network, rate limit) Retry with backoff <code>Fatal</code> Permanent failures (auth, bad config) Shutdown connector <code>Ok(vec![])</code> No data or skippable errors Continue polling <p>Mechanism:</p> <ul> <li>Temporary errors: Network blips, rate limits, timeouts \u2192 <code>Retryable</code></li> <li>Invalid data: Log warning, skip message \u2192 <code>Ok(vec![])</code> with empty result</li> <li>Fatal errors: Auth failure, config error \u2192 <code>Fatal</code> (stops connector)</li> </ul> <p>The runtime:</p> <ul> <li>Retries <code>Retryable</code> errors with exponential backoff</li> <li>Stops connector on <code>Fatal</code> errors</li> <li>Continues normally on <code>Ok(vec![])</code></li> </ul>"},{"location":"integrations/source_connector_development/#5-optional-offset-management","title":"5. Optional: Offset Management","text":"<p><code>async fn commit(&amp;mut self, offsets: Vec&lt;Offset&gt;)</code></p> <p>Purpose: Track processing position for exactly-once semantics and resumability.</p> <p>When to implement:</p> <ul> <li>Exactly-once processing required</li> <li>Need to resume from specific position after restart</li> <li>External system supports offset/cursor tracking</li> </ul> <p>When to skip:</p> <ul> <li>At-least-once is acceptable (most cases)</li> <li>Stateless data sources (webhooks, streams)</li> <li>Idempotent downstream consumers</li> </ul> <p>Mechanism:</p> <ol> <li>Runtime calls <code>commit()</code> after successfully publishing batch to Danube</li> <li>You save offsets to external system or local storage</li> <li>On restart, load offsets and resume from last committed position</li> </ol>"},{"location":"integrations/source_connector_development/#6-optional-graceful-shutdown","title":"6. Optional: Graceful Shutdown","text":"<p><code>async fn shutdown(&amp;mut self)</code></p> <p>Purpose: Clean up resources before connector stops.</p> <p>What to do:</p> <ul> <li>Unsubscribe from data sources</li> <li>Flush any buffered data</li> <li>Close connections to external system</li> <li>Save final state/offsets</li> </ul> <p>Runtime guarantees:</p> <ul> <li><code>shutdown()</code> called on SIGTERM/SIGINT</li> <li>All pending <code>SourceRecord</code>s published before shutdown</li> <li>No new <code>poll()</code> calls after shutdown starts</li> </ul>"},{"location":"integrations/source_connector_development/#schema-registry-integration","title":"Schema Registry Integration","text":""},{"location":"integrations/source_connector_development/#why-use-schemas","title":"Why Use Schemas?","text":"<p>Without schemas:</p> <ul> <li>Manual serialization/deserialization</li> <li>No validation or type safety</li> <li>Schema drift between producers/consumers</li> <li>Difficult evolution</li> </ul> <p>With schemas:</p> <ul> <li>\u2705 Runtime handles serialization based on schema type</li> <li>\u2705 Automatic validation at message ingestion</li> <li>\u2705 Type safety and data contracts</li> <li>\u2705 Managed schema evolution</li> <li>\u2705 You work with <code>serde_json::Value</code>, not bytes</li> </ul>"},{"location":"integrations/source_connector_development/#schema-configuration","title":"Schema Configuration","text":"<p>Schemas are defined in <code>ConnectorConfig</code> and accessed via <code>config.core.schemas</code>:</p> <pre><code>[[schemas]]\ntopic = \"/iot/sensors\"              # Danube topic\nsubject = \"sensor-data-v1\"          # Registry subject\nschema_type = \"json_schema\"         # Type\nschema_file = \"schemas/sensor.json\" # File path\nauto_register = true                # Auto-register\nversion_strategy = \"latest\"         # Version\n</code></pre>"},{"location":"integrations/source_connector_development/#schema-types","title":"Schema Types","text":"Type Use Case Schema File <code>json_schema</code> Structured data, events Required <code>string</code> Logs, plain text Not needed <code>bytes</code> Binary data, images Not needed <code>number</code> Metrics, counters Not needed"},{"location":"integrations/source_connector_development/#version-strategies","title":"Version Strategies","text":"<ul> <li><code>\"latest\"</code> - Always use newest version (development)</li> <li><code>{ pinned = 2 }</code> - Lock to version 2 (production stability)</li> <li><code>{ minimum = 1 }</code> - Use version \u2265 1 (backward compatibility)</li> </ul>"},{"location":"integrations/source_connector_development/#how-it-works","title":"How It Works","text":"<p>You create records:</p> <pre><code>let record = SourceRecord::from_json(\"/iot/sensors\", &amp;sensor_data)?;\n</code></pre> <p>Runtime automatically:</p> <ol> <li>Validates payload against JSON schema (if <code>json_schema</code> type)</li> <li>Serializes based on <code>schema_type</code></li> <li>Registers schema with registry (if <code>auto_register = true</code>)</li> <li>Attaches schema ID to message</li> <li>Publishes to Danube</li> </ol> <p>Consumers receive:</p> <ul> <li>Typed data already deserialized</li> <li>Schema metadata (subject, version)</li> <li>No manual schema operations needed</li> </ul>"},{"location":"integrations/source_connector_development/#main-entry-point","title":"Main Entry Point","text":"<pre><code>#[tokio::main]\nasync fn main() -&gt; ConnectorResult&lt;()&gt; {\n    // 1. Initialize logging\n    tracing_subscriber::fmt::init();\n\n    // 2. Load configuration\n    let config = MySourceConfig::load()?;\n\n    // 3. Create connector with settings and schemas\n    let connector = MySourceConnector::new(\n        config.my_source,\n        config.core.schemas.clone(),  // \u2190 From ConnectorConfig\n    );\n\n    // 4. Create and run runtime (handles everything else)\n    let runtime = SourceRuntime::new(connector, config.core).await?;\n    runtime.run().await\n}\n</code></pre> <p>Runtime handles:</p> <ul> <li>Danube connection</li> <li>Producer creation with schema configs</li> <li>Polling loop at configured interval</li> <li>Publishing records with retries</li> <li>Metrics and health monitoring</li> <li>Graceful shutdown on signals</li> </ul>"},{"location":"integrations/source_connector_development/#best-practices","title":"Best Practices","text":""},{"location":"integrations/source_connector_development/#configuration","title":"Configuration","text":"<ul> <li>\u2705 Use flattened <code>ConnectorConfig</code> - schemas via <code>config.core.schemas</code></li> <li>\u2705 Never duplicate schemas field in your config</li> <li>\u2705 Keep secrets in environment variables, not TOML</li> <li>\u2705 Validate configuration at startup</li> </ul>"},{"location":"integrations/source_connector_development/#schema-management","title":"Schema Management","text":"<ul> <li>\u2705 Use <code>json_schema</code> for structured data validation</li> <li>\u2705 Set <code>auto_register = true</code> for development</li> <li>\u2705 Pin versions in production (<code>{ pinned = N }</code>)</li> <li>\u2705 Define schemas for all structured topics</li> <li>\u2705 Access via <code>config.core.schemas</code>, not separate field</li> </ul>"},{"location":"integrations/source_connector_development/#data-polling","title":"Data Polling","text":"<ul> <li>\u2705 Return empty <code>Vec</code> when no data (non-blocking)</li> <li>\u2705 Use appropriate batch sizes (100-1000 records)</li> <li>\u2705 Handle rate limits gracefully</li> <li>\u2705 Log warnings for skipped messages</li> <li>\u2705 Add attributes for routing metadata</li> </ul>"},{"location":"integrations/source_connector_development/#error-handling","title":"Error Handling","text":"<ul> <li>\u2705 <code>Retryable</code> for temporary failures</li> <li>\u2705 <code>Fatal</code> for permanent failures</li> <li>\u2705 <code>Ok(vec![])</code> for no data or skippable errors</li> <li>\u2705 Log detailed context for debugging</li> </ul>"},{"location":"integrations/source_connector_development/#performance","title":"Performance","text":"<ul> <li>\u2705 Batch data retrieval when possible</li> <li>\u2705 Avoid blocking operations in <code>poll()</code></li> <li>\u2705 Use connection pooling</li> <li>\u2705 Configure appropriate poll intervals</li> <li>\u2705 Monitor external system rate limits</li> </ul>"},{"location":"integrations/source_connector_development/#common-patterns","title":"Common Patterns","text":""},{"location":"integrations/source_connector_development/#pull-based-polling","title":"Pull-Based Polling","text":"<p>Query external system with cursor/offset tracking. Suitable for databases, REST APIs.</p>"},{"location":"integrations/source_connector_development/#push-based-subscription","title":"Push-Based Subscription","text":"<p>External system pushes data to in-memory buffer. Drain buffer in <code>poll()</code>. Suitable for MQTT, webhooks.</p>"},{"location":"integrations/source_connector_development/#stream-based","title":"Stream-Based","text":"<p>Continuous stream reader. Read batches in <code>poll()</code>. Suitable for Kafka, message queues.</p>"},{"location":"integrations/source_connector_development/#topic-routing","title":"Topic Routing","text":"<p>Map external identifiers to Danube topics using pattern matching, lookup tables, or configuration mappings.</p>"},{"location":"integrations/source_connector_development/#testing","title":"Testing","text":""},{"location":"integrations/source_connector_development/#local-development","title":"Local Development","text":"<ol> <li>Start Danube: <code>docker-compose up -d</code></li> <li>Set config: <code>export CONNECTOR_CONFIG_PATH=./config/connector.toml</code></li> <li>Run: <code>cargo run</code></li> <li>Verify: <code>danube-cli consume --topic /your/topic</code></li> </ol>"},{"location":"integrations/source_connector_development/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test message transformation logic</li> <li>Test pattern matching/routing</li> <li>Mock external system for isolated testing</li> <li>Validate configuration parsing</li> </ul>"},{"location":"integrations/source_connector_development/#summary","title":"Summary","text":"<p>You implement:</p> <ul> <li><code>SourceConnector</code> trait (3 required, 3 optional methods)</li> <li>Configuration with flattened <code>ConnectorConfig</code></li> <li>External system integration</li> <li>Message transformation to <code>SourceRecord</code></li> <li>Topic routing logic</li> </ul> <p>Runtime handles:</p> <ul> <li>Danube connection</li> <li>Schema validation &amp; serialization</li> <li>Message publishing &amp; retries</li> <li>Lifecycle &amp; monitoring</li> <li>Metrics &amp; health</li> </ul> <p>Remember: Configuration uses flattened <code>ConnectorConfig</code>. Access schemas via <code>config.core.schemas</code>. Never duplicate schema fields. Focus on external system integration and data transformation - runtime handles Danube operations.</p>"},{"location":"reference/broker_configuration/","title":"Broker Configuration Reference","text":"<p>Complete reference for <code>danube_broker.yml</code> configuration options. This guide explains each parameter, its impact, and when to adjust it.</p>"},{"location":"reference/broker_configuration/#basic-configuration","title":"Basic Configuration","text":""},{"location":"reference/broker_configuration/#cluster-identity","title":"Cluster Identity","text":"<pre><code>cluster_name: \"MY_CLUSTER\"\n</code></pre> <p>cluster_name</p> <ul> <li>Type: String</li> <li>Default: None (required)</li> <li>Impact: Identifies your Danube cluster in metrics, logs, and multi-cluster setups</li> <li>When to change: Set a meaningful name for production environments (e.g., <code>production-us-east</code>, <code>staging-cluster</code>)</li> </ul>"},{"location":"reference/broker_configuration/#broker-services","title":"Broker Services","text":"<pre><code>broker:\n  host: \"0.0.0.0\"\n  ports:\n    client: 6650      # Producer/consumer connections\n    admin: 50051      # Admin API (gRPC)\n    prometheus: 9040  # Metrics exporter\n</code></pre> <p>broker.host</p> <ul> <li>Type: String (IP address or hostname)</li> <li>Default: <code>0.0.0.0</code> (all interfaces)</li> <li>Impact: Controls which network interfaces the broker binds to</li> <li>When to change:</li> <li>Use <code>0.0.0.0</code> for containers or multi-interface servers</li> <li>Use specific IP for security/isolation</li> <li>Use <code>127.0.0.1</code> for local-only testing</li> </ul> <p>broker.ports.client</p> <ul> <li>Type: Integer</li> <li>Default: <code>6650</code></li> <li>Impact: Port where producers and consumers connect</li> <li>When to change: Resolve port conflicts or follow network policies</li> </ul> <p>broker.ports.admin</p> <ul> <li>Type: Integer</li> <li>Default: <code>50051</code></li> <li>Impact: gRPC port for admin operations (topic creation, subscriptions, etc.)</li> <li>When to change: Avoid conflicts with other gRPC services</li> </ul> <p>broker.ports.prometheus</p> <ul> <li>Type: Integer (optional)</li> <li>Default: <code>9040</code></li> <li>Impact: Prometheus metrics scraping endpoint</li> <li>When to change: Set to <code>null</code> to disable metrics exporter</li> </ul>"},{"location":"reference/broker_configuration/#metadata-store-etcd","title":"Metadata Store (ETCD)","text":"<pre><code>meta_store:\n  host: \"127.0.0.1\"\n  port: 2379\n</code></pre> <p>meta_store.host</p> <ul> <li>Type: String</li> <li>Default: <code>127.0.0.1</code></li> <li>Impact: ETCD connection endpoint for cluster metadata</li> <li>When to change: Point to your ETCD cluster (single node or load balancer)</li> </ul> <p>meta_store.port</p> <ul> <li>Type: Integer</li> <li>Default: <code>2379</code></li> <li>Impact: ETCD port</li> <li>When to change: Rarely (ETCD standard port is 2379)</li> </ul>"},{"location":"reference/broker_configuration/#bootstrap-configuration","title":"Bootstrap Configuration","text":"<pre><code>bootstrap_namespaces:\n  - \"default\"\n\nauto_create_topics: true\n</code></pre> <p>bootstrap_namespaces</p> <ul> <li>Type: Array of strings</li> <li>Default: <code>[\"default\"]</code></li> <li>Impact: Namespaces created automatically on broker startup</li> <li>When to change: Pre-create namespaces for multi-tenant environments (e.g., <code>[\"default\", \"team-a\", \"team-b\"]</code>)</li> </ul> <p>auto_create_topics</p> <ul> <li>Type: Boolean</li> <li>Default: <code>true</code></li> <li>Impact: Whether producers can create topics automatically when publishing</li> <li>When to change:</li> <li><code>true</code>: Development, rapid prototyping</li> <li><code>false</code>: Production with controlled topic creation via Admin API</li> </ul>"},{"location":"reference/broker_configuration/#security-configuration","title":"Security Configuration","text":"<pre><code>auth:\n  mode: tls  # Options: none | tls | tlswithjwt\n  tls:\n    cert_file: \"./cert/server-cert.pem\"\n    key_file: \"./cert/server-key.pem\"\n    ca_file: \"./cert/ca-cert.pem\"\n    verify_client: false\n  jwt:\n    secret_key: \"your-secret-key\"\n    issuer: \"danube-auth\"\n    expiration_time: 3600\n</code></pre>"},{"location":"reference/broker_configuration/#authentication-modes","title":"Authentication Modes","text":"Mode Description Use Case <code>none</code> No authentication Local development, trusted networks <code>tls</code> Mutual TLS (mTLS) Secure production, service-to-service <code>tlswithjwt</code> TLS + JWT tokens Multi-tenant, user-level auth <p>auth.mode</p> <ul> <li>Type: String enum</li> <li>Default: <code>none</code></li> <li>Impact: Security layer for client connections</li> <li>When to change: Always use <code>tls</code> or <code>tlswithjwt</code> in production</li> </ul>"},{"location":"reference/broker_configuration/#tls-settings","title":"TLS Settings","text":"<p>auth.tls.cert_file</p> <ul> <li>Type: File path</li> <li>Impact: Server certificate (public key)</li> <li>When to change: Use valid certificates from your PKI/CA</li> </ul> <p>auth.tls.key_file</p> <ul> <li>Type: File path</li> <li>Impact: Server private key</li> <li>When to change: Keep secure, rotate periodically</li> </ul> <p>auth.tls.ca_file</p> <ul> <li>Type: File path</li> <li>Impact: Certificate authority for validating client certs</li> <li>When to change: Match your organization's CA</li> </ul> <p>auth.tls.verify_client</p> <ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Impact: Require client certificates (mutual TLS)</li> <li>When to change:</li> <li><code>true</code>: Maximum security, requires client certs</li> <li><code>false</code>: Server-only TLS, simpler client setup</li> </ul>"},{"location":"reference/broker_configuration/#jwt-settings","title":"JWT Settings","text":"<p>auth.jwt.secret_key</p> <ul> <li>Type: String</li> <li>Impact: Shared secret for signing/validating JWT tokens</li> <li>When to change: Use strong random key, rotate regularly, store in secrets manager</li> </ul> <p>auth.jwt.issuer</p> <ul> <li>Type: String</li> <li>Default: <code>danube-auth</code></li> <li>Impact: JWT issuer claim for validation</li> <li>When to change: Match your auth service's issuer</li> </ul> <p>auth.jwt.expiration_time</p> <ul> <li>Type: Integer (seconds)</li> <li>Default: <code>3600</code> (1 hour)</li> <li>Impact: Token validity duration</li> <li>When to change:</li> <li>Shorter: More secure, requires frequent renewal</li> <li>Longer: Less overhead, wider security window</li> </ul>"},{"location":"reference/broker_configuration/#load-manager-configuration","title":"Load Manager Configuration","text":"<p>The Load Manager handles topic assignment and automated rebalancing. See Load Manager Architecture for concepts.</p> <pre><code>load_manager:\n  assignment_strategy: \"balanced\"\n  load_report_interval_seconds: 30\n  rebalancing:\n    enabled: false\n    aggressiveness: \"balanced\"\n    check_interval_seconds: 300\n    max_moves_per_hour: 10\n    cooldown_seconds: 60\n    min_brokers_for_rebalance: 2\n    min_topic_age_seconds: 300\n    blacklist_topics: []\n</code></pre>"},{"location":"reference/broker_configuration/#topic-assignment-strategy","title":"Topic Assignment Strategy","text":"<p>Controls how NEW topics are assigned to brokers.</p> Strategy Algorithm CPU Overhead Best For <code>fair</code> Topic count only Lowest Development, testing <code>balanced</code> Multi-factor (topic load + CPU + memory) Medium Production (recommended) <code>weighted_load</code> Adaptive bottleneck detection Highest Variable workloads <p>assignment_strategy</p> <ul> <li>Type: String enum</li> <li>Default: <code>balanced</code></li> <li>Impact: How new topics are placed across brokers</li> <li>When to change:</li> <li><code>fair</code>: Simple setups, predictable placement</li> <li><code>balanced</code>: General production (recommended)</li> <li><code>weighted_load</code>: Clusters with highly variable workloads</li> </ul> <p>Formula for <code>balanced</code> strategy:</p> <pre><code>score = (weighted_topic_load \u00d7 0.3) + (CPU% \u00d7 0.35) + (Memory% \u00d7 0.35)\n</code></pre>"},{"location":"reference/broker_configuration/#load-report-interval","title":"Load Report Interval","text":"<p>load_report_interval_seconds</p> <ul> <li>Type: Integer (seconds)</li> <li>Default: <code>30</code></li> <li>Impact: How often brokers publish resource metrics to ETCD</li> <li>When to change:</li> <li><code>5-10</code>: Testing, rapid response to changes (higher ETCD traffic)</li> <li><code>30-60</code>: Production, balanced overhead</li> <li><code>&gt;60</code>: Low-traffic clusters, reduce ETCD load</li> </ul>"},{"location":"reference/broker_configuration/#automated-rebalancing","title":"Automated Rebalancing","text":"<p>Proactively moves topics between brokers to maintain cluster balance.</p> <p>rebalancing.enabled</p> <ul> <li>Type: Boolean</li> <li>Default: <code>false</code> (disabled for safety)</li> <li>Impact: Enables automatic topic movement</li> <li>When to enable: After cluster is stable, you understand baseline behavior</li> <li>Caution: Start with <code>conservative</code> aggressiveness</li> </ul>"},{"location":"reference/broker_configuration/#aggressiveness-levels","title":"Aggressiveness Levels","text":"<p>Controls how aggressively the system optimizes cluster balance.</p> Level CV Threshold Check Interval Max Moves/Hour Cooldown Use Case <code>conservative</code> &gt;40% 600s (10m) 5 120s Stable production, risk-averse <code>balanced</code> &gt;30% 300s (5m) 10 60s General production <code>aggressive</code> &gt;20% 180s (3m) 20 30s Dynamic workloads, testing <p>CV (Coefficient of Variation) measures cluster imbalance:</p> <ul> <li>&lt;20%: Excellent balance</li> <li>20-30%: Good balance</li> <li>30-40%: Moderate imbalance</li> <li>&gt;40%: Significant imbalance</li> </ul> <p>rebalancing.aggressiveness</p> <ul> <li>Type: String enum</li> <li>Default: <code>balanced</code></li> <li>Impact: Sets default values for check interval, thresholds, and move limits</li> <li>When to change:</li> <li><code>conservative</code>: Production, minimize disruption</li> <li><code>balanced</code>: Most production clusters</li> <li><code>aggressive</code>: Testing, rapid optimization</li> </ul>"},{"location":"reference/broker_configuration/#rebalancing-knobs","title":"Rebalancing Knobs","text":"<p>rebalancing.check_interval_seconds</p> <ul> <li>Type: Integer</li> <li>Default: Varies by aggressiveness</li> <li>Impact: How often to evaluate cluster balance</li> <li>When to change: Override aggressiveness defaults for fine-tuning</li> </ul> <p>rebalancing.max_moves_per_hour</p> <ul> <li>Type: Integer</li> <li>Default: Varies by aggressiveness</li> <li>Impact: Rate limit for topic moves (prevents storms)</li> <li>When to change: Increase for large rebalancing operations, decrease for stability</li> </ul> <p>rebalancing.cooldown_seconds</p> <ul> <li>Type: Integer</li> <li>Default: Varies by aggressiveness</li> <li>Impact: Minimum time between consecutive topic moves</li> <li>When to change: Increase to slow down rebalancing, decrease for faster convergence</li> </ul> <p>rebalancing.min_brokers_for_rebalance</p> <ul> <li>Type: Integer</li> <li>Default: <code>2</code></li> <li>Impact: Rebalancing skipped if cluster has fewer brokers</li> <li>When to change: Rarely (single-broker clusters don't need rebalancing)</li> </ul> <p>rebalancing.min_topic_age_seconds</p> <ul> <li>Type: Integer</li> <li>Default: <code>300</code> (5 minutes)</li> <li>Impact: Don't move topics younger than this</li> <li>When to change: Increase to protect recently created/moved topics</li> </ul>"},{"location":"reference/broker_configuration/#topic-blacklist","title":"Topic Blacklist","text":"<p>rebalancing.blacklist_topics</p> <ul> <li>Type: Array of strings (patterns)</li> <li>Default: <code>[]</code> (empty)</li> <li>Impact: Topics matching these patterns will NEVER be rebalanced</li> <li>Patterns:</li> <li>Exact: <code>/admin/critical-topic</code></li> <li>Namespace wildcard: <code>/system/*</code></li> <li>When to use: Protect critical topics, pin topics to specific brokers</li> </ul> <p>Example:</p> <pre><code>blacklist_topics:\n  - \"/system/*\"              # All system topics\n  - \"/admin/critical-topic\"  # Specific critical topic\n  - \"/production/high-priority/*\"\n</code></pre>"},{"location":"reference/broker_configuration/#wal-and-cloud-storage-configuration","title":"WAL and Cloud Storage Configuration","text":"<p>Cloud-native persistence: local WAL (fast writes) + background cloud uploads (durability). See Persistence Architecture for details.</p> <pre><code>wal_cloud:\n  wal: { ... }\n  uploader: { ... }\n  cloud: { ... }\n  metadata: { ... }\n</code></pre>"},{"location":"reference/broker_configuration/#local-write-ahead-log-wal","title":"Local Write-Ahead Log (WAL)","text":"<pre><code>wal:\n  dir: \"./danube-data/wal\"\n  file_name: \"wal.log\"\n  cache_capacity: 1024\n  file_sync:\n    interval_ms: 5000\n    max_batch_bytes: 10485760\n  rotation:\n    max_bytes: 536870912\n    # max_hours: 24\n  retention:\n    time_minutes: 2880\n    size_mb: 20480\n    check_interval_minutes: 5\n</code></pre> <p>wal.dir</p> <ul> <li>Type: String (path) or <code>null</code></li> <li>Default: <code>./danube-data/wal</code></li> <li>Impact: Root directory for WAL files (per-topic subdirectories created automatically)</li> <li>When to change:</li> <li>Point to fast SSD/NVMe for best performance</li> <li>Set to <code>null</code> for in-memory mode (testing only, no durability)</li> </ul> <p>wal.file_name</p> <ul> <li>Type: String</li> <li>Default: <code>wal.log</code></li> <li>Impact: Base filename (rotation creates <code>wal.0.log</code>, <code>wal.1.log</code>, etc.)</li> <li>When to change: Rarely needed</li> </ul> <p>wal.cache_capacity</p> <ul> <li>Type: Integer (number of messages)</li> <li>Default: <code>1024</code></li> <li>Impact: In-memory replay cache for consumer reads</li> <li>When to change:</li> <li>Higher (<code>4096+</code>): Better consumer hit rates, more memory usage</li> <li>Lower (<code>512</code>): Less memory, more disk reads for catch-up consumers</li> </ul>"},{"location":"reference/broker_configuration/#file-sync-durability","title":"File Sync (Durability)","text":"<p>wal.file_sync.interval_ms</p> <ul> <li>Type: Integer (milliseconds)</li> <li>Default: <code>5000</code> (5 seconds)</li> <li>Impact: How often buffered writes are fsynced to disk</li> <li>Trade-offs:</li> </ul> Value Checkpoint Freshness Fsync Pressure Data Loss Window 1000 (1s) Very fresh High 1 second 5000 (5s) Fresh Balanced 5 seconds 10000 (10s) Stale Low 10 seconds <p>When to change:</p> <ul> <li>Low-latency: <code>1000</code> (1s)</li> <li>Production: <code>5000</code> (5s) - recommended</li> <li>High-throughput: <code>10000</code> (10s)</li> </ul> <p>wal.file_sync.max_batch_bytes</p> <ul> <li>Type: Integer (bytes)</li> <li>Default: <code>10485760</code> (10 MiB)</li> <li>Impact: Force flush when buffer reaches this size</li> <li>When to change:</li> <li>Increase for high-volume topics (better throughput)</li> <li>Decrease for lower latency (more frequent fsyncs)</li> </ul>"},{"location":"reference/broker_configuration/#file-rotation","title":"File Rotation","text":"<p>wal.rotation.max_bytes</p> <ul> <li>Type: Integer (bytes)</li> <li>Default: <code>536870912</code> (512 MiB)</li> <li>Impact: Create new WAL file when current file exceeds this size</li> <li>When to change:</li> <li><code>512 MiB</code>: Balanced (recommended)</li> <li><code>1 GiB</code>: Fewer files, larger cloud objects</li> <li><code>256 MiB</code>: More files, smaller cloud objects</li> </ul> <p>wal.rotation.max_hours</p> <ul> <li>Type: Integer (hours, optional)</li> <li>Default: Disabled (commented out)</li> <li>Impact: Rotate even if size threshold not reached</li> <li>When to enable: Good for low-traffic topics to prevent infinitely old files</li> <li>Recommended: <code>24</code> hours for production</li> </ul>"},{"location":"reference/broker_configuration/#wal-retention-local-cleanup","title":"WAL Retention (Local Cleanup)","text":"<p>wal.retention.time_minutes</p> <ul> <li>Type: Integer (minutes)</li> <li>Default: <code>2880</code> (48 hours)</li> <li>Impact: Delete local WAL files older than this (after cloud upload)</li> <li>When to change:</li> <li>Production: <code>2880</code> (48h) for safety margin</li> <li>Space-constrained: <code>1440</code> (24h) minimum</li> <li>Must be larger than uploader interval</li> </ul> <p>wal.retention.size_mb</p> <ul> <li>Type: Integer (megabytes per topic)</li> <li>Default: <code>20480</code> (20 GiB)</li> <li>Impact: Delete oldest files when total exceeds this</li> <li>When to change:</li> <li>High-volume: <code>20480</code> (20 GiB) or higher</li> <li>Space-constrained: <code>10240</code> (10 GiB) minimum</li> </ul> <p>wal.retention.check_interval_minutes</p> <ul> <li>Type: Integer (minutes)</li> <li>Default: <code>5</code></li> <li>Impact: How often retention policy runs</li> <li>When to change: Rarely (5 minutes is responsive without overhead)</li> </ul>"},{"location":"reference/broker_configuration/#cloud-uploader","title":"Cloud Uploader","text":"<pre><code>uploader:\n  enabled: true\n  interval_seconds: 30\n  root_prefix: \"/danube-data\"\n  max_object_mb: 256\n</code></pre> <p>uploader.enabled</p> <ul> <li>Type: Boolean</li> <li>Default: <code>true</code></li> <li>Impact: Enable/disable background cloud uploads</li> <li>When to disable: Local-only testing, ephemeral workloads</li> <li>When to enable: Production (always)</li> </ul> <p>uploader.interval_seconds</p> <ul> <li>Type: Integer (seconds)</li> <li>Default: <code>300</code> (5 minutes)</li> <li>Impact: Upload cycle frequency</li> </ul> Value RPO (Recovery Point) Cloud Overhead Use Case 30s 30 seconds High Testing, fast feedback 60s 1 minute Medium-High High-durability production 300s 5 minutes Balanced General production 600s 10 minutes Low Cost-sensitive <p>uploader.root_prefix</p> <ul> <li>Type: String</li> <li>Default: <code>/danube-data</code></li> <li>Impact: ETCD metadata prefix for cloud object descriptors</li> <li>When to change: Only for multiple independent Danube clusters sharing ETCD</li> </ul> <p>uploader.max_object_mb</p> <ul> <li>Type: Integer (megabytes, optional)</li> <li>Default: <code>256</code></li> <li>Impact: Maximum size per cloud object</li> <li>When to change:</li> <li><code>256 MB</code>: Optimal for S3/GCS multipart uploads (recommended)</li> <li>Larger: Fewer objects, higher per-object latency</li> <li>Smaller: More objects, better parallelism</li> </ul>"},{"location":"reference/broker_configuration/#cloud-storage-backend","title":"Cloud Storage Backend","text":"<pre><code>cloud:\n  backend: \"fs\"\n  root: \"./danube-data/cloud-storage\"\n  # Backend-specific options below\n</code></pre> <p>cloud.backend</p> <ul> <li>Type: String enum</li> <li>Options: <code>fs</code> | <code>s3</code> | <code>gcs</code> | <code>azblob</code> | <code>memory</code></li> <li>Default: <code>fs</code></li> <li>Impact: Cloud storage provider</li> </ul> Backend Use Case Multi-Broker Support <code>memory</code> Testing only (no durability) No <code>fs</code> Local development, shared NFS/EFS Yes (with shared storage) <code>s3</code> Production (AWS, MinIO, etc.) Yes <code>gcs</code> Production (Google Cloud) Yes <code>azblob</code> Production (Azure) Yes <p>cloud.root</p> <ul> <li>Type: String (backend-specific format)</li> <li>Impact: Storage location</li> <li>Formats:</li> <li><code>fs</code>: <code>./path/to/directory</code></li> <li><code>s3</code>: <code>s3://bucket-name/prefix</code></li> <li><code>gcs</code>: <code>gcs://bucket-name/prefix</code></li> <li><code>azblob</code>: <code>container-name/prefix</code></li> <li><code>memory</code>: <code>namespace-prefix</code></li> <li>Multi-broker requirement: Must be shared/accessible across all brokers</li> </ul>"},{"location":"reference/broker_configuration/#backend-specific-options","title":"Backend-Specific Options","text":"<p>Amazon S3:</p> <pre><code>cloud:\n  backend: \"s3\"\n  root: \"s3://my-bucket/danube-data\"\n  region: \"us-east-1\"\n  endpoint: \"https://s3.us-east-1.amazonaws.com\"  # Optional\n  access_key: \"${AWS_ACCESS_KEY_ID}\"              # Or use IAM roles\n  secret_key: \"${AWS_SECRET_ACCESS_KEY}\"\n  anonymous: false\n</code></pre> <p>Google Cloud Storage:</p> <pre><code>cloud:\n  backend: \"gcs\"\n  root: \"gcs://my-bucket/danube-data\"\n  project: \"my-gcp-project\"\n  credentials_path: \"/path/to/service-account.json\"\n  # Or credentials_json: \"{ ... }\"  # Inline JSON string\n</code></pre> <p>Azure Blob Storage:</p> <pre><code>cloud:\n  backend: \"azblob\"\n  root: \"my-container/danube-data\"\n  endpoint: \"https://myaccount.blob.core.windows.net\"\n  account_name: \"myaccount\"\n  account_key: \"${AZURE_STORAGE_KEY}\"\n</code></pre> <p>Local Filesystem:</p> <pre><code>cloud:\n  backend: \"fs\"\n  root: \"./danube-data/cloud-storage\"\n  # For multi-broker: use shared storage like NFS\n  # root: \"/mnt/shared-nfs/danube-cloud\"\n</code></pre>"},{"location":"reference/broker_configuration/#metadata-store","title":"Metadata Store","text":"<pre><code>metadata:\n  etcd_endpoint: \"127.0.0.1:2379\"\n  in_memory: false\n</code></pre> <p>metadata.etcd_endpoint</p> <ul> <li>Type: String (host:port)</li> <li>Default: <code>127.0.0.1:2379</code></li> <li>Impact: ETCD endpoint for storing cloud object descriptors and indexes</li> <li>When to change: Should match <code>meta_store.host:port</code> for consistency</li> <li>Production: Use clustered ETCD for high availability</li> </ul> <p>metadata.in_memory</p> <ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Impact: Use in-memory metadata storage (no persistence)</li> <li>When to enable: Testing only (ephemeral)</li> <li>Production: Always <code>false</code></li> </ul>"},{"location":"reference/broker_configuration/#broker-policies","title":"Broker Policies","text":"<p>Default resource limits for topics. Can be overridden at namespace or topic level.</p> <pre><code>policies:\n  max_producers_per_topic: 0\n  max_subscriptions_per_topic: 0\n  max_consumers_per_topic: 0\n  max_consumers_per_subscription: 0\n  max_publish_rate: 0\n  max_subscription_dispatch_rate: 0\n  max_message_size: 10485760  # 10 MB\n</code></pre> <p>Default value <code>0</code> means unlimited for all rate/count limits.</p>"},{"location":"reference/broker_configuration/#connection-limits","title":"Connection Limits","text":"<p>max_producers_per_topic</p> <ul> <li>Type: Integer</li> <li>Default: <code>0</code> (unlimited)</li> <li>Impact: Maximum concurrent producers per topic</li> <li>When to set: Prevent resource exhaustion from producer storms</li> </ul> <p>max_subscriptions_per_topic</p> <ul> <li>Type: Integer</li> <li>Default: <code>0</code> (unlimited)</li> <li>Impact: Maximum subscriptions per topic</li> <li>When to set: Control fan-out complexity</li> </ul> <p>max_consumers_per_topic</p> <ul> <li>Type: Integer</li> <li>Default: <code>0</code> (unlimited)</li> <li>Impact: Maximum concurrent consumers across all subscriptions</li> <li>When to set: Limit total consumer connections</li> </ul> <p>max_consumers_per_subscription</p> <ul> <li>Type: Integer</li> <li>Default: <code>0</code> (unlimited)</li> <li>Impact: Maximum consumers sharing a single subscription</li> <li>When to set: Control load-sharing fan-out</li> </ul>"},{"location":"reference/broker_configuration/#rate-limits","title":"Rate Limits","text":"<p>max_publish_rate</p> <ul> <li>Type: Integer (messages/second or bytes/second)</li> <li>Default: <code>0</code> (unlimited)</li> <li>Impact: Throttle producer publish rate</li> <li>When to set: Prevent producer overwhelming broker</li> </ul> <p>max_subscription_dispatch_rate</p> <ul> <li>Type: Integer (messages/second)</li> <li>Default: <code>0</code> (unlimited)</li> <li>Impact: Throttle consumer dispatch rate</li> <li>When to set: Protect slow consumers</li> </ul>"},{"location":"reference/broker_configuration/#message-size","title":"Message Size","text":"<p>max_message_size</p> <ul> <li>Type: Integer (bytes)</li> <li>Default: <code>10485760</code> (10 MB)</li> <li>Impact: Maximum single message size</li> <li>When to change:</li> <li>Increase: Large payloads (video, logs)</li> <li>Decrease: Prevent memory issues from huge messages</li> <li>Note: Very large messages impact performance</li> </ul>"}]}